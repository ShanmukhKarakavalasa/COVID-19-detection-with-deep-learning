{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d3df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('updated_coughvid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90e38b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc{1}</th>\n",
       "      <th>mfcc{2}</th>\n",
       "      <th>mfcc{3}</th>\n",
       "      <th>mfcc{4}</th>\n",
       "      <th>...</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>ddpJitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>aqpq5Shimmer</th>\n",
       "      <th>apq11Shimmer</th>\n",
       "      <th>ddaShimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065850</td>\n",
       "      <td>0.403767</td>\n",
       "      <td>1316.806414</td>\n",
       "      <td>1373.998076</td>\n",
       "      <td>2637.860622</td>\n",
       "      <td>0.057043</td>\n",
       "      <td>-396.59204</td>\n",
       "      <td>69.540160</td>\n",
       "      <td>2.152846</td>\n",
       "      <td>13.354017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.064553</td>\n",
       "      <td>0.187378</td>\n",
       "      <td>1.718899</td>\n",
       "      <td>0.089158</td>\n",
       "      <td>0.143649</td>\n",
       "      <td>0.352439</td>\n",
       "      <td>0.267473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033997</td>\n",
       "      <td>0.532892</td>\n",
       "      <td>2474.234037</td>\n",
       "      <td>2125.162327</td>\n",
       "      <td>4869.731365</td>\n",
       "      <td>0.186172</td>\n",
       "      <td>-435.21085</td>\n",
       "      <td>45.288998</td>\n",
       "      <td>-12.166409</td>\n",
       "      <td>10.258451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014014</td>\n",
       "      <td>0.018379</td>\n",
       "      <td>0.042043</td>\n",
       "      <td>0.130333</td>\n",
       "      <td>1.313323</td>\n",
       "      <td>0.049385</td>\n",
       "      <td>0.059807</td>\n",
       "      <td>0.110768</td>\n",
       "      <td>0.148156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.370873</td>\n",
       "      <td>2158.381678</td>\n",
       "      <td>2007.817231</td>\n",
       "      <td>4750.294555</td>\n",
       "      <td>0.125032</td>\n",
       "      <td>-412.62552</td>\n",
       "      <td>54.555480</td>\n",
       "      <td>-1.768253</td>\n",
       "      <td>3.977824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.037981</td>\n",
       "      <td>0.100577</td>\n",
       "      <td>0.272967</td>\n",
       "      <td>2.125802</td>\n",
       "      <td>0.132889</td>\n",
       "      <td>0.196880</td>\n",
       "      <td>0.160610</td>\n",
       "      <td>0.398668</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047035</td>\n",
       "      <td>0.479319</td>\n",
       "      <td>2678.491315</td>\n",
       "      <td>2139.232294</td>\n",
       "      <td>5136.450596</td>\n",
       "      <td>0.246256</td>\n",
       "      <td>-393.00226</td>\n",
       "      <td>48.030190</td>\n",
       "      <td>-29.901045</td>\n",
       "      <td>25.478853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>0.031223</td>\n",
       "      <td>0.096085</td>\n",
       "      <td>0.175612</td>\n",
       "      <td>1.617597</td>\n",
       "      <td>0.070595</td>\n",
       "      <td>0.094968</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.211784</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.741700</td>\n",
       "      <td>3316.010424</td>\n",
       "      <td>2345.969321</td>\n",
       "      <td>6007.887783</td>\n",
       "      <td>0.278178</td>\n",
       "      <td>-556.18726</td>\n",
       "      <td>10.974088</td>\n",
       "      <td>-10.207122</td>\n",
       "      <td>6.857212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023268</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.069803</td>\n",
       "      <td>0.138516</td>\n",
       "      <td>1.320449</td>\n",
       "      <td>0.075879</td>\n",
       "      <td>0.081973</td>\n",
       "      <td>0.145956</td>\n",
       "      <td>0.227636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24437</th>\n",
       "      <td>0.030827</td>\n",
       "      <td>0.429203</td>\n",
       "      <td>2974.741815</td>\n",
       "      <td>2265.905377</td>\n",
       "      <td>5435.183318</td>\n",
       "      <td>0.216093</td>\n",
       "      <td>-424.03302</td>\n",
       "      <td>38.712093</td>\n",
       "      <td>-16.247238</td>\n",
       "      <td>12.712377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029542</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>0.088625</td>\n",
       "      <td>0.202130</td>\n",
       "      <td>1.619779</td>\n",
       "      <td>0.075903</td>\n",
       "      <td>0.199575</td>\n",
       "      <td>0.160610</td>\n",
       "      <td>0.227709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24438</th>\n",
       "      <td>0.030711</td>\n",
       "      <td>0.535591</td>\n",
       "      <td>2719.621677</td>\n",
       "      <td>2132.117936</td>\n",
       "      <td>5260.719083</td>\n",
       "      <td>0.198633</td>\n",
       "      <td>-471.09518</td>\n",
       "      <td>28.604359</td>\n",
       "      <td>-8.991700</td>\n",
       "      <td>12.397835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019939</td>\n",
       "      <td>0.022426</td>\n",
       "      <td>0.059816</td>\n",
       "      <td>0.176616</td>\n",
       "      <td>1.347352</td>\n",
       "      <td>0.107413</td>\n",
       "      <td>0.148694</td>\n",
       "      <td>0.117848</td>\n",
       "      <td>0.322239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24439</th>\n",
       "      <td>0.159654</td>\n",
       "      <td>0.389324</td>\n",
       "      <td>2360.664509</td>\n",
       "      <td>1696.391140</td>\n",
       "      <td>4114.219514</td>\n",
       "      <td>0.169187</td>\n",
       "      <td>-206.32933</td>\n",
       "      <td>59.016940</td>\n",
       "      <td>-74.789270</td>\n",
       "      <td>-1.210189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019389</td>\n",
       "      <td>0.020776</td>\n",
       "      <td>0.058168</td>\n",
       "      <td>0.164419</td>\n",
       "      <td>1.429379</td>\n",
       "      <td>0.093083</td>\n",
       "      <td>0.119234</td>\n",
       "      <td>0.175221</td>\n",
       "      <td>0.279249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24440</th>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.451451</td>\n",
       "      <td>2788.705294</td>\n",
       "      <td>1800.585083</td>\n",
       "      <td>4708.568653</td>\n",
       "      <td>0.239603</td>\n",
       "      <td>-469.87784</td>\n",
       "      <td>59.399067</td>\n",
       "      <td>-41.076590</td>\n",
       "      <td>-2.028796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>0.036794</td>\n",
       "      <td>0.098542</td>\n",
       "      <td>0.205180</td>\n",
       "      <td>1.715662</td>\n",
       "      <td>0.105993</td>\n",
       "      <td>0.139264</td>\n",
       "      <td>0.166415</td>\n",
       "      <td>0.317979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24441</th>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>951.673749</td>\n",
       "      <td>828.363544</td>\n",
       "      <td>1897.664311</td>\n",
       "      <td>0.050561</td>\n",
       "      <td>-587.85986</td>\n",
       "      <td>24.133877</td>\n",
       "      <td>-4.841230</td>\n",
       "      <td>9.684202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.037088</td>\n",
       "      <td>0.084477</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>1.495134</td>\n",
       "      <td>0.047783</td>\n",
       "      <td>0.049730</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>0.143348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24442 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse  chroma_stft    spec_cent      spec_bw      rolloff       zcr  \\\n",
       "0      0.065850     0.403767  1316.806414  1373.998076  2637.860622  0.057043   \n",
       "1      0.033997     0.532892  2474.234037  2125.162327  4869.731365  0.186172   \n",
       "2      0.023310     0.370873  2158.381678  2007.817231  4750.294555  0.125032   \n",
       "3      0.047035     0.479319  2678.491315  2139.232294  5136.450596  0.246256   \n",
       "4      0.011785     0.741700  3316.010424  2345.969321  6007.887783  0.278178   \n",
       "...         ...          ...          ...          ...          ...       ...   \n",
       "24437  0.030827     0.429203  2974.741815  2265.905377  5435.183318  0.216093   \n",
       "24438  0.030711     0.535591  2719.621677  2132.117936  5260.719083  0.198633   \n",
       "24439  0.159654     0.389324  2360.664509  1696.391140  4114.219514  0.169187   \n",
       "24440  0.030981     0.451451  2788.705294  1800.585083  4708.568653  0.239603   \n",
       "24441  0.007046     0.167502   951.673749   828.363544  1897.664311  0.050561   \n",
       "\n",
       "         mfcc{1}    mfcc{2}    mfcc{3}    mfcc{4}  ...  rapJitter  ppq5Jitter  \\\n",
       "0     -396.59204  69.540160   2.152846  13.354017  ...   0.021518    0.024053   \n",
       "1     -435.21085  45.288998 -12.166409  10.258451  ...   0.014014    0.018379   \n",
       "2     -412.62552  54.555480  -1.768253   3.977824  ...   0.033526    0.037981   \n",
       "3     -393.00226  48.030190 -29.901045  25.478853  ...   0.032028    0.031223   \n",
       "4     -556.18726  10.974088 -10.207122   6.857212  ...   0.023268    0.026566   \n",
       "...          ...        ...        ...        ...  ...        ...         ...   \n",
       "24437 -424.03302  38.712093 -16.247238  12.712377  ...   0.029542    0.036789   \n",
       "24438 -471.09518  28.604359  -8.991700  12.397835  ...   0.019939    0.022426   \n",
       "24439 -206.32933  59.016940 -74.789270  -1.210189  ...   0.019389    0.020776   \n",
       "24440 -469.87784  59.399067 -41.076590  -2.028796  ...   0.032847    0.036794   \n",
       "24441 -587.85986  24.133877  -4.841230   9.684202  ...   0.028159    0.037088   \n",
       "\n",
       "       ddpJitter  localShimmer  localdbShimmer  apq3Shimmer  aqpq5Shimmer  \\\n",
       "0       0.064553      0.187378        1.718899     0.089158      0.143649   \n",
       "1       0.042043      0.130333        1.313323     0.049385      0.059807   \n",
       "2       0.100577      0.272967        2.125802     0.132889      0.196880   \n",
       "3       0.096085      0.175612        1.617597     0.070595      0.094968   \n",
       "4       0.069803      0.138516        1.320449     0.075879      0.081973   \n",
       "...          ...           ...             ...          ...           ...   \n",
       "24437   0.088625      0.202130        1.619779     0.075903      0.199575   \n",
       "24438   0.059816      0.176616        1.347352     0.107413      0.148694   \n",
       "24439   0.058168      0.164419        1.429379     0.093083      0.119234   \n",
       "24440   0.098542      0.205180        1.715662     0.105993      0.139264   \n",
       "24441   0.084477      0.119565        1.495134     0.047783      0.049730   \n",
       "\n",
       "       apq11Shimmer  ddaShimmer  label  \n",
       "0          0.352439    0.267473      0  \n",
       "1          0.110768    0.148156      1  \n",
       "2          0.160610    0.398668      1  \n",
       "3          0.154762    0.211784      0  \n",
       "4          0.145956    0.227636      0  \n",
       "...             ...         ...    ...  \n",
       "24437      0.160610    0.227709      1  \n",
       "24438      0.117848    0.322239      1  \n",
       "24439      0.175221    0.279249      0  \n",
       "24440      0.166415    0.317979      1  \n",
       "24441      0.021235    0.143348      0  \n",
       "\n",
       "[24442 rows x 80 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311e092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d973c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a4f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57675174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4ddacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "612/612 [==============================] - 17s 24ms/step - loss: 0.6206 - accuracy: 0.6592 - val_loss: 0.5856 - val_accuracy: 0.6965\n",
      "Epoch 2/50\n",
      "612/612 [==============================] - 13s 21ms/step - loss: 0.5836 - accuracy: 0.7012 - val_loss: 0.5787 - val_accuracy: 0.7065\n",
      "Epoch 3/50\n",
      "612/612 [==============================] - 17s 28ms/step - loss: 0.5768 - accuracy: 0.7068 - val_loss: 0.5751 - val_accuracy: 0.7155\n",
      "Epoch 4/50\n",
      "612/612 [==============================] - 16s 26ms/step - loss: 0.5717 - accuracy: 0.7110 - val_loss: 0.5763 - val_accuracy: 0.7087\n",
      "Epoch 5/50\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 0.5671 - accuracy: 0.7183 - val_loss: 0.5675 - val_accuracy: 0.7147\n",
      "Epoch 6/50\n",
      "612/612 [==============================] - 22s 36ms/step - loss: 0.5633 - accuracy: 0.7194 - val_loss: 0.5627 - val_accuracy: 0.7224\n",
      "Epoch 7/50\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 0.5598 - accuracy: 0.7215 - val_loss: 0.5622 - val_accuracy: 0.7202\n",
      "Epoch 8/50\n",
      "612/612 [==============================] - 21s 35ms/step - loss: 0.5565 - accuracy: 0.7257 - val_loss: 0.5589 - val_accuracy: 0.7233\n",
      "Epoch 9/50\n",
      "612/612 [==============================] - 21s 35ms/step - loss: 0.5525 - accuracy: 0.7304 - val_loss: 0.5564 - val_accuracy: 0.7265\n",
      "Epoch 10/50\n",
      "612/612 [==============================] - 22s 35ms/step - loss: 0.5499 - accuracy: 0.7324 - val_loss: 0.5567 - val_accuracy: 0.7273\n",
      "Epoch 11/50\n",
      "612/612 [==============================] - 23s 38ms/step - loss: 0.5476 - accuracy: 0.7327 - val_loss: 0.5515 - val_accuracy: 0.7327\n",
      "Epoch 12/50\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 0.5448 - accuracy: 0.7355 - val_loss: 0.5516 - val_accuracy: 0.7345\n",
      "Epoch 13/50\n",
      "612/612 [==============================] - 22s 35ms/step - loss: 0.5425 - accuracy: 0.7378 - val_loss: 0.5478 - val_accuracy: 0.7378\n",
      "Epoch 14/50\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 0.5381 - accuracy: 0.7412 - val_loss: 0.5526 - val_accuracy: 0.7314\n",
      "Epoch 15/50\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 0.5369 - accuracy: 0.7422 - val_loss: 0.5426 - val_accuracy: 0.7439\n",
      "Epoch 16/50\n",
      "612/612 [==============================] - 23s 38ms/step - loss: 0.5317 - accuracy: 0.7451 - val_loss: 0.5406 - val_accuracy: 0.7484\n",
      "Epoch 17/50\n",
      "612/612 [==============================] - 23s 37ms/step - loss: 0.5285 - accuracy: 0.7482 - val_loss: 0.5439 - val_accuracy: 0.7396\n",
      "Epoch 18/50\n",
      "612/612 [==============================] - 17s 28ms/step - loss: 0.5261 - accuracy: 0.7481 - val_loss: 0.5364 - val_accuracy: 0.7486\n",
      "Epoch 19/50\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 0.5212 - accuracy: 0.7539 - val_loss: 0.5353 - val_accuracy: 0.7517\n",
      "Epoch 20/50\n",
      "612/612 [==============================] - 19s 32ms/step - loss: 0.5183 - accuracy: 0.7552 - val_loss: 0.5291 - val_accuracy: 0.7490\n",
      "Epoch 21/50\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 0.5141 - accuracy: 0.7555 - val_loss: 0.5233 - val_accuracy: 0.7531\n",
      "Epoch 22/50\n",
      "612/612 [==============================] - 20s 32ms/step - loss: 0.5091 - accuracy: 0.7615 - val_loss: 0.5278 - val_accuracy: 0.7496\n",
      "Epoch 23/50\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 0.5048 - accuracy: 0.7655 - val_loss: 0.5167 - val_accuracy: 0.7588\n",
      "Epoch 24/50\n",
      "612/612 [==============================] - 22s 37ms/step - loss: 0.4985 - accuracy: 0.7676 - val_loss: 0.5248 - val_accuracy: 0.7578\n",
      "Epoch 25/50\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 0.4942 - accuracy: 0.7720 - val_loss: 0.5046 - val_accuracy: 0.7697\n",
      "Epoch 26/50\n",
      "612/612 [==============================] - 19s 32ms/step - loss: 0.4907 - accuracy: 0.7747 - val_loss: 0.5024 - val_accuracy: 0.7730\n",
      "Epoch 27/50\n",
      "612/612 [==============================] - 19s 32ms/step - loss: 0.4845 - accuracy: 0.7791 - val_loss: 0.4885 - val_accuracy: 0.7822\n",
      "Epoch 28/50\n",
      "612/612 [==============================] - 19s 31ms/step - loss: 0.4789 - accuracy: 0.7835 - val_loss: 0.4866 - val_accuracy: 0.7809\n",
      "Epoch 29/50\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 0.4719 - accuracy: 0.7861 - val_loss: 0.4890 - val_accuracy: 0.7807\n",
      "Epoch 30/50\n",
      "612/612 [==============================] - 20s 33ms/step - loss: 0.4671 - accuracy: 0.7900 - val_loss: 0.4730 - val_accuracy: 0.7989\n",
      "Epoch 31/50\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 0.4619 - accuracy: 0.7948 - val_loss: 0.4701 - val_accuracy: 0.7922\n",
      "Epoch 32/50\n",
      "612/612 [==============================] - 21s 35ms/step - loss: 0.4573 - accuracy: 0.7991 - val_loss: 0.4978 - val_accuracy: 0.7719\n",
      "Epoch 33/50\n",
      "612/612 [==============================] - 21s 35ms/step - loss: 0.4529 - accuracy: 0.7995 - val_loss: 0.4643 - val_accuracy: 0.7918\n",
      "Epoch 34/50\n",
      "612/612 [==============================] - 21s 35ms/step - loss: 0.4500 - accuracy: 0.8024 - val_loss: 0.4632 - val_accuracy: 0.7987\n",
      "Epoch 35/50\n",
      "612/612 [==============================] - 22s 37ms/step - loss: 0.4469 - accuracy: 0.8048 - val_loss: 0.4541 - val_accuracy: 0.8022\n",
      "Epoch 36/50\n",
      "612/612 [==============================] - 23s 38ms/step - loss: 0.4435 - accuracy: 0.8049 - val_loss: 0.4570 - val_accuracy: 0.7983\n",
      "Epoch 37/50\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 0.4395 - accuracy: 0.8093 - val_loss: 0.4501 - val_accuracy: 0.8014\n",
      "Epoch 38/50\n",
      "612/612 [==============================] - 22s 36ms/step - loss: 0.4366 - accuracy: 0.8097 - val_loss: 0.4424 - val_accuracy: 0.8100\n",
      "Epoch 39/50\n",
      "612/612 [==============================] - 22s 36ms/step - loss: 0.4301 - accuracy: 0.8152 - val_loss: 0.4445 - val_accuracy: 0.8108\n",
      "Epoch 40/50\n",
      "612/612 [==============================] - 28s 45ms/step - loss: 0.4270 - accuracy: 0.8179 - val_loss: 0.4459 - val_accuracy: 0.8038\n",
      "Epoch 41/50\n",
      "612/612 [==============================] - 26s 43ms/step - loss: 0.4257 - accuracy: 0.8147 - val_loss: 0.4423 - val_accuracy: 0.8083\n",
      "Epoch 42/50\n",
      "612/612 [==============================] - 26s 42ms/step - loss: 0.4232 - accuracy: 0.8189 - val_loss: 0.4562 - val_accuracy: 0.7955\n",
      "Epoch 43/50\n",
      "612/612 [==============================] - 24s 39ms/step - loss: 0.4232 - accuracy: 0.8200 - val_loss: 0.4407 - val_accuracy: 0.8147\n",
      "Epoch 44/50\n",
      "612/612 [==============================] - 21s 35ms/step - loss: 0.4152 - accuracy: 0.8231 - val_loss: 0.4500 - val_accuracy: 0.8038\n",
      "Epoch 45/50\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 0.4146 - accuracy: 0.8231 - val_loss: 0.4377 - val_accuracy: 0.8118\n",
      "Epoch 46/50\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 0.4131 - accuracy: 0.8233 - val_loss: 0.4434 - val_accuracy: 0.8108\n",
      "Epoch 47/50\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 0.4097 - accuracy: 0.8259 - val_loss: 0.4474 - val_accuracy: 0.8030\n",
      "Epoch 48/50\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 0.4049 - accuracy: 0.8302 - val_loss: 0.4381 - val_accuracy: 0.8110\n",
      "Epoch 49/50\n",
      "612/612 [==============================] - 21s 34ms/step - loss: 0.4081 - accuracy: 0.8263 - val_loss: 0.4410 - val_accuracy: 0.8065\n",
      "Epoch 50/50\n",
      "612/612 [==============================] - 20s 32ms/step - loss: 0.4214 - accuracy: 0.8174 - val_loss: 0.4306 - val_accuracy: 0.8161\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU, Dense, Dropout,BatchNormalization\n",
    "model = Sequential()\n",
    "model.add(GRU(units=32, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4775a830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 2s 8ms/step\n",
      "Accuracy: 0.816117815504193\n",
      "Precision: 0.8453318335208099\n",
      "Recall: 0.7066290550070522\n",
      "F1 Score: 0.7697823303457105\n",
      "ROC AUC: 0.8558896052852416\n",
      "Specificity: 0.9004344677769732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model on test set\n",
    "accuracy = accuracy_score(y_test, y_pred.round())\n",
    "precision = precision_score(y_test, y_pred.round())\n",
    "recall = recall_score(y_test, y_pred.round())\n",
    "f1 = f1_score(y_test, y_pred.round())\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred.round()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b589f85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "612/612 [==============================] - 61s 85ms/step - loss: 0.6790 - accuracy: 0.6221 - val_loss: 0.6465 - val_accuracy: 0.6024\n",
      "Epoch 2/50\n",
      "612/612 [==============================] - 49s 80ms/step - loss: 0.6498 - accuracy: 0.6414 - val_loss: 0.6202 - val_accuracy: 0.6764\n",
      "Epoch 3/50\n",
      "612/612 [==============================] - 48s 79ms/step - loss: 0.6558 - accuracy: 0.6509 - val_loss: 0.6529 - val_accuracy: 0.6756\n",
      "Epoch 4/50\n",
      "612/612 [==============================] - 47s 77ms/step - loss: 0.6662 - accuracy: 0.6532 - val_loss: 0.6427 - val_accuracy: 0.6222\n",
      "Epoch 5/50\n",
      "612/612 [==============================] - 46s 75ms/step - loss: 0.6439 - accuracy: 0.6327 - val_loss: 0.6356 - val_accuracy: 0.6508\n",
      "Epoch 6/50\n",
      "612/612 [==============================] - 49s 80ms/step - loss: 0.6355 - accuracy: 0.6459 - val_loss: 0.6228 - val_accuracy: 0.6609\n",
      "Epoch 7/50\n",
      "612/612 [==============================] - 55s 90ms/step - loss: 0.6238 - accuracy: 0.6610 - val_loss: 0.6050 - val_accuracy: 0.6965\n",
      "Epoch 8/50\n",
      "612/612 [==============================] - 52s 86ms/step - loss: 0.6115 - accuracy: 0.6768 - val_loss: 0.5934 - val_accuracy: 0.6932\n",
      "Epoch 9/50\n",
      "612/612 [==============================] - 48s 79ms/step - loss: 0.5970 - accuracy: 0.6953 - val_loss: 0.5535 - val_accuracy: 0.7243\n",
      "Epoch 10/50\n",
      "612/612 [==============================] - 50s 82ms/step - loss: 0.5707 - accuracy: 0.7218 - val_loss: 0.5435 - val_accuracy: 0.7384\n",
      "Epoch 11/50\n",
      "612/612 [==============================] - 46s 75ms/step - loss: 0.6038 - accuracy: 0.6887 - val_loss: 0.5770 - val_accuracy: 0.7096\n",
      "Epoch 12/50\n",
      "612/612 [==============================] - 48s 79ms/step - loss: 0.5991 - accuracy: 0.6928 - val_loss: 0.6181 - val_accuracy: 0.6778\n",
      "Epoch 13/50\n",
      "612/612 [==============================] - 49s 81ms/step - loss: 0.5925 - accuracy: 0.6992 - val_loss: 0.5817 - val_accuracy: 0.7059\n",
      "Epoch 14/50\n",
      "612/612 [==============================] - 48s 79ms/step - loss: 0.5807 - accuracy: 0.7178 - val_loss: 0.5594 - val_accuracy: 0.7249\n",
      "Epoch 15/50\n",
      "612/612 [==============================] - 48s 79ms/step - loss: 0.5748 - accuracy: 0.7192 - val_loss: 0.5560 - val_accuracy: 0.7263\n",
      "Epoch 16/50\n",
      "612/612 [==============================] - 56s 92ms/step - loss: 0.5669 - accuracy: 0.7191 - val_loss: 0.5456 - val_accuracy: 0.7382\n",
      "Epoch 17/50\n",
      "612/612 [==============================] - 49s 81ms/step - loss: 0.5811 - accuracy: 0.7070 - val_loss: 0.6265 - val_accuracy: 0.6537\n",
      "Epoch 18/50\n",
      "612/612 [==============================] - 53s 87ms/step - loss: 0.6105 - accuracy: 0.6743 - val_loss: 0.5880 - val_accuracy: 0.6999\n",
      "Epoch 19/50\n",
      "612/612 [==============================] - 47s 76ms/step - loss: 0.5887 - accuracy: 0.7010 - val_loss: 0.5707 - val_accuracy: 0.7177\n",
      "Epoch 20/50\n",
      "612/612 [==============================] - 49s 79ms/step - loss: 0.5808 - accuracy: 0.7167 - val_loss: 0.5509 - val_accuracy: 0.7331\n",
      "Epoch 21/50\n",
      "612/612 [==============================] - 47s 77ms/step - loss: 0.5683 - accuracy: 0.7229 - val_loss: 0.6082 - val_accuracy: 0.6713\n",
      "Epoch 22/50\n",
      "612/612 [==============================] - 45s 73ms/step - loss: 0.5669 - accuracy: 0.7228 - val_loss: 0.5274 - val_accuracy: 0.7554\n",
      "Epoch 23/50\n",
      "612/612 [==============================] - 51s 84ms/step - loss: 0.5611 - accuracy: 0.7265 - val_loss: 0.5583 - val_accuracy: 0.7394\n",
      "Epoch 24/50\n",
      "612/612 [==============================] - 50s 81ms/step - loss: 0.5573 - accuracy: 0.7331 - val_loss: 0.6490 - val_accuracy: 0.6271\n",
      "Epoch 25/50\n",
      "612/612 [==============================] - 50s 82ms/step - loss: 0.5935 - accuracy: 0.6985 - val_loss: 0.5987 - val_accuracy: 0.6856\n",
      "Epoch 26/50\n",
      "612/612 [==============================] - 50s 81ms/step - loss: 0.5716 - accuracy: 0.7189 - val_loss: 0.5229 - val_accuracy: 0.7629\n",
      "Epoch 27/50\n",
      "612/612 [==============================] - 50s 82ms/step - loss: 0.5368 - accuracy: 0.7573 - val_loss: 0.5146 - val_accuracy: 0.7621\n",
      "Epoch 28/50\n",
      "612/612 [==============================] - 50s 81ms/step - loss: 0.5602 - accuracy: 0.7318 - val_loss: 0.5745 - val_accuracy: 0.7104\n",
      "Epoch 29/50\n",
      "612/612 [==============================] - 50s 82ms/step - loss: 0.5844 - accuracy: 0.7037 - val_loss: 0.5755 - val_accuracy: 0.7145\n",
      "Epoch 30/50\n",
      "612/612 [==============================] - 52s 85ms/step - loss: 0.5599 - accuracy: 0.7258 - val_loss: 0.5485 - val_accuracy: 0.7327\n",
      "Epoch 31/50\n",
      "612/612 [==============================] - 47s 77ms/step - loss: 0.5566 - accuracy: 0.7385 - val_loss: 0.6027 - val_accuracy: 0.6838\n",
      "Epoch 32/50\n",
      "612/612 [==============================] - 46s 75ms/step - loss: 0.5832 - accuracy: 0.7001 - val_loss: 0.6242 - val_accuracy: 0.6601\n",
      "Epoch 33/50\n",
      "612/612 [==============================] - 46s 76ms/step - loss: 0.5790 - accuracy: 0.7087 - val_loss: 0.6055 - val_accuracy: 0.6697\n",
      "Epoch 34/50\n",
      "612/612 [==============================] - 44s 72ms/step - loss: 0.5467 - accuracy: 0.7413 - val_loss: 0.5164 - val_accuracy: 0.7648\n",
      "Epoch 35/50\n",
      "612/612 [==============================] - 44s 72ms/step - loss: 0.5810 - accuracy: 0.7017 - val_loss: 0.6033 - val_accuracy: 0.6789\n",
      "Epoch 36/50\n",
      "612/612 [==============================] - 44s 73ms/step - loss: 0.5905 - accuracy: 0.6971 - val_loss: 0.6829 - val_accuracy: 0.5651\n",
      "Epoch 37/50\n",
      "612/612 [==============================] - 45s 73ms/step - loss: 0.5688 - accuracy: 0.7219 - val_loss: 0.5335 - val_accuracy: 0.7462\n",
      "Epoch 38/50\n",
      "612/612 [==============================] - 45s 74ms/step - loss: 0.5252 - accuracy: 0.7602 - val_loss: 0.4898 - val_accuracy: 0.7781\n",
      "Epoch 39/50\n",
      "612/612 [==============================] - 47s 76ms/step - loss: 0.5123 - accuracy: 0.7734 - val_loss: 0.4937 - val_accuracy: 0.7744\n",
      "Epoch 40/50\n",
      "612/612 [==============================] - 44s 72ms/step - loss: 0.5214 - accuracy: 0.7770 - val_loss: 0.5950 - val_accuracy: 0.6852\n",
      "Epoch 41/50\n",
      "612/612 [==============================] - 48s 79ms/step - loss: 0.5407 - accuracy: 0.7500 - val_loss: 0.4856 - val_accuracy: 0.7920\n",
      "Epoch 42/50\n",
      "612/612 [==============================] - 48s 79ms/step - loss: 0.5525 - accuracy: 0.7461 - val_loss: 0.5394 - val_accuracy: 0.7413\n",
      "Epoch 43/50\n",
      "612/612 [==============================] - 45s 73ms/step - loss: 0.5305 - accuracy: 0.7523 - val_loss: 0.5020 - val_accuracy: 0.7932\n",
      "Epoch 44/50\n",
      "612/612 [==============================] - 49s 81ms/step - loss: 0.5667 - accuracy: 0.7215 - val_loss: 0.5607 - val_accuracy: 0.7286\n",
      "Epoch 45/50\n",
      "612/612 [==============================] - 51s 84ms/step - loss: 0.5540 - accuracy: 0.7382 - val_loss: 0.5378 - val_accuracy: 0.7460\n",
      "Epoch 46/50\n",
      "612/612 [==============================] - 54s 88ms/step - loss: 0.5462 - accuracy: 0.7487 - val_loss: 0.5288 - val_accuracy: 0.7517\n",
      "Epoch 47/50\n",
      "612/612 [==============================] - 45s 74ms/step - loss: 0.5675 - accuracy: 0.7239 - val_loss: 0.5529 - val_accuracy: 0.7388\n",
      "Epoch 48/50\n",
      "612/612 [==============================] - 42s 68ms/step - loss: 0.5268 - accuracy: 0.7638 - val_loss: 0.5148 - val_accuracy: 0.7664\n",
      "Epoch 49/50\n",
      "612/612 [==============================] - 42s 68ms/step - loss: 0.5135 - accuracy: 0.7755 - val_loss: 0.4627 - val_accuracy: 0.8030\n",
      "Epoch 50/50\n",
      "612/612 [==============================] - 42s 68ms/step - loss: 0.4979 - accuracy: 0.7904 - val_loss: 0.5514 - val_accuracy: 0.7312\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout\n",
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    GRU(units=64, return_sequences=True,  input_shape=(X_train.shape[1], 1)),\n",
    "    Dropout(0.2),\n",
    "    GRU(units=64),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f86c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 4s 16ms/step\n",
      "Accuracy: 0.7312333810595214\n",
      "Precision: 0.8562664329535495\n",
      "Recall: 0.459332393041843\n",
      "F1 Score: 0.5979192166462668\n",
      "ROC AUC: 0.8025110412758005\n",
      "Specificity: 0.940622737146995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model on test set\n",
    "accuracy = accuracy_score(y_test, y_pred.round())\n",
    "precision = precision_score(y_test, y_pred.round())\n",
    "recall = recall_score(y_test, y_pred.round())\n",
    "f1 = f1_score(y_test, y_pred.round())\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred.round()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d42f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "612/612 [==============================] - 73s 104ms/step - loss: 0.6053 - accuracy: 0.6809 - val_loss: 0.5968 - val_accuracy: 0.6916\n",
      "Epoch 2/50\n",
      "612/612 [==============================] - 62s 102ms/step - loss: 0.5761 - accuracy: 0.7139 - val_loss: 0.5633 - val_accuracy: 0.7247\n",
      "Epoch 3/50\n",
      "612/612 [==============================] - 67s 110ms/step - loss: 0.5560 - accuracy: 0.7302 - val_loss: 0.5356 - val_accuracy: 0.7466\n",
      "Epoch 4/50\n",
      "612/612 [==============================] - 67s 109ms/step - loss: 0.5355 - accuracy: 0.7462 - val_loss: 0.5296 - val_accuracy: 0.7488\n",
      "Epoch 5/50\n",
      "612/612 [==============================] - 71s 116ms/step - loss: 0.5267 - accuracy: 0.7537 - val_loss: 0.5109 - val_accuracy: 0.7597\n",
      "Epoch 6/50\n",
      "612/612 [==============================] - 68s 111ms/step - loss: 0.5182 - accuracy: 0.7590 - val_loss: 0.5029 - val_accuracy: 0.7689\n",
      "Epoch 7/50\n",
      "612/612 [==============================] - 71s 115ms/step - loss: 0.5092 - accuracy: 0.7671 - val_loss: 0.5125 - val_accuracy: 0.7515\n",
      "Epoch 8/50\n",
      "612/612 [==============================] - 73s 120ms/step - loss: 0.5034 - accuracy: 0.7700 - val_loss: 0.5011 - val_accuracy: 0.7730\n",
      "Epoch 9/50\n",
      "612/612 [==============================] - 72s 118ms/step - loss: 0.5001 - accuracy: 0.7715 - val_loss: 0.4754 - val_accuracy: 0.7879\n",
      "Epoch 10/50\n",
      "612/612 [==============================] - 116s 189ms/step - loss: 0.4879 - accuracy: 0.7820 - val_loss: 0.4674 - val_accuracy: 0.7957\n",
      "Epoch 11/50\n",
      "612/612 [==============================] - 77s 125ms/step - loss: 0.4733 - accuracy: 0.7884 - val_loss: 0.4613 - val_accuracy: 0.8000\n",
      "Epoch 12/50\n",
      "612/612 [==============================] - 60s 99ms/step - loss: 0.5517 - accuracy: 0.7267 - val_loss: 0.5361 - val_accuracy: 0.7380\n",
      "Epoch 13/50\n",
      "612/612 [==============================] - 66s 108ms/step - loss: 0.5161 - accuracy: 0.7570 - val_loss: 0.5262 - val_accuracy: 0.7574\n",
      "Epoch 14/50\n",
      "612/612 [==============================] - 65s 106ms/step - loss: 0.4866 - accuracy: 0.7784 - val_loss: 0.4782 - val_accuracy: 0.7920\n",
      "Epoch 15/50\n",
      "612/612 [==============================] - 62s 101ms/step - loss: 0.4632 - accuracy: 0.7938 - val_loss: 0.4524 - val_accuracy: 0.8088\n",
      "Epoch 16/50\n",
      "612/612 [==============================] - 64s 104ms/step - loss: 0.4554 - accuracy: 0.8013 - val_loss: 0.4398 - val_accuracy: 0.8112\n",
      "Epoch 17/50\n",
      "612/612 [==============================] - 63s 103ms/step - loss: 0.4337 - accuracy: 0.8138 - val_loss: 0.4408 - val_accuracy: 0.8090\n",
      "Epoch 18/50\n",
      "612/612 [==============================] - 63s 103ms/step - loss: 0.4267 - accuracy: 0.8190 - val_loss: 0.4226 - val_accuracy: 0.8276\n",
      "Epoch 19/50\n",
      "612/612 [==============================] - 2694s 4s/step - loss: 0.4153 - accuracy: 0.8249 - val_loss: 0.4147 - val_accuracy: 0.8255\n",
      "Epoch 20/50\n",
      "612/612 [==============================] - 2537s 4s/step - loss: 0.4062 - accuracy: 0.8305 - val_loss: 0.4063 - val_accuracy: 0.8319\n",
      "Epoch 21/50\n",
      "612/612 [==============================] - 59s 96ms/step - loss: 0.4010 - accuracy: 0.8351 - val_loss: 0.4081 - val_accuracy: 0.8341\n",
      "Epoch 22/50\n",
      "612/612 [==============================] - 67s 110ms/step - loss: 0.3962 - accuracy: 0.8351 - val_loss: 0.3922 - val_accuracy: 0.8419\n",
      "Epoch 23/50\n",
      "612/612 [==============================] - 68s 112ms/step - loss: 0.3869 - accuracy: 0.8401 - val_loss: 0.4042 - val_accuracy: 0.8343\n",
      "Epoch 24/50\n",
      "612/612 [==============================] - 68s 111ms/step - loss: 0.3854 - accuracy: 0.8413 - val_loss: 0.4098 - val_accuracy: 0.8292\n",
      "Epoch 25/50\n",
      "612/612 [==============================] - 80s 131ms/step - loss: 0.3775 - accuracy: 0.8471 - val_loss: 0.3967 - val_accuracy: 0.8407\n",
      "Epoch 26/50\n",
      "612/612 [==============================] - 69s 113ms/step - loss: 0.3786 - accuracy: 0.8462 - val_loss: 0.3975 - val_accuracy: 0.8384\n",
      "Epoch 27/50\n",
      "612/612 [==============================] - 71s 117ms/step - loss: 0.3856 - accuracy: 0.8426 - val_loss: 0.4074 - val_accuracy: 0.8292\n",
      "Epoch 28/50\n",
      "612/612 [==============================] - 69s 113ms/step - loss: 0.3660 - accuracy: 0.8530 - val_loss: 0.3995 - val_accuracy: 0.8394\n",
      "Epoch 29/50\n",
      "612/612 [==============================] - 69s 113ms/step - loss: 0.3636 - accuracy: 0.8533 - val_loss: 0.3968 - val_accuracy: 0.8376\n",
      "Epoch 30/50\n",
      "612/612 [==============================] - 67s 110ms/step - loss: 0.3549 - accuracy: 0.8579 - val_loss: 0.3964 - val_accuracy: 0.8452\n",
      "Epoch 31/50\n",
      "612/612 [==============================] - 60s 98ms/step - loss: 0.3535 - accuracy: 0.8584 - val_loss: 0.3981 - val_accuracy: 0.8417\n",
      "Epoch 32/50\n",
      "612/612 [==============================] - 54s 88ms/step - loss: 0.3466 - accuracy: 0.8618 - val_loss: 0.4137 - val_accuracy: 0.8368\n",
      "Epoch 33/50\n",
      "612/612 [==============================] - 51s 83ms/step - loss: 0.3446 - accuracy: 0.8642 - val_loss: 0.4008 - val_accuracy: 0.8419\n",
      "Epoch 34/50\n",
      "612/612 [==============================] - 49s 80ms/step - loss: 0.3400 - accuracy: 0.8640 - val_loss: 0.4071 - val_accuracy: 0.8429\n",
      "Epoch 35/50\n",
      "612/612 [==============================] - 49s 80ms/step - loss: 0.3369 - accuracy: 0.8671 - val_loss: 0.4009 - val_accuracy: 0.8437\n",
      "Epoch 36/50\n",
      "612/612 [==============================] - 48s 78ms/step - loss: 0.3352 - accuracy: 0.8668 - val_loss: 0.4011 - val_accuracy: 0.8441\n",
      "Epoch 37/50\n",
      "612/612 [==============================] - 49s 80ms/step - loss: 0.3261 - accuracy: 0.8719 - val_loss: 0.4082 - val_accuracy: 0.8368\n",
      "Epoch 38/50\n",
      "612/612 [==============================] - 47s 77ms/step - loss: 0.3306 - accuracy: 0.8700 - val_loss: 0.4064 - val_accuracy: 0.8427\n",
      "Epoch 39/50\n",
      "612/612 [==============================] - 48s 79ms/step - loss: 0.3227 - accuracy: 0.8711 - val_loss: 0.3982 - val_accuracy: 0.8400\n",
      "Epoch 40/50\n",
      "612/612 [==============================] - 48s 78ms/step - loss: 0.3185 - accuracy: 0.8735 - val_loss: 0.4092 - val_accuracy: 0.8400\n",
      "Epoch 41/50\n",
      "612/612 [==============================] - 49s 80ms/step - loss: 0.3180 - accuracy: 0.8753 - val_loss: 0.3989 - val_accuracy: 0.8441\n",
      "Epoch 42/50\n",
      "612/612 [==============================] - 48s 79ms/step - loss: 0.3124 - accuracy: 0.8776 - val_loss: 0.4079 - val_accuracy: 0.8476\n",
      "Epoch 43/50\n",
      "612/612 [==============================] - 49s 80ms/step - loss: 0.3087 - accuracy: 0.8801 - val_loss: 0.4060 - val_accuracy: 0.8386\n",
      "Epoch 44/50\n",
      "612/612 [==============================] - 47s 77ms/step - loss: 0.3104 - accuracy: 0.8775 - val_loss: 0.4090 - val_accuracy: 0.8415\n",
      "Epoch 45/50\n",
      "612/612 [==============================] - 47s 77ms/step - loss: 0.3001 - accuracy: 0.8819 - val_loss: 0.4116 - val_accuracy: 0.8423\n",
      "Epoch 46/50\n",
      "612/612 [==============================] - 48s 79ms/step - loss: 0.2957 - accuracy: 0.8849 - val_loss: 0.4165 - val_accuracy: 0.8429\n",
      "Epoch 47/50\n",
      "612/612 [==============================] - 49s 80ms/step - loss: 0.2949 - accuracy: 0.8844 - val_loss: 0.4228 - val_accuracy: 0.8398\n",
      "Epoch 48/50\n",
      "612/612 [==============================] - 50s 82ms/step - loss: 0.2893 - accuracy: 0.8884 - val_loss: 0.4260 - val_accuracy: 0.8362\n",
      "Epoch 49/50\n",
      "612/612 [==============================] - 56s 92ms/step - loss: 0.2858 - accuracy: 0.8871 - val_loss: 0.4511 - val_accuracy: 0.8360\n",
      "Epoch 50/50\n",
      "612/612 [==============================] - 55s 90ms/step - loss: 0.2861 - accuracy: 0.8882 - val_loss: 0.4405 - val_accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    Dropout(0.2),\n",
    "    GRU(units=64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(units=64),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349a8510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 6s 26ms/step\n",
      "Accuracy: 0.8382\n",
      "Precision: 0.8687\n",
      "Recall: 0.7400\n",
      "F1 score: 0.7992\n",
      "AUC: 0.8668\n",
      "Specificity: 0.9138\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "acc = (y_pred_binary.flatten() == y_test).mean()\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 score: {f1:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7620f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "612/612 [==============================] - 217s 336ms/step - loss: 0.6101 - accuracy: 0.6775 - precision: 0.6557 - recall: 0.5511 - auc: 0.7215 - specificity: 0.7736 - val_loss: 0.5840 - val_accuracy: 0.7014 - val_precision: 0.6701 - val_recall: 0.6178 - val_auc: 0.7518 - val_specificity: 0.7660\n",
      "Epoch 2/50\n",
      "612/612 [==============================] - 204s 333ms/step - loss: 0.5769 - accuracy: 0.7138 - precision: 0.7037 - recall: 0.5955 - auc: 0.7590 - specificity: 0.8036 - val_loss: 0.5666 - val_accuracy: 0.7237 - val_precision: 0.7036 - val_recall: 0.6305 - val_auc: 0.7714 - val_specificity: 0.7943\n",
      "Epoch 3/50\n",
      "612/612 [==============================] - 198s 324ms/step - loss: 0.5587 - accuracy: 0.7298 - precision: 0.7227 - recall: 0.6187 - auc: 0.7755 - specificity: 0.8155 - val_loss: 0.5520 - val_accuracy: 0.7269 - val_precision: 0.7716 - val_recall: 0.5289 - val_auc: 0.7876 - val_specificity: 0.8797\n",
      "Epoch 4/50\n",
      "612/612 [==============================] - 196s 320ms/step - loss: 0.5444 - accuracy: 0.7404 - precision: 0.7412 - recall: 0.6235 - auc: 0.7888 - specificity: 0.8301 - val_loss: 0.5323 - val_accuracy: 0.7556 - val_precision: 0.8343 - val_recall: 0.5468 - val_auc: 0.8078 - val_specificity: 0.9159\n",
      "Epoch 5/50\n",
      "612/612 [==============================] - 196s 321ms/step - loss: 0.5327 - accuracy: 0.7453 - precision: 0.7521 - recall: 0.6218 - auc: 0.7972 - specificity: 0.8416 - val_loss: 0.5264 - val_accuracy: 0.7503 - val_precision: 0.7745 - val_recall: 0.6008 - val_auc: 0.8040 - val_specificity: 0.8652\n",
      "Epoch 6/50\n",
      "612/612 [==============================] - 195s 319ms/step - loss: 0.5176 - accuracy: 0.7561 - precision: 0.7725 - recall: 0.6260 - auc: 0.8091 - specificity: 0.8556 - val_loss: 0.4942 - val_accuracy: 0.7777 - val_precision: 0.7925 - val_recall: 0.6624 - val_auc: 0.8268 - val_specificity: 0.8653\n",
      "Epoch 7/50\n",
      "612/612 [==============================] - 195s 319ms/step - loss: 0.5141 - accuracy: 0.7601 - precision: 0.7799 - recall: 0.6281 - auc: 0.8097 - specificity: 0.8634 - val_loss: 0.4796 - val_accuracy: 0.7861 - val_precision: 0.8185 - val_recall: 0.6530 - val_auc: 0.8340 - val_specificity: 0.8872\n",
      "Epoch 8/50\n",
      "612/612 [==============================] - 196s 320ms/step - loss: 0.4894 - accuracy: 0.7786 - precision: 0.8056 - recall: 0.6498 - auc: 0.8268 - specificity: 0.8797 - val_loss: 0.4787 - val_accuracy: 0.7838 - val_precision: 0.7946 - val_recall: 0.6784 - val_auc: 0.8366 - val_specificity: 0.8625\n",
      "Epoch 9/50\n",
      "612/612 [==============================] - 196s 320ms/step - loss: 0.4817 - accuracy: 0.7831 - precision: 0.8157 - recall: 0.6503 - auc: 0.8309 - specificity: 0.8844 - val_loss: 0.4692 - val_accuracy: 0.7953 - val_precision: 0.8017 - val_recall: 0.7033 - val_auc: 0.8422 - val_specificity: 0.8655\n",
      "Epoch 10/50\n",
      "612/612 [==============================] - 198s 323ms/step - loss: 0.4704 - accuracy: 0.7932 - precision: 0.8321 - recall: 0.6596 - auc: 0.8375 - specificity: 0.8971 - val_loss: 0.5019 - val_accuracy: 0.7705 - val_precision: 0.7893 - val_recall: 0.6446 - val_auc: 0.8192 - val_specificity: 0.8658\n",
      "Epoch 11/50\n",
      "612/612 [==============================] - 195s 318ms/step - loss: 0.4602 - accuracy: 0.7996 - precision: 0.8405 - recall: 0.6681 - auc: 0.8415 - specificity: 0.9010 - val_loss: 0.4473 - val_accuracy: 0.8014 - val_precision: 0.8677 - val_recall: 0.6413 - val_auc: 0.8528 - val_specificity: 0.9242\n",
      "Epoch 12/50\n",
      "612/612 [==============================] - 194s 318ms/step - loss: 0.4493 - accuracy: 0.8070 - precision: 0.8502 - recall: 0.6777 - auc: 0.8500 - specificity: 0.9078 - val_loss: 0.4539 - val_accuracy: 0.8000 - val_precision: 0.8772 - val_recall: 0.6281 - val_auc: 0.8483 - val_specificity: 0.9315\n",
      "Epoch 13/50\n",
      "612/612 [==============================] - 195s 319ms/step - loss: 0.4408 - accuracy: 0.8105 - precision: 0.8536 - recall: 0.6833 - auc: 0.8539 - specificity: 0.9098 - val_loss: 0.4548 - val_accuracy: 0.8051 - val_precision: 0.8601 - val_recall: 0.6591 - val_auc: 0.8432 - val_specificity: 0.9166\n",
      "Epoch 14/50\n",
      "612/612 [==============================] - 196s 321ms/step - loss: 0.4310 - accuracy: 0.8183 - precision: 0.8659 - recall: 0.6911 - auc: 0.8574 - specificity: 0.9151 - val_loss: 0.4383 - val_accuracy: 0.8128 - val_precision: 0.8727 - val_recall: 0.6671 - val_auc: 0.8572 - val_specificity: 0.9235\n",
      "Epoch 15/50\n",
      "612/612 [==============================] - 196s 320ms/step - loss: 0.4223 - accuracy: 0.8223 - precision: 0.8747 - recall: 0.6924 - auc: 0.8635 - specificity: 0.9232 - val_loss: 0.4285 - val_accuracy: 0.8204 - val_precision: 0.9144 - val_recall: 0.6479 - val_auc: 0.8611 - val_specificity: 0.9531\n",
      "Epoch 16/50\n",
      "612/612 [==============================] - 195s 319ms/step - loss: 0.4097 - accuracy: 0.8297 - precision: 0.8854 - recall: 0.7010 - auc: 0.8695 - specificity: 0.9299 - val_loss: 0.4612 - val_accuracy: 0.7953 - val_precision: 0.9164 - val_recall: 0.5825 - val_auc: 0.8484 - val_specificity: 0.9593\n",
      "Epoch 17/50\n",
      "612/612 [==============================] - 194s 317ms/step - loss: 0.4051 - accuracy: 0.8297 - precision: 0.8837 - recall: 0.7027 - auc: 0.8727 - specificity: 0.9266 - val_loss: 0.4433 - val_accuracy: 0.8085 - val_precision: 0.8448 - val_recall: 0.6859 - val_auc: 0.8555 - val_specificity: 0.9020\n",
      "Epoch 18/50\n",
      "612/612 [==============================] - 196s 320ms/step - loss: 0.4168 - accuracy: 0.8246 - precision: 0.8710 - recall: 0.7026 - auc: 0.8672 - specificity: 0.9197 - val_loss: 0.4091 - val_accuracy: 0.8270 - val_precision: 0.8894 - val_recall: 0.6878 - val_auc: 0.8633 - val_specificity: 0.9335\n",
      "Epoch 19/50\n",
      "612/612 [==============================] - 195s 319ms/step - loss: 0.3880 - accuracy: 0.8396 - precision: 0.9002 - recall: 0.7116 - auc: 0.8808 - specificity: 0.9375 - val_loss: 0.4243 - val_accuracy: 0.8171 - val_precision: 0.8612 - val_recall: 0.6911 - val_auc: 0.8572 - val_specificity: 0.9150\n",
      "Epoch 20/50\n",
      "612/612 [==============================] - 193s 316ms/step - loss: 0.3958 - accuracy: 0.8371 - precision: 0.8953 - recall: 0.7101 - auc: 0.8755 - specificity: 0.9360 - val_loss: 0.4031 - val_accuracy: 0.8382 - val_precision: 0.8906 - val_recall: 0.7160 - val_auc: 0.8690 - val_specificity: 0.9316\n",
      "Epoch 21/50\n",
      "612/612 [==============================] - 194s 316ms/step - loss: 0.3747 - accuracy: 0.8464 - precision: 0.9039 - recall: 0.7254 - auc: 0.8873 - specificity: 0.9394 - val_loss: 0.4103 - val_accuracy: 0.8292 - val_precision: 0.9083 - val_recall: 0.6756 - val_auc: 0.8637 - val_specificity: 0.9467\n",
      "Epoch 22/50\n",
      "612/612 [==============================] - 195s 318ms/step - loss: 0.3638 - accuracy: 0.8520 - precision: 0.9141 - recall: 0.7299 - auc: 0.8926 - specificity: 0.9471 - val_loss: 0.3989 - val_accuracy: 0.8343 - val_precision: 0.9118 - val_recall: 0.6855 - val_auc: 0.8706 - val_specificity: 0.9475\n",
      "Epoch 23/50\n",
      "612/612 [==============================] - 197s 322ms/step - loss: 0.3557 - accuracy: 0.8574 - precision: 0.9207 - recall: 0.7370 - auc: 0.8953 - specificity: 0.9506 - val_loss: 0.4177 - val_accuracy: 0.8265 - val_precision: 0.9050 - val_recall: 0.6718 - val_auc: 0.8603 - val_specificity: 0.9459\n",
      "Epoch 24/50\n",
      "612/612 [==============================] - 195s 318ms/step - loss: 0.3519 - accuracy: 0.8589 - precision: 0.9209 - recall: 0.7405 - auc: 0.8978 - specificity: 0.9513 - val_loss: 0.4106 - val_accuracy: 0.8313 - val_precision: 0.9125 - val_recall: 0.6770 - val_auc: 0.8600 - val_specificity: 0.9496\n",
      "Epoch 25/50\n",
      "612/612 [==============================] - 195s 319ms/step - loss: 0.3436 - accuracy: 0.8630 - precision: 0.9217 - recall: 0.7500 - auc: 0.9046 - specificity: 0.9486 - val_loss: 0.4121 - val_accuracy: 0.8345 - val_precision: 0.9124 - val_recall: 0.6855 - val_auc: 0.8654 - val_specificity: 0.9489\n",
      "Epoch 26/50\n",
      "612/612 [==============================] - 197s 321ms/step - loss: 0.3357 - accuracy: 0.8666 - precision: 0.9274 - recall: 0.7537 - auc: 0.9080 - specificity: 0.9524 - val_loss: 0.4096 - val_accuracy: 0.8345 - val_precision: 0.9098 - val_recall: 0.6878 - val_auc: 0.8621 - val_specificity: 0.9476\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612/612 [==============================] - 193s 316ms/step - loss: 0.3302 - accuracy: 0.8682 - precision: 0.9283 - recall: 0.7567 - auc: 0.9112 - specificity: 0.9545 - val_loss: 0.4144 - val_accuracy: 0.8317 - val_precision: 0.8952 - val_recall: 0.6944 - val_auc: 0.8553 - val_specificity: 0.9358\n",
      "Epoch 28/50\n",
      "612/612 [==============================] - 197s 321ms/step - loss: 0.3209 - accuracy: 0.8736 - precision: 0.9336 - recall: 0.7650 - auc: 0.9150 - specificity: 0.9563 - val_loss: 0.4291 - val_accuracy: 0.8313 - val_precision: 0.8593 - val_recall: 0.7320 - val_auc: 0.8621 - val_specificity: 0.9073\n",
      "Epoch 29/50\n",
      "612/612 [==============================] - 196s 320ms/step - loss: 0.3166 - accuracy: 0.8741 - precision: 0.9337 - recall: 0.7662 - auc: 0.9180 - specificity: 0.9557 - val_loss: 0.4375 - val_accuracy: 0.8296 - val_precision: 0.8710 - val_recall: 0.7142 - val_auc: 0.8618 - val_specificity: 0.9181\n",
      "Epoch 30/50\n",
      "612/612 [==============================] - 194s 318ms/step - loss: 0.3069 - accuracy: 0.8768 - precision: 0.9333 - recall: 0.7732 - auc: 0.9231 - specificity: 0.9560 - val_loss: 0.4454 - val_accuracy: 0.8247 - val_precision: 0.8692 - val_recall: 0.7029 - val_auc: 0.8590 - val_specificity: 0.9192\n",
      "Epoch 31/50\n",
      "612/612 [==============================] - 196s 320ms/step - loss: 0.3155 - accuracy: 0.8731 - precision: 0.9253 - recall: 0.7717 - auc: 0.9205 - specificity: 0.9514 - val_loss: 0.4475 - val_accuracy: 0.8210 - val_precision: 0.8220 - val_recall: 0.7513 - val_auc: 0.8632 - val_specificity: 0.8749\n",
      "Epoch 32/50\n",
      "612/612 [==============================] - 196s 321ms/step - loss: 0.3013 - accuracy: 0.8812 - precision: 0.9353 - recall: 0.7822 - auc: 0.9272 - specificity: 0.9582 - val_loss: 0.4349 - val_accuracy: 0.8317 - val_precision: 0.8709 - val_recall: 0.7198 - val_auc: 0.8601 - val_specificity: 0.9173\n",
      "Epoch 33/50\n",
      "612/612 [==============================] - 196s 320ms/step - loss: 0.2936 - accuracy: 0.8822 - precision: 0.9350 - recall: 0.7849 - auc: 0.9310 - specificity: 0.9573 - val_loss: 0.4484 - val_accuracy: 0.8339 - val_precision: 0.8587 - val_recall: 0.7400 - val_auc: 0.8648 - val_specificity: 0.9058\n",
      "Epoch 34/50\n",
      "612/612 [==============================] - 197s 322ms/step - loss: 0.2854 - accuracy: 0.8862 - precision: 0.9364 - recall: 0.7932 - auc: 0.9345 - specificity: 0.9580 - val_loss: 0.4434 - val_accuracy: 0.8329 - val_precision: 0.8960 - val_recall: 0.6968 - val_auc: 0.8618 - val_specificity: 0.9371\n",
      "Epoch 35/50\n",
      "612/612 [==============================] - 196s 320ms/step - loss: 0.2809 - accuracy: 0.8889 - precision: 0.9353 - recall: 0.8010 - auc: 0.9382 - specificity: 0.9567 - val_loss: 0.4359 - val_accuracy: 0.8323 - val_precision: 0.8719 - val_recall: 0.7203 - val_auc: 0.8633 - val_specificity: 0.9177\n",
      "Epoch 36/50\n",
      "612/612 [==============================] - 196s 320ms/step - loss: 0.2794 - accuracy: 0.8895 - precision: 0.9385 - recall: 0.7993 - auc: 0.9379 - specificity: 0.9596 - val_loss: 0.4445 - val_accuracy: 0.8290 - val_precision: 0.8645 - val_recall: 0.7198 - val_auc: 0.8656 - val_specificity: 0.9125\n",
      "Epoch 37/50\n",
      "612/612 [==============================] - 197s 323ms/step - loss: 0.2738 - accuracy: 0.8913 - precision: 0.9404 - recall: 0.8019 - auc: 0.9412 - specificity: 0.9607 - val_loss: 0.4577 - val_accuracy: 0.8261 - val_precision: 0.8438 - val_recall: 0.7367 - val_auc: 0.8633 - val_specificity: 0.8950\n",
      "Epoch 38/50\n",
      "612/612 [==============================] - 197s 322ms/step - loss: 0.2678 - accuracy: 0.8941 - precision: 0.9362 - recall: 0.8129 - auc: 0.9436 - specificity: 0.9567 - val_loss: 0.4639 - val_accuracy: 0.8231 - val_precision: 0.8356 - val_recall: 0.7386 - val_auc: 0.8575 - val_specificity: 0.8880\n",
      "Epoch 39/50\n",
      "612/612 [==============================] - 197s 322ms/step - loss: 0.2634 - accuracy: 0.8962 - precision: 0.9417 - recall: 0.8127 - auc: 0.9457 - specificity: 0.9591 - val_loss: 0.4676 - val_accuracy: 0.8208 - val_precision: 0.8315 - val_recall: 0.7377 - val_auc: 0.8593 - val_specificity: 0.8853\n",
      "Epoch 40/50\n",
      "612/612 [==============================] - 197s 321ms/step - loss: 0.3399 - accuracy: 0.8577 - precision: 0.8999 - recall: 0.7586 - auc: 0.9151 - specificity: 0.9330 - val_loss: 0.4442 - val_accuracy: 0.8347 - val_precision: 0.8771 - val_recall: 0.7212 - val_auc: 0.8580 - val_specificity: 0.9210\n",
      "Epoch 41/50\n",
      "612/612 [==============================] - 198s 323ms/step - loss: 0.2730 - accuracy: 0.8904 - precision: 0.9311 - recall: 0.8089 - auc: 0.9423 - specificity: 0.9524 - val_loss: 0.4719 - val_accuracy: 0.8296 - val_precision: 0.8497 - val_recall: 0.7391 - val_auc: 0.8606 - val_specificity: 0.8983\n",
      "Epoch 42/50\n",
      "612/612 [==============================] - 197s 322ms/step - loss: 0.2488 - accuracy: 0.9010 - precision: 0.9431 - recall: 0.8231 - auc: 0.9515 - specificity: 0.9614 - val_loss: 0.4719 - val_accuracy: 0.8280 - val_precision: 0.8435 - val_recall: 0.7424 - val_auc: 0.8607 - val_specificity: 0.8938\n",
      "Epoch 43/50\n",
      "612/612 [==============================] - 198s 323ms/step - loss: 0.2472 - accuracy: 0.9020 - precision: 0.9410 - recall: 0.8275 - auc: 0.9527 - specificity: 0.9579 - val_loss: 0.4875 - val_accuracy: 0.8202 - val_precision: 0.8226 - val_recall: 0.7480 - val_auc: 0.8582 - val_specificity: 0.8747\n",
      "Epoch 44/50\n",
      "612/612 [==============================] - 198s 324ms/step - loss: 0.2405 - accuracy: 0.9034 - precision: 0.9369 - recall: 0.8350 - auc: 0.9566 - specificity: 0.9564 - val_loss: 0.4861 - val_accuracy: 0.8251 - val_precision: 0.8597 - val_recall: 0.7146 - val_auc: 0.8527 - val_specificity: 0.9097\n",
      "Epoch 45/50\n",
      "612/612 [==============================] - 198s 324ms/step - loss: 0.2338 - accuracy: 0.9060 - precision: 0.9386 - recall: 0.8397 - auc: 0.9589 - specificity: 0.9559 - val_loss: 0.4916 - val_accuracy: 0.8143 - val_precision: 0.8282 - val_recall: 0.7231 - val_auc: 0.8551 - val_specificity: 0.8847\n",
      "Epoch 46/50\n",
      "612/612 [==============================] - 197s 323ms/step - loss: 0.2486 - accuracy: 0.8981 - precision: 0.9278 - recall: 0.8315 - auc: 0.9547 - specificity: 0.9495 - val_loss: 0.4858 - val_accuracy: 0.8202 - val_precision: 0.8323 - val_recall: 0.7348 - val_auc: 0.8612 - val_specificity: 0.8864\n",
      "Epoch 47/50\n",
      "612/612 [==============================] - 200s 327ms/step - loss: 0.2365 - accuracy: 0.9050 - precision: 0.9344 - recall: 0.8416 - auc: 0.9591 - specificity: 0.9539 - val_loss: 0.4910 - val_accuracy: 0.8153 - val_precision: 0.8235 - val_recall: 0.7325 - val_auc: 0.8529 - val_specificity: 0.8784\n",
      "Epoch 48/50\n",
      "612/612 [==============================] - 196s 321ms/step - loss: 0.2407 - accuracy: 0.9041 - precision: 0.9330 - recall: 0.8408 - auc: 0.9573 - specificity: 0.9536 - val_loss: 0.4882 - val_accuracy: 0.8233 - val_precision: 0.8322 - val_recall: 0.7438 - val_auc: 0.8607 - val_specificity: 0.8854\n",
      "Epoch 49/50\n",
      "612/612 [==============================] - 198s 324ms/step - loss: 0.2282 - accuracy: 0.9081 - precision: 0.9351 - recall: 0.8486 - auc: 0.9615 - specificity: 0.9545 - val_loss: 0.4940 - val_accuracy: 0.8300 - val_precision: 0.8678 - val_recall: 0.7189 - val_auc: 0.8600 - val_specificity: 0.9153\n",
      "Epoch 50/50\n",
      "612/612 [==============================] - 196s 321ms/step - loss: 0.2300 - accuracy: 0.9068 - precision: 0.9323 - recall: 0.8481 - auc: 0.9611 - specificity: 0.9525 - val_loss: 0.4913 - val_accuracy: 0.8188 - val_precision: 0.8334 - val_recall: 0.7292 - val_auc: 0.8538 - val_specificity: 0.8885\n",
      "153/153 [==============================] - 18s 105ms/step\n",
      "Loss:  0.491291880607605\n",
      "Accuracy:  0.8187768459320068\n",
      "Precision:  0.8334228992462158\n",
      "Recall:  0.7291960716247559\n",
      "AUC:  0.8537800908088684\n",
      "Specificity:  0.888481080532074\n",
      "Confusion Matrix:  [[2452  310]\n",
      " [ 576 1551]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import keras.backend as K\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1-y_true)*(1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true)*y_pred, 0, 1)))\n",
    "    specificity = tn / (tn + fp + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(GRU(64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC(name='auc'), specificity])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "loss, accuracy, precision, recall, auc, specificity = model.evaluate(X_test, y_test, verbose=0)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print('Loss: ', loss)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('AUC: ', auc)\n",
    "print('Specificity: ', specificity)\n",
    "print('Confusion Matrix: ', conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "875690ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1...\n",
      "Epoch 1/5\n",
      "510/510 [==============================] - 168s 309ms/step - loss: 0.6423 - accuracy: 0.6307\n",
      "Epoch 2/5\n",
      "510/510 [==============================] - 157s 308ms/step - loss: 0.6080 - accuracy: 0.6787\n",
      "Epoch 3/5\n",
      "510/510 [==============================] - 158s 311ms/step - loss: 0.5681 - accuracy: 0.7238\n",
      "Epoch 4/5\n",
      "510/510 [==============================] - 154s 302ms/step - loss: 0.5430 - accuracy: 0.7403\n",
      "Epoch 5/5\n",
      "510/510 [==============================] - 154s 303ms/step - loss: 0.5270 - accuracy: 0.7539\n",
      "255/255 [==============================] - 27s 103ms/step\n",
      "Fold 2...\n",
      "Epoch 1/5\n",
      "510/510 [==============================] - 165s 310ms/step - loss: 0.6441 - accuracy: 0.6311\n",
      "Epoch 2/5\n",
      "510/510 [==============================] - 154s 301ms/step - loss: 0.5977 - accuracy: 0.6915\n",
      "Epoch 3/5\n",
      "510/510 [==============================] - 158s 310ms/step - loss: 0.5598 - accuracy: 0.7280\n",
      "Epoch 4/5\n",
      "510/510 [==============================] - 157s 308ms/step - loss: 0.5472 - accuracy: 0.7402\n",
      "Epoch 5/5\n",
      "510/510 [==============================] - 153s 301ms/step - loss: 0.5336 - accuracy: 0.7497\n",
      "255/255 [==============================] - 27s 100ms/step\n",
      "Fold 3...\n",
      "Epoch 1/5\n",
      "510/510 [==============================] - 166s 310ms/step - loss: 0.6440 - accuracy: 0.6355\n",
      "Epoch 2/5\n",
      "510/510 [==============================] - 152s 298ms/step - loss: 0.6064 - accuracy: 0.6825\n",
      "Epoch 3/5\n",
      "510/510 [==============================] - 155s 304ms/step - loss: 0.5642 - accuracy: 0.7251\n",
      "Epoch 4/5\n",
      "510/510 [==============================] - 158s 310ms/step - loss: 0.5464 - accuracy: 0.7394\n",
      "Epoch 5/5\n",
      "510/510 [==============================] - 152s 299ms/step - loss: 0.5282 - accuracy: 0.7547\n",
      "255/255 [==============================] - 25s 94ms/step\n",
      "Accuracy: 76.037% +/- 1.364%\n",
      "Precision: 78.673% +/- 5.215%\n",
      "Recall: 62.821% +/- 3.763%\n",
      "F1 Score: 69.586% +/- 0.317%\n",
      "ROC AUC: 81.339% +/- 1.027%\n",
      "ROC AUC: 78.990% +/- 0.000%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Dropout, Bidirectional\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Define evaluation metrics\n",
    "def evaluate(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    spec = recall_score(y_true, y_pred, pos_label=0)\n",
    "    return acc, prec, rec, f1, roc_auc, spec\n",
    "\n",
    "# Define GRU model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(256, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define number of folds and batch size\n",
    "n_folds = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "f1s = []\n",
    "roc_aucs = []\n",
    "specs = []\n",
    "\n",
    "# Create K-Fold cross-validation object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "# Perform cross-validation\n",
    "for i, (train, test) in enumerate(kf.split(X)):\n",
    "    print('Fold %d...' % (i+1))\n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    \n",
    "    # Create and fit model\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=5, verbose=1)\n",
    "    \n",
    "    # Evaluate model on test set and calculate metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred_binary)\n",
    "    prec = precision_score(y_test, y_pred_binary)\n",
    "    rec = recall_score(y_test, y_pred_binary)\n",
    "    f1 = f1_score(y_test, y_pred_binary)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n",
    "    spec = tn / (tn + fp)  \n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accs.append(acc)\n",
    "    precs.append(prec)\n",
    "    recs.append(rec)\n",
    "    f1s.append(f1)\n",
    "    roc_aucs.append(roc_auc)\n",
    "    specs.append(spec)\n",
    "\n",
    "# Calculate average and standard deviation of metrics across folds\n",
    "print('Accuracy: %.3f%% +/- %.3f%%' % (np.mean(accs)*100, np.std(accs)*100))\n",
    "print('Precision: %.3f%% +/- %.3f%%' % (np.mean(precs)*100, np.std(precs)*100))\n",
    "print('Recall: %.3f%% +/- %.3f%%' % (np.mean(recs)*100, np.std(recs)*100))\n",
    "print('F1 Score: %.3f%% +/- %.3f%%' % (np.mean(f1s)*100, np.std(f1s)*100))\n",
    "print('ROC AUC: %.3f%% +/- %.3f%%' % (np.mean(roc_aucs)*100, np.std(roc_aucs)*100))\n",
    "print('ROC AUC: %.3f%% +/- %.3f%%' % (np.mean(spec)*100, np.std(spec)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876f864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
