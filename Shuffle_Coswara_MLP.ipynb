{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1c15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('updated_coswara.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2650c66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc{1}</th>\n",
       "      <th>mfcc{2}</th>\n",
       "      <th>mfcc{3}</th>\n",
       "      <th>mfcc{4}</th>\n",
       "      <th>...</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>ddpJitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>aqpq5Shimmer</th>\n",
       "      <th>apq11Shimmer</th>\n",
       "      <th>ddaShimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039102</td>\n",
       "      <td>0.378903</td>\n",
       "      <td>786.823461</td>\n",
       "      <td>966.699650</td>\n",
       "      <td>1387.984940</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>-412.08945</td>\n",
       "      <td>126.752335</td>\n",
       "      <td>31.558170</td>\n",
       "      <td>18.483738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.212818</td>\n",
       "      <td>1.734878</td>\n",
       "      <td>0.097995</td>\n",
       "      <td>0.147546</td>\n",
       "      <td>0.206467</td>\n",
       "      <td>0.293984</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051812</td>\n",
       "      <td>0.436672</td>\n",
       "      <td>2219.820298</td>\n",
       "      <td>1874.652272</td>\n",
       "      <td>4134.754998</td>\n",
       "      <td>0.209401</td>\n",
       "      <td>-398.93295</td>\n",
       "      <td>50.929253</td>\n",
       "      <td>-17.480385</td>\n",
       "      <td>4.325164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.092139</td>\n",
       "      <td>0.283908</td>\n",
       "      <td>2.113383</td>\n",
       "      <td>0.137225</td>\n",
       "      <td>0.241707</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.411675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.238371</td>\n",
       "      <td>2002.598308</td>\n",
       "      <td>2058.223470</td>\n",
       "      <td>4324.443295</td>\n",
       "      <td>0.146994</td>\n",
       "      <td>-599.82200</td>\n",
       "      <td>40.048190</td>\n",
       "      <td>6.373952</td>\n",
       "      <td>13.369130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.022602</td>\n",
       "      <td>0.056016</td>\n",
       "      <td>0.134015</td>\n",
       "      <td>1.358899</td>\n",
       "      <td>0.061172</td>\n",
       "      <td>0.091838</td>\n",
       "      <td>0.164672</td>\n",
       "      <td>0.183516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.078437</td>\n",
       "      <td>0.242433</td>\n",
       "      <td>569.328347</td>\n",
       "      <td>506.956042</td>\n",
       "      <td>979.591497</td>\n",
       "      <td>0.035271</td>\n",
       "      <td>-423.74878</td>\n",
       "      <td>77.112580</td>\n",
       "      <td>-11.768955</td>\n",
       "      <td>6.080464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050445</td>\n",
       "      <td>0.070875</td>\n",
       "      <td>0.151334</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>1.673087</td>\n",
       "      <td>0.101366</td>\n",
       "      <td>0.093390</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.304098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072712</td>\n",
       "      <td>0.327475</td>\n",
       "      <td>1344.446613</td>\n",
       "      <td>1062.582139</td>\n",
       "      <td>2431.292693</td>\n",
       "      <td>0.089481</td>\n",
       "      <td>-327.21740</td>\n",
       "      <td>160.910540</td>\n",
       "      <td>-60.493977</td>\n",
       "      <td>-20.299130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.003605</td>\n",
       "      <td>0.085836</td>\n",
       "      <td>0.803849</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>0.054323</td>\n",
       "      <td>0.093975</td>\n",
       "      <td>0.107976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24123</th>\n",
       "      <td>0.056854</td>\n",
       "      <td>0.467262</td>\n",
       "      <td>2845.574993</td>\n",
       "      <td>2044.704050</td>\n",
       "      <td>5269.369989</td>\n",
       "      <td>0.235494</td>\n",
       "      <td>-388.71793</td>\n",
       "      <td>32.990032</td>\n",
       "      <td>-8.879510</td>\n",
       "      <td>20.782476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023928</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>0.071784</td>\n",
       "      <td>0.160208</td>\n",
       "      <td>1.659819</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>0.137543</td>\n",
       "      <td>0.331425</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24124</th>\n",
       "      <td>0.050149</td>\n",
       "      <td>0.267105</td>\n",
       "      <td>1199.737564</td>\n",
       "      <td>1636.413852</td>\n",
       "      <td>2392.060470</td>\n",
       "      <td>0.059086</td>\n",
       "      <td>-433.10117</td>\n",
       "      <td>58.741127</td>\n",
       "      <td>37.350792</td>\n",
       "      <td>51.354850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.419731</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.041864</td>\n",
       "      <td>0.054251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24125</th>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.525523</td>\n",
       "      <td>4996.270042</td>\n",
       "      <td>2579.510698</td>\n",
       "      <td>8270.021928</td>\n",
       "      <td>0.418827</td>\n",
       "      <td>-745.49110</td>\n",
       "      <td>-27.318123</td>\n",
       "      <td>-5.540486</td>\n",
       "      <td>16.303144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.050605</td>\n",
       "      <td>0.137923</td>\n",
       "      <td>1.255627</td>\n",
       "      <td>0.061327</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.183981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24126</th>\n",
       "      <td>0.031053</td>\n",
       "      <td>0.167581</td>\n",
       "      <td>1890.217497</td>\n",
       "      <td>2748.644250</td>\n",
       "      <td>5175.546000</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>-499.73740</td>\n",
       "      <td>32.764600</td>\n",
       "      <td>46.792103</td>\n",
       "      <td>40.457275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.016609</td>\n",
       "      <td>0.134111</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.023441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24127</th>\n",
       "      <td>0.024217</td>\n",
       "      <td>0.287329</td>\n",
       "      <td>1296.724388</td>\n",
       "      <td>1259.118567</td>\n",
       "      <td>2319.688878</td>\n",
       "      <td>0.079520</td>\n",
       "      <td>-457.49475</td>\n",
       "      <td>98.845490</td>\n",
       "      <td>-1.881587</td>\n",
       "      <td>40.956110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>0.013602</td>\n",
       "      <td>0.037466</td>\n",
       "      <td>0.106427</td>\n",
       "      <td>1.033221</td>\n",
       "      <td>0.047599</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>0.095882</td>\n",
       "      <td>0.142798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24128 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse  chroma_stft    spec_cent      spec_bw      rolloff       zcr  \\\n",
       "0      0.039102     0.378903   786.823461   966.699650  1387.984940  0.043870   \n",
       "1      0.051812     0.436672  2219.820298  1874.652272  4134.754998  0.209401   \n",
       "2      0.010413     0.238371  2002.598308  2058.223470  4324.443295  0.146994   \n",
       "3      0.078437     0.242433   569.328347   506.956042   979.591497  0.035271   \n",
       "4      0.072712     0.327475  1344.446613  1062.582139  2431.292693  0.089481   \n",
       "...         ...          ...          ...          ...          ...       ...   \n",
       "24123  0.056854     0.467262  2845.574993  2044.704050  5269.369989  0.235494   \n",
       "24124  0.050149     0.267105  1199.737564  1636.413852  2392.060470  0.059086   \n",
       "24125  0.000454     0.525523  4996.270042  2579.510698  8270.021928  0.418827   \n",
       "24126  0.031053     0.167581  1890.217497  2748.644250  5175.546000  0.033493   \n",
       "24127  0.024217     0.287329  1296.724388  1259.118567  2319.688878  0.079520   \n",
       "\n",
       "         mfcc{1}     mfcc{2}    mfcc{3}    mfcc{4}  ...  rapJitter  \\\n",
       "0     -412.08945  126.752335  31.558170  18.483738  ...   0.018672   \n",
       "1     -398.93295   50.929253 -17.480385   4.325164  ...   0.030713   \n",
       "2     -599.82200   40.048190   6.373952  13.369130  ...   0.018672   \n",
       "3     -423.74878   77.112580 -11.768955   6.080464  ...   0.050445   \n",
       "4     -327.21740  160.910540 -60.493977 -20.299130  ...   0.001202   \n",
       "...          ...         ...        ...        ...  ...        ...   \n",
       "24123 -388.71793   32.990032  -8.879510  20.782476  ...   0.023928   \n",
       "24124 -433.10117   58.741127  37.350792  51.354850  ...   0.002124   \n",
       "24125 -745.49110  -27.318123  -5.540486  16.303144  ...   0.016868   \n",
       "24126 -499.73740   32.764600  46.792103  40.457275  ...   0.001794   \n",
       "24127 -457.49475   98.845490  -1.881587  40.956110  ...   0.012489   \n",
       "\n",
       "       ppq5Jitter  ddpJitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "0        0.021894   0.056015      0.212818        1.734878     0.097995   \n",
       "1        0.037553   0.092139      0.283908        2.113383     0.137225   \n",
       "2        0.022602   0.056016      0.134015        1.358899     0.061172   \n",
       "3        0.070875   0.151334      0.181641        1.673087     0.101366   \n",
       "4        0.001537   0.003605      0.085836        0.803849     0.035992   \n",
       "...           ...        ...           ...             ...          ...   \n",
       "24123    0.029485   0.071784      0.160208        1.659819     0.064901   \n",
       "24124    0.002396   0.006373      0.042722        0.419731     0.018084   \n",
       "24125    0.019114   0.050605      0.137923        1.255627     0.061327   \n",
       "24126    0.001854   0.005381      0.016609        0.134111     0.007814   \n",
       "24127    0.013602   0.037466      0.106427        1.033221     0.047599   \n",
       "\n",
       "       aqpq5Shimmer  apq11Shimmer  ddaShimmer  label  \n",
       "0          0.147546      0.206467    0.293984      1  \n",
       "1          0.241707      0.116884    0.411675      0  \n",
       "2          0.091838      0.164672    0.183516      1  \n",
       "3          0.093390      0.116884    0.304098      0  \n",
       "4          0.054323      0.093975    0.107976      1  \n",
       "...             ...           ...         ...    ...  \n",
       "24123      0.137543      0.331425    0.194703      0  \n",
       "24124      0.027000      0.041864    0.054251      0  \n",
       "24125      0.081247      0.116884    0.183981      0  \n",
       "24126      0.009204      0.011837    0.023441      1  \n",
       "24127      0.060751      0.095882    0.142798      0  \n",
       "\n",
       "[24128 rows x 80 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7467b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2f993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), df['label'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96d7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a50ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa99fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(50, 50), max_iter=500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the MLP model and set the hyperparameters\n",
    "model = MLPClassifier(hidden_layer_sizes=(50, 50), activation='logistic', solver='adam', max_iter=500)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477efbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98ea5c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7898881060920017\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a2115f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7898881060920017\n",
      "Precision: 0.7713032581453634\n",
      "Recall: 0.7568398401475561\n",
      "F1 Score: 0.7640031031807603\n",
      "ROC AUC: 0.786849423335193\n",
      "Specificity: 0.81685900652283\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model on test set\n",
    "accuracy = accuracy_score(y_test, y_pred.round())\n",
    "precision = precision_score(y_test, y_pred.round())\n",
    "recall = recall_score(y_test, y_pred.round())\n",
    "f1 = f1_score(y_test, y_pred.round())\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred.round()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c94b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "528/528 [==============================] - 6s 5ms/step - loss: 0.5647 - accuracy: 0.7096 - val_loss: 0.4752 - val_accuracy: 0.7620\n",
      "Epoch 2/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.4829 - accuracy: 0.7760 - val_loss: 0.4447 - val_accuracy: 0.7828\n",
      "Epoch 3/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.4586 - accuracy: 0.7874 - val_loss: 0.4366 - val_accuracy: 0.7891\n",
      "Epoch 4/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.4428 - accuracy: 0.7932 - val_loss: 0.4275 - val_accuracy: 0.7940\n",
      "Epoch 5/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.4305 - accuracy: 0.8016 - val_loss: 0.4151 - val_accuracy: 0.8026\n",
      "Epoch 6/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.4209 - accuracy: 0.8083 - val_loss: 0.4129 - val_accuracy: 0.7994\n",
      "Epoch 7/50\n",
      "528/528 [==============================] - 3s 5ms/step - loss: 0.4125 - accuracy: 0.8096 - val_loss: 0.4041 - val_accuracy: 0.8066\n",
      "Epoch 8/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.4071 - accuracy: 0.8130 - val_loss: 0.4018 - val_accuracy: 0.8062\n",
      "Epoch 9/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.4025 - accuracy: 0.8179 - val_loss: 0.3999 - val_accuracy: 0.8072\n",
      "Epoch 10/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3985 - accuracy: 0.8191 - val_loss: 0.3991 - val_accuracy: 0.8113\n",
      "Epoch 11/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3932 - accuracy: 0.8240 - val_loss: 0.3956 - val_accuracy: 0.8145\n",
      "Epoch 12/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3924 - accuracy: 0.8231 - val_loss: 0.3963 - val_accuracy: 0.8139\n",
      "Epoch 13/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3863 - accuracy: 0.8288 - val_loss: 0.3964 - val_accuracy: 0.8105\n",
      "Epoch 14/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3856 - accuracy: 0.8282 - val_loss: 0.3942 - val_accuracy: 0.8117\n",
      "Epoch 15/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3812 - accuracy: 0.8282 - val_loss: 0.3939 - val_accuracy: 0.8113\n",
      "Epoch 16/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3789 - accuracy: 0.8287 - val_loss: 0.3927 - val_accuracy: 0.8167\n",
      "Epoch 17/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3757 - accuracy: 0.8314 - val_loss: 0.3903 - val_accuracy: 0.8164\n",
      "Epoch 18/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3747 - accuracy: 0.8313 - val_loss: 0.3889 - val_accuracy: 0.8181\n",
      "Epoch 19/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3701 - accuracy: 0.8324 - val_loss: 0.3920 - val_accuracy: 0.8149\n",
      "Epoch 20/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3695 - accuracy: 0.8330 - val_loss: 0.3877 - val_accuracy: 0.8175\n",
      "Epoch 21/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3677 - accuracy: 0.8341 - val_loss: 0.3870 - val_accuracy: 0.8185\n",
      "Epoch 22/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3662 - accuracy: 0.8337 - val_loss: 0.3876 - val_accuracy: 0.8175\n",
      "Epoch 23/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3639 - accuracy: 0.8363 - val_loss: 0.3858 - val_accuracy: 0.8188\n",
      "Epoch 24/50\n",
      "528/528 [==============================] - 3s 5ms/step - loss: 0.3610 - accuracy: 0.8362 - val_loss: 0.3847 - val_accuracy: 0.8199\n",
      "Epoch 25/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3615 - accuracy: 0.8360 - val_loss: 0.3859 - val_accuracy: 0.8200\n",
      "Epoch 26/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3571 - accuracy: 0.8392 - val_loss: 0.3869 - val_accuracy: 0.8204\n",
      "Epoch 27/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3621 - accuracy: 0.8367 - val_loss: 0.3820 - val_accuracy: 0.8214\n",
      "Epoch 28/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3536 - accuracy: 0.8432 - val_loss: 0.3889 - val_accuracy: 0.8157\n",
      "Epoch 29/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3533 - accuracy: 0.8413 - val_loss: 0.3884 - val_accuracy: 0.8150\n",
      "Epoch 30/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3542 - accuracy: 0.8407 - val_loss: 0.3861 - val_accuracy: 0.8210\n",
      "Epoch 31/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3534 - accuracy: 0.8416 - val_loss: 0.3884 - val_accuracy: 0.8174\n",
      "Epoch 32/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3539 - accuracy: 0.8416 - val_loss: 0.3836 - val_accuracy: 0.8219\n",
      "Epoch 33/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3480 - accuracy: 0.8443 - val_loss: 0.3849 - val_accuracy: 0.8218\n",
      "Epoch 34/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3498 - accuracy: 0.8426 - val_loss: 0.3825 - val_accuracy: 0.8218\n",
      "Epoch 35/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3533 - accuracy: 0.8429 - val_loss: 0.3849 - val_accuracy: 0.8204\n",
      "Epoch 36/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3514 - accuracy: 0.8448 - val_loss: 0.3876 - val_accuracy: 0.8235\n",
      "Epoch 37/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3445 - accuracy: 0.8432 - val_loss: 0.3837 - val_accuracy: 0.8215\n",
      "Epoch 38/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3412 - accuracy: 0.8461 - val_loss: 0.3835 - val_accuracy: 0.8212\n",
      "Epoch 39/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3450 - accuracy: 0.8461 - val_loss: 0.3821 - val_accuracy: 0.8240\n",
      "Epoch 40/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3465 - accuracy: 0.8452 - val_loss: 0.3884 - val_accuracy: 0.8204\n",
      "Epoch 41/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3424 - accuracy: 0.8451 - val_loss: 0.3880 - val_accuracy: 0.8218\n",
      "Epoch 42/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3371 - accuracy: 0.8491 - val_loss: 0.3886 - val_accuracy: 0.8195\n",
      "Epoch 43/50\n",
      "528/528 [==============================] - 3s 5ms/step - loss: 0.3413 - accuracy: 0.8468 - val_loss: 0.3841 - val_accuracy: 0.8199\n",
      "Epoch 44/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3387 - accuracy: 0.8490 - val_loss: 0.3869 - val_accuracy: 0.8201\n",
      "Epoch 45/50\n",
      "528/528 [==============================] - 3s 5ms/step - loss: 0.3398 - accuracy: 0.8479 - val_loss: 0.3848 - val_accuracy: 0.8199\n",
      "Epoch 46/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3370 - accuracy: 0.8510 - val_loss: 0.3828 - val_accuracy: 0.8243\n",
      "Epoch 47/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 0.3383 - accuracy: 0.8466 - val_loss: 0.3855 - val_accuracy: 0.8204\n",
      "Epoch 48/50\n",
      "528/528 [==============================] - 3s 5ms/step - loss: 0.3371 - accuracy: 0.8498 - val_loss: 0.3890 - val_accuracy: 0.8244\n",
      "Epoch 49/50\n",
      "528/528 [==============================] - 2s 4ms/step - loss: 0.3301 - accuracy: 0.8525 - val_loss: 0.3868 - val_accuracy: 0.8230\n",
      "Epoch 50/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3330 - accuracy: 0.8508 - val_loss: 0.3835 - val_accuracy: 0.8248\n",
      "227/227 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.8248376847630888\n",
      "Precision: 0.8921374950612406\n",
      "Recall: 0.6941284967722102\n",
      "F1-score: 0.780774550484094\n",
      "ROC AUC: 0.8128193913866069\n",
      "Specificity: 0.9315102860010035\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "spec = tn / (tn+fp)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('Accuracy:', acc)\n",
    "print('Precision:', prec)\n",
    "print('Recall:', rec)\n",
    "print('F1-score:', f1)\n",
    "print('ROC AUC:', auc)\n",
    "print('Specificity:', spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e63a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "528/528 [==============================] - 8s 7ms/step - loss: 0.6445 - accuracy: 0.6427 - val_loss: 0.5272 - val_accuracy: 0.7533\n",
      "Epoch 2/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.5337 - accuracy: 0.7463 - val_loss: 0.4692 - val_accuracy: 0.7718\n",
      "Epoch 3/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4957 - accuracy: 0.7674 - val_loss: 0.4613 - val_accuracy: 0.7766\n",
      "Epoch 4/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4769 - accuracy: 0.7777 - val_loss: 0.4536 - val_accuracy: 0.7812\n",
      "Epoch 5/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4688 - accuracy: 0.7837 - val_loss: 0.4360 - val_accuracy: 0.7893\n",
      "Epoch 6/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4536 - accuracy: 0.7922 - val_loss: 0.4353 - val_accuracy: 0.7935\n",
      "Epoch 7/50\n",
      "528/528 [==============================] - 3s 5ms/step - loss: 0.4455 - accuracy: 0.7973 - val_loss: 0.4281 - val_accuracy: 0.7962\n",
      "Epoch 8/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4378 - accuracy: 0.7959 - val_loss: 0.4151 - val_accuracy: 0.7956\n",
      "Epoch 9/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4336 - accuracy: 0.8003 - val_loss: 0.4104 - val_accuracy: 0.8012\n",
      "Epoch 10/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4245 - accuracy: 0.8098 - val_loss: 0.4157 - val_accuracy: 0.7990\n",
      "Epoch 11/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4230 - accuracy: 0.8027 - val_loss: 0.4060 - val_accuracy: 0.8038\n",
      "Epoch 12/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4149 - accuracy: 0.8086 - val_loss: 0.4076 - val_accuracy: 0.8054\n",
      "Epoch 13/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4151 - accuracy: 0.8073 - val_loss: 0.4052 - val_accuracy: 0.8036\n",
      "Epoch 14/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4120 - accuracy: 0.8103 - val_loss: 0.4028 - val_accuracy: 0.8067\n",
      "Epoch 15/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4129 - accuracy: 0.8131 - val_loss: 0.4097 - val_accuracy: 0.8016\n",
      "Epoch 16/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4114 - accuracy: 0.8132 - val_loss: 0.4065 - val_accuracy: 0.8047\n",
      "Epoch 17/50\n",
      "528/528 [==============================] - 3s 7ms/step - loss: 0.4058 - accuracy: 0.8173 - val_loss: 0.4001 - val_accuracy: 0.8077\n",
      "Epoch 18/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4038 - accuracy: 0.8163 - val_loss: 0.4045 - val_accuracy: 0.8099\n",
      "Epoch 19/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.4003 - accuracy: 0.8162 - val_loss: 0.3963 - val_accuracy: 0.8114\n",
      "Epoch 20/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3949 - accuracy: 0.8201 - val_loss: 0.3968 - val_accuracy: 0.8130\n",
      "Epoch 21/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.4035 - accuracy: 0.8211 - val_loss: 0.3948 - val_accuracy: 0.8113\n",
      "Epoch 22/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3944 - accuracy: 0.8218 - val_loss: 0.3949 - val_accuracy: 0.8132\n",
      "Epoch 23/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3902 - accuracy: 0.8227 - val_loss: 0.3907 - val_accuracy: 0.8139\n",
      "Epoch 24/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3897 - accuracy: 0.8236 - val_loss: 0.3915 - val_accuracy: 0.8128\n",
      "Epoch 25/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3868 - accuracy: 0.8254 - val_loss: 0.3924 - val_accuracy: 0.8164\n",
      "Epoch 26/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3906 - accuracy: 0.8254 - val_loss: 0.3896 - val_accuracy: 0.8164\n",
      "Epoch 27/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3854 - accuracy: 0.8263 - val_loss: 0.3887 - val_accuracy: 0.8153\n",
      "Epoch 28/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3858 - accuracy: 0.8262 - val_loss: 0.3834 - val_accuracy: 0.8168\n",
      "Epoch 29/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3853 - accuracy: 0.8288 - val_loss: 0.3881 - val_accuracy: 0.8207\n",
      "Epoch 30/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3784 - accuracy: 0.8302 - val_loss: 0.3848 - val_accuracy: 0.8199\n",
      "Epoch 31/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3788 - accuracy: 0.8298 - val_loss: 0.3889 - val_accuracy: 0.8102\n",
      "Epoch 32/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3801 - accuracy: 0.8273 - val_loss: 0.3837 - val_accuracy: 0.8200\n",
      "Epoch 33/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3761 - accuracy: 0.8305 - val_loss: 0.3879 - val_accuracy: 0.8167\n",
      "Epoch 34/50\n",
      "528/528 [==============================] - 3s 7ms/step - loss: 0.3750 - accuracy: 0.8319 - val_loss: 0.3869 - val_accuracy: 0.8210\n",
      "Epoch 35/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3783 - accuracy: 0.8305 - val_loss: 0.3955 - val_accuracy: 0.8174\n",
      "Epoch 36/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3724 - accuracy: 0.8356 - val_loss: 0.3885 - val_accuracy: 0.8201\n",
      "Epoch 37/50\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.3673 - accuracy: 0.8349 - val_loss: 0.3815 - val_accuracy: 0.8212\n",
      "Epoch 38/50\n",
      "528/528 [==============================] - 4s 8ms/step - loss: 0.3730 - accuracy: 0.8346 - val_loss: 0.3872 - val_accuracy: 0.8159\n",
      "Epoch 39/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3734 - accuracy: 0.8313 - val_loss: 0.3852 - val_accuracy: 0.8215\n",
      "Epoch 40/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3710 - accuracy: 0.8346 - val_loss: 0.3822 - val_accuracy: 0.8206\n",
      "Epoch 41/50\n",
      "528/528 [==============================] - 4s 8ms/step - loss: 0.3662 - accuracy: 0.8367 - val_loss: 0.3840 - val_accuracy: 0.8199\n",
      "Epoch 42/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3625 - accuracy: 0.8392 - val_loss: 0.3838 - val_accuracy: 0.8199\n",
      "Epoch 43/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3625 - accuracy: 0.8358 - val_loss: 0.3791 - val_accuracy: 0.8210\n",
      "Epoch 44/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3642 - accuracy: 0.8365 - val_loss: 0.3800 - val_accuracy: 0.8228\n",
      "Epoch 45/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3621 - accuracy: 0.8386 - val_loss: 0.3839 - val_accuracy: 0.8163\n",
      "Epoch 46/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3614 - accuracy: 0.8386 - val_loss: 0.3797 - val_accuracy: 0.8228\n",
      "Epoch 47/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3595 - accuracy: 0.8358 - val_loss: 0.3796 - val_accuracy: 0.8211\n",
      "Epoch 48/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3530 - accuracy: 0.8429 - val_loss: 0.3805 - val_accuracy: 0.8228\n",
      "Epoch 49/50\n",
      "528/528 [==============================] - 4s 7ms/step - loss: 0.3596 - accuracy: 0.8375 - val_loss: 0.3814 - val_accuracy: 0.8196\n",
      "Epoch 50/50\n",
      "528/528 [==============================] - 4s 8ms/step - loss: 0.3568 - accuracy: 0.8373 - val_loss: 0.3791 - val_accuracy: 0.8222\n",
      "227/227 [==============================] - 2s 4ms/step\n",
      "Accuracy: 0.8222130128470784\n",
      "Precision: 0.9237068965517241\n",
      "Recall: 0.6587765139870888\n",
      "F1-score: 0.7690651354746095\n",
      "ROC AUC: 0.8974843785403847\n",
      "Specificity: 0.9555945810336176\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary values\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Calculate metrics\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "auc_score = auc(fpr, tpr)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1_score}\")\n",
    "print(f\"ROC AUC: {auc_score}\")\n",
    "print(f\"Specificity: {specificity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "919e5527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 1s 3ms/step\n",
      "Test Loss: 0.688245415687561\n",
      "Test Accuracy: 0.5496705174446106\n",
      "Test Precision: 0.0\n",
      "Test Recall: 0.0\n",
      "Test F1 score: 0.0\n",
      "Test ROC AUC score: 0.5\n",
      "Test Specificity: 1.0\n",
      "252/252 [==============================] - 0s 2ms/step\n",
      "Test Loss: 0.6879045367240906\n",
      "Test Accuracy: 0.5511624813079834\n",
      "Test Precision: 0.0\n",
      "Test Recall: 0.0\n",
      "Test F1 score: 0.0\n",
      "Test ROC AUC score: 0.5\n",
      "Test Specificity: 1.0\n",
      "252/252 [==============================] - 0s 2ms/step\n",
      "Test Loss: 0.686949610710144\n",
      "Test Accuracy: 0.5558319091796875\n",
      "Test Precision: 0.0\n",
      "Test Recall: 0.0\n",
      "Test F1 score: 0.0\n",
      "Test ROC AUC score: 0.5\n",
      "Test Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from keras import backend as K\n",
    "\n",
    "X=df.drop('label', axis=1)\n",
    "y=df['label']\n",
    "X = X.values\n",
    "\n",
    "# define metrics\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true)*(1-y_pred), 0, 1)))\n",
    "    true_positives = K.sum(K.round(K.clip(y_true*y_pred, 0, 1)))\n",
    "    false_negatives = K.sum(K.round(K.clip(y_true*(1-y_pred), 0, 1)))\n",
    "    false_positives = K.sum(K.round(K.clip((1-y_true)*y_pred, 0, 1)))\n",
    "    specificity = true_negatives/(true_negatives+false_positives+K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "\n",
    "def confusion_matrix_tf(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.int32)\n",
    "    values, indices, counts = tf.unique_with_counts(tf.stack([y_true, y_pred], axis=1))\n",
    "    num_classes = tf.shape(values)[0]\n",
    "    cm = tf.zeros((num_classes, num_classes), dtype=tf.int32)\n",
    "    updates = tf.reshape(counts, (-1,))\n",
    "    indices = tf.unstack(indices, axis=1)\n",
    "    cm = tf.tensor_scatter_nd_add(cm, indices, updates)\n",
    "    return cm\n",
    "\n",
    "# define MLP model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), \n",
    "              tf.keras.metrics.Recall(), tf.keras.metrics.AUC(), specificity])\n",
    "\n",
    "# define cross-validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# train and evaluate model using cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=0)\n",
    "    loss, accuracy, precision, recall, auc, specificity = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = np.round(model.predict(X_test))\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print('Test Loss:', loss)\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    print('Test Precision:', precision)\n",
    "    print('Test Recall:', recall)\n",
    "    print('Test F1 score:', f1)\n",
    "    print('Test ROC AUC score:', auc)\n",
    "    print('Test Specificity:', specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7c80681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 1s 2ms/step\n",
      "252/252 [==============================] - 0s 1ms/step\n",
      "252/252 [==============================] - 2s 4ms/step\n",
      "Accuracy: 0.554\n",
      "Precision: 0.459\n",
      "Recall: 0.185\n",
      "ROC AUC: 0.519\n",
      "Specificity: 0.854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Define the evaluation metrics\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn = confusion_matrix(y_true, y_pred)[0, 0]\n",
    "    fp = confusion_matrix(y_true, y_pred)[0, 1]\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "X=df.drop('label', axis=1)\n",
    "y=df['label']\n",
    "X = X.values\n",
    "\n",
    "# Define the MLP model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize the K-Fold cross-validator\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "roc_auc_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "# Loop over the folds\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    # Split the data into train and test sets for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Create and fit the model on the train set\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = np.round(model.predict(X_test)).flatten()\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    specificity = specificity_score(y_test, y_pred)\n",
    "    \n",
    "    # Append the evaluation metrics for this fold to the lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Compute and print the average evaluation metrics over all folds\n",
    "print('Accuracy: {:.3f}'.format(np.mean(accuracy_scores)))\n",
    "print('Precision: {:.3f}'.format(np.mean(precision_scores)))\n",
    "print('Recall: {:.3f}'.format(np.mean(recall_scores)))\n",
    "print('ROC AUC: {:.3f}'.format(np.mean(roc_auc_scores)))\n",
    "print('Specificity: {:.3f}'.format(np.mean(specificity_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec58c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
