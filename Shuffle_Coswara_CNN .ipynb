{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca1c15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('updated_coswara.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2650c66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc{1}</th>\n",
       "      <th>mfcc{2}</th>\n",
       "      <th>mfcc{3}</th>\n",
       "      <th>mfcc{4}</th>\n",
       "      <th>...</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>ddpJitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>aqpq5Shimmer</th>\n",
       "      <th>apq11Shimmer</th>\n",
       "      <th>ddaShimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039102</td>\n",
       "      <td>0.378903</td>\n",
       "      <td>786.823461</td>\n",
       "      <td>966.699650</td>\n",
       "      <td>1387.984940</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>-412.08945</td>\n",
       "      <td>126.752335</td>\n",
       "      <td>31.558170</td>\n",
       "      <td>18.483738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.212818</td>\n",
       "      <td>1.734878</td>\n",
       "      <td>0.097995</td>\n",
       "      <td>0.147546</td>\n",
       "      <td>0.206467</td>\n",
       "      <td>0.293984</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051812</td>\n",
       "      <td>0.436672</td>\n",
       "      <td>2219.820298</td>\n",
       "      <td>1874.652272</td>\n",
       "      <td>4134.754998</td>\n",
       "      <td>0.209401</td>\n",
       "      <td>-398.93295</td>\n",
       "      <td>50.929253</td>\n",
       "      <td>-17.480385</td>\n",
       "      <td>4.325164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.092139</td>\n",
       "      <td>0.283908</td>\n",
       "      <td>2.113383</td>\n",
       "      <td>0.137225</td>\n",
       "      <td>0.241707</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.411675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.238371</td>\n",
       "      <td>2002.598308</td>\n",
       "      <td>2058.223470</td>\n",
       "      <td>4324.443295</td>\n",
       "      <td>0.146994</td>\n",
       "      <td>-599.82200</td>\n",
       "      <td>40.048190</td>\n",
       "      <td>6.373952</td>\n",
       "      <td>13.369130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.022602</td>\n",
       "      <td>0.056016</td>\n",
       "      <td>0.134015</td>\n",
       "      <td>1.358899</td>\n",
       "      <td>0.061172</td>\n",
       "      <td>0.091838</td>\n",
       "      <td>0.164672</td>\n",
       "      <td>0.183516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.078437</td>\n",
       "      <td>0.242433</td>\n",
       "      <td>569.328347</td>\n",
       "      <td>506.956042</td>\n",
       "      <td>979.591497</td>\n",
       "      <td>0.035271</td>\n",
       "      <td>-423.74878</td>\n",
       "      <td>77.112580</td>\n",
       "      <td>-11.768955</td>\n",
       "      <td>6.080464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050445</td>\n",
       "      <td>0.070875</td>\n",
       "      <td>0.151334</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>1.673087</td>\n",
       "      <td>0.101366</td>\n",
       "      <td>0.093390</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.304098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072712</td>\n",
       "      <td>0.327475</td>\n",
       "      <td>1344.446613</td>\n",
       "      <td>1062.582139</td>\n",
       "      <td>2431.292693</td>\n",
       "      <td>0.089481</td>\n",
       "      <td>-327.21740</td>\n",
       "      <td>160.910540</td>\n",
       "      <td>-60.493977</td>\n",
       "      <td>-20.299130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.003605</td>\n",
       "      <td>0.085836</td>\n",
       "      <td>0.803849</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>0.054323</td>\n",
       "      <td>0.093975</td>\n",
       "      <td>0.107976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24123</th>\n",
       "      <td>0.056854</td>\n",
       "      <td>0.467262</td>\n",
       "      <td>2845.574993</td>\n",
       "      <td>2044.704050</td>\n",
       "      <td>5269.369989</td>\n",
       "      <td>0.235494</td>\n",
       "      <td>-388.71793</td>\n",
       "      <td>32.990032</td>\n",
       "      <td>-8.879510</td>\n",
       "      <td>20.782476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023928</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>0.071784</td>\n",
       "      <td>0.160208</td>\n",
       "      <td>1.659819</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>0.137543</td>\n",
       "      <td>0.331425</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24124</th>\n",
       "      <td>0.050149</td>\n",
       "      <td>0.267105</td>\n",
       "      <td>1199.737564</td>\n",
       "      <td>1636.413852</td>\n",
       "      <td>2392.060470</td>\n",
       "      <td>0.059086</td>\n",
       "      <td>-433.10117</td>\n",
       "      <td>58.741127</td>\n",
       "      <td>37.350792</td>\n",
       "      <td>51.354850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.419731</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.041864</td>\n",
       "      <td>0.054251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24125</th>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.525523</td>\n",
       "      <td>4996.270042</td>\n",
       "      <td>2579.510698</td>\n",
       "      <td>8270.021928</td>\n",
       "      <td>0.418827</td>\n",
       "      <td>-745.49110</td>\n",
       "      <td>-27.318123</td>\n",
       "      <td>-5.540486</td>\n",
       "      <td>16.303144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.050605</td>\n",
       "      <td>0.137923</td>\n",
       "      <td>1.255627</td>\n",
       "      <td>0.061327</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.183981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24126</th>\n",
       "      <td>0.031053</td>\n",
       "      <td>0.167581</td>\n",
       "      <td>1890.217497</td>\n",
       "      <td>2748.644250</td>\n",
       "      <td>5175.546000</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>-499.73740</td>\n",
       "      <td>32.764600</td>\n",
       "      <td>46.792103</td>\n",
       "      <td>40.457275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.016609</td>\n",
       "      <td>0.134111</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.023441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24127</th>\n",
       "      <td>0.024217</td>\n",
       "      <td>0.287329</td>\n",
       "      <td>1296.724388</td>\n",
       "      <td>1259.118567</td>\n",
       "      <td>2319.688878</td>\n",
       "      <td>0.079520</td>\n",
       "      <td>-457.49475</td>\n",
       "      <td>98.845490</td>\n",
       "      <td>-1.881587</td>\n",
       "      <td>40.956110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>0.013602</td>\n",
       "      <td>0.037466</td>\n",
       "      <td>0.106427</td>\n",
       "      <td>1.033221</td>\n",
       "      <td>0.047599</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>0.095882</td>\n",
       "      <td>0.142798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24128 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse  chroma_stft    spec_cent      spec_bw      rolloff       zcr  \\\n",
       "0      0.039102     0.378903   786.823461   966.699650  1387.984940  0.043870   \n",
       "1      0.051812     0.436672  2219.820298  1874.652272  4134.754998  0.209401   \n",
       "2      0.010413     0.238371  2002.598308  2058.223470  4324.443295  0.146994   \n",
       "3      0.078437     0.242433   569.328347   506.956042   979.591497  0.035271   \n",
       "4      0.072712     0.327475  1344.446613  1062.582139  2431.292693  0.089481   \n",
       "...         ...          ...          ...          ...          ...       ...   \n",
       "24123  0.056854     0.467262  2845.574993  2044.704050  5269.369989  0.235494   \n",
       "24124  0.050149     0.267105  1199.737564  1636.413852  2392.060470  0.059086   \n",
       "24125  0.000454     0.525523  4996.270042  2579.510698  8270.021928  0.418827   \n",
       "24126  0.031053     0.167581  1890.217497  2748.644250  5175.546000  0.033493   \n",
       "24127  0.024217     0.287329  1296.724388  1259.118567  2319.688878  0.079520   \n",
       "\n",
       "         mfcc{1}     mfcc{2}    mfcc{3}    mfcc{4}  ...  rapJitter  \\\n",
       "0     -412.08945  126.752335  31.558170  18.483738  ...   0.018672   \n",
       "1     -398.93295   50.929253 -17.480385   4.325164  ...   0.030713   \n",
       "2     -599.82200   40.048190   6.373952  13.369130  ...   0.018672   \n",
       "3     -423.74878   77.112580 -11.768955   6.080464  ...   0.050445   \n",
       "4     -327.21740  160.910540 -60.493977 -20.299130  ...   0.001202   \n",
       "...          ...         ...        ...        ...  ...        ...   \n",
       "24123 -388.71793   32.990032  -8.879510  20.782476  ...   0.023928   \n",
       "24124 -433.10117   58.741127  37.350792  51.354850  ...   0.002124   \n",
       "24125 -745.49110  -27.318123  -5.540486  16.303144  ...   0.016868   \n",
       "24126 -499.73740   32.764600  46.792103  40.457275  ...   0.001794   \n",
       "24127 -457.49475   98.845490  -1.881587  40.956110  ...   0.012489   \n",
       "\n",
       "       ppq5Jitter  ddpJitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "0        0.021894   0.056015      0.212818        1.734878     0.097995   \n",
       "1        0.037553   0.092139      0.283908        2.113383     0.137225   \n",
       "2        0.022602   0.056016      0.134015        1.358899     0.061172   \n",
       "3        0.070875   0.151334      0.181641        1.673087     0.101366   \n",
       "4        0.001537   0.003605      0.085836        0.803849     0.035992   \n",
       "...           ...        ...           ...             ...          ...   \n",
       "24123    0.029485   0.071784      0.160208        1.659819     0.064901   \n",
       "24124    0.002396   0.006373      0.042722        0.419731     0.018084   \n",
       "24125    0.019114   0.050605      0.137923        1.255627     0.061327   \n",
       "24126    0.001854   0.005381      0.016609        0.134111     0.007814   \n",
       "24127    0.013602   0.037466      0.106427        1.033221     0.047599   \n",
       "\n",
       "       aqpq5Shimmer  apq11Shimmer  ddaShimmer  label  \n",
       "0          0.147546      0.206467    0.293984      1  \n",
       "1          0.241707      0.116884    0.411675      0  \n",
       "2          0.091838      0.164672    0.183516      1  \n",
       "3          0.093390      0.116884    0.304098      0  \n",
       "4          0.054323      0.093975    0.107976      1  \n",
       "...             ...           ...         ...    ...  \n",
       "24123      0.137543      0.331425    0.194703      0  \n",
       "24124      0.027000      0.041864    0.054251      0  \n",
       "24125      0.081247      0.116884    0.183981      0  \n",
       "24126      0.009204      0.011837    0.023441      1  \n",
       "24127      0.060751      0.095882    0.142798      0  \n",
       "\n",
       "[24128 rows x 80 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef70d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "89e06694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4e85b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for use in a CNN model\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "30ddb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer 1\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=2, activation='sigmoid', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f935251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c4545e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "604/604 [==============================] - 8s 12ms/step - loss: 0.6533 - accuracy: 0.4466 - val_loss: 0.6027 - val_accuracy: 0.4523\n",
      "Epoch 2/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.5938 - accuracy: 0.4466 - val_loss: 0.5831 - val_accuracy: 0.4523\n",
      "Epoch 3/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.5595 - accuracy: 0.4466 - val_loss: 0.5466 - val_accuracy: 0.4523\n",
      "Epoch 4/50\n",
      "604/604 [==============================] - 5s 7ms/step - loss: 0.5295 - accuracy: 0.4466 - val_loss: 0.5303 - val_accuracy: 0.4523\n",
      "Epoch 5/50\n",
      "604/604 [==============================] - 5s 8ms/step - loss: 0.5120 - accuracy: 0.4466 - val_loss: 0.5114 - val_accuracy: 0.4523\n",
      "Epoch 6/50\n",
      "604/604 [==============================] - 5s 8ms/step - loss: 0.4973 - accuracy: 0.4466 - val_loss: 0.4913 - val_accuracy: 0.4523\n",
      "Epoch 7/50\n",
      "604/604 [==============================] - 5s 8ms/step - loss: 0.4867 - accuracy: 0.4466 - val_loss: 0.5080 - val_accuracy: 0.4523\n",
      "Epoch 8/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4783 - accuracy: 0.4466 - val_loss: 0.4757 - val_accuracy: 0.4523\n",
      "Epoch 9/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4717 - accuracy: 0.4466 - val_loss: 0.4778 - val_accuracy: 0.4523\n",
      "Epoch 10/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4703 - accuracy: 0.4466 - val_loss: 0.4695 - val_accuracy: 0.4523\n",
      "Epoch 11/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4678 - accuracy: 0.4466 - val_loss: 0.4707 - val_accuracy: 0.4523\n",
      "Epoch 12/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4639 - accuracy: 0.4466 - val_loss: 0.4661 - val_accuracy: 0.4523\n",
      "Epoch 13/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4623 - accuracy: 0.4466 - val_loss: 0.4622 - val_accuracy: 0.4523\n",
      "Epoch 14/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4610 - accuracy: 0.4466 - val_loss: 0.4754 - val_accuracy: 0.4523\n",
      "Epoch 15/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4567 - accuracy: 0.4466 - val_loss: 0.4596 - val_accuracy: 0.4523\n",
      "Epoch 16/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4563 - accuracy: 0.4466 - val_loss: 0.4606 - val_accuracy: 0.4523\n",
      "Epoch 17/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4522 - accuracy: 0.4466 - val_loss: 0.4720 - val_accuracy: 0.4523\n",
      "Epoch 18/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4552 - accuracy: 0.4466 - val_loss: 0.4876 - val_accuracy: 0.4523\n",
      "Epoch 19/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4501 - accuracy: 0.4466 - val_loss: 0.4674 - val_accuracy: 0.4523\n",
      "Epoch 20/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4499 - accuracy: 0.4466 - val_loss: 0.4684 - val_accuracy: 0.4523\n",
      "Epoch 21/50\n",
      "604/604 [==============================] - 5s 8ms/step - loss: 0.4510 - accuracy: 0.4466 - val_loss: 0.4690 - val_accuracy: 0.4523\n",
      "Epoch 22/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4515 - accuracy: 0.4466 - val_loss: 0.4536 - val_accuracy: 0.4523\n",
      "Epoch 23/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4476 - accuracy: 0.4466 - val_loss: 0.4520 - val_accuracy: 0.4523\n",
      "Epoch 24/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4468 - accuracy: 0.4466 - val_loss: 0.4554 - val_accuracy: 0.4523\n",
      "Epoch 25/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4465 - accuracy: 0.4466 - val_loss: 0.4507 - val_accuracy: 0.4523\n",
      "Epoch 26/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4452 - accuracy: 0.4466 - val_loss: 0.4587 - val_accuracy: 0.4523\n",
      "Epoch 27/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4480 - accuracy: 0.4466 - val_loss: 0.4504 - val_accuracy: 0.4523\n",
      "Epoch 28/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4427 - accuracy: 0.4466 - val_loss: 0.4553 - val_accuracy: 0.4523\n",
      "Epoch 29/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4434 - accuracy: 0.4466 - val_loss: 0.4678 - val_accuracy: 0.4523\n",
      "Epoch 30/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4410 - accuracy: 0.4466 - val_loss: 0.4461 - val_accuracy: 0.4523\n",
      "Epoch 31/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4397 - accuracy: 0.4466 - val_loss: 0.4536 - val_accuracy: 0.4523\n",
      "Epoch 32/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4416 - accuracy: 0.4466 - val_loss: 0.4440 - val_accuracy: 0.4523\n",
      "Epoch 33/50\n",
      "604/604 [==============================] - 5s 8ms/step - loss: 0.4387 - accuracy: 0.4466 - val_loss: 0.4620 - val_accuracy: 0.4523\n",
      "Epoch 34/50\n",
      "604/604 [==============================] - 5s 9ms/step - loss: 0.4382 - accuracy: 0.4466 - val_loss: 0.4767 - val_accuracy: 0.4523\n",
      "Epoch 35/50\n",
      "604/604 [==============================] - 5s 9ms/step - loss: 0.4379 - accuracy: 0.4466 - val_loss: 0.4544 - val_accuracy: 0.4523\n",
      "Epoch 36/50\n",
      "604/604 [==============================] - 5s 8ms/step - loss: 0.4396 - accuracy: 0.4466 - val_loss: 0.4429 - val_accuracy: 0.4523\n",
      "Epoch 37/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4354 - accuracy: 0.4466 - val_loss: 0.4431 - val_accuracy: 0.4523\n",
      "Epoch 38/50\n",
      "604/604 [==============================] - 5s 8ms/step - loss: 0.4335 - accuracy: 0.4466 - val_loss: 0.4611 - val_accuracy: 0.4523\n",
      "Epoch 39/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4389 - accuracy: 0.4466 - val_loss: 0.4501 - val_accuracy: 0.4523\n",
      "Epoch 40/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4342 - accuracy: 0.4466 - val_loss: 0.4439 - val_accuracy: 0.4523\n",
      "Epoch 41/50\n",
      "604/604 [==============================] - 4s 6ms/step - loss: 0.4326 - accuracy: 0.4466 - val_loss: 0.4419 - val_accuracy: 0.4523\n",
      "Epoch 42/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4348 - accuracy: 0.4466 - val_loss: 0.4477 - val_accuracy: 0.4523\n",
      "Epoch 43/50\n",
      "604/604 [==============================] - 4s 6ms/step - loss: 0.4321 - accuracy: 0.4466 - val_loss: 0.4406 - val_accuracy: 0.4523\n",
      "Epoch 44/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4323 - accuracy: 0.4466 - val_loss: 0.4455 - val_accuracy: 0.4523\n",
      "Epoch 45/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4311 - accuracy: 0.4466 - val_loss: 0.4444 - val_accuracy: 0.4523\n",
      "Epoch 46/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4306 - accuracy: 0.4466 - val_loss: 0.4394 - val_accuracy: 0.4523\n",
      "Epoch 47/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4304 - accuracy: 0.4466 - val_loss: 0.4419 - val_accuracy: 0.4523\n",
      "Epoch 48/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4283 - accuracy: 0.4466 - val_loss: 0.4378 - val_accuracy: 0.4523\n",
      "Epoch 49/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4284 - accuracy: 0.4466 - val_loss: 0.4361 - val_accuracy: 0.4523\n",
      "Epoch 50/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.4288 - accuracy: 0.4466 - val_loss: 0.4469 - val_accuracy: 0.4523\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 0.4469 - accuracy: 0.4523\n",
      "Test accuracy: 0.4523414969444275\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "000c1287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 1s 4ms/step\n",
      "Accuracy: 0.45234148363033566\n",
      "Precision: 0.45234148363033566\n",
      "Recall: 1.0\n",
      "F1 Score: 0.6229133970609216\n",
      "ROC AUC: 0.5\n",
      "Specificity: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"Specificity: {specificity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f75530e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "377/377 [==============================] - 8s 16ms/step - loss: 0.6929 - accuracy: 0.4478 - val_loss: 0.6606 - val_accuracy: 0.4478\n",
      "Epoch 2/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.6382 - accuracy: 0.4478 - val_loss: 0.6477 - val_accuracy: 0.4478\n",
      "Epoch 3/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.6119 - accuracy: 0.4478 - val_loss: 0.6084 - val_accuracy: 0.4478\n",
      "Epoch 4/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.5930 - accuracy: 0.4478 - val_loss: 0.5821 - val_accuracy: 0.4478\n",
      "Epoch 5/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.5761 - accuracy: 0.4478 - val_loss: 0.5650 - val_accuracy: 0.4478\n",
      "Epoch 6/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.5606 - accuracy: 0.4478 - val_loss: 0.5578 - val_accuracy: 0.4478\n",
      "Epoch 7/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.5406 - accuracy: 0.4478 - val_loss: 0.5314 - val_accuracy: 0.4478\n",
      "Epoch 8/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.5224 - accuracy: 0.4478 - val_loss: 0.5195 - val_accuracy: 0.4478\n",
      "Epoch 9/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.5131 - accuracy: 0.4478 - val_loss: 0.5421 - val_accuracy: 0.4478\n",
      "Epoch 10/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.5040 - accuracy: 0.4478 - val_loss: 0.5166 - val_accuracy: 0.4478\n",
      "Epoch 11/50\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.4938 - accuracy: 0.4478 - val_loss: 0.5014 - val_accuracy: 0.4478\n",
      "Epoch 12/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4854 - accuracy: 0.4478 - val_loss: 0.5008 - val_accuracy: 0.4478\n",
      "Epoch 13/50\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.4782 - accuracy: 0.4478 - val_loss: 0.4822 - val_accuracy: 0.4478\n",
      "Epoch 14/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4743 - accuracy: 0.4478 - val_loss: 0.5030 - val_accuracy: 0.4478\n",
      "Epoch 15/50\n",
      "377/377 [==============================] - 6s 17ms/step - loss: 0.4662 - accuracy: 0.4478 - val_loss: 0.4735 - val_accuracy: 0.4478\n",
      "Epoch 16/50\n",
      "377/377 [==============================] - 5s 12ms/step - loss: 0.4676 - accuracy: 0.4478 - val_loss: 0.5006 - val_accuracy: 0.4478\n",
      "Epoch 17/50\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.4590 - accuracy: 0.4478 - val_loss: 0.4869 - val_accuracy: 0.4478\n",
      "Epoch 18/50\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.4584 - accuracy: 0.4478 - val_loss: 0.4634 - val_accuracy: 0.4478\n",
      "Epoch 19/50\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.4526 - accuracy: 0.4478 - val_loss: 0.4654 - val_accuracy: 0.4478\n",
      "Epoch 20/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4509 - accuracy: 0.4478 - val_loss: 0.4579 - val_accuracy: 0.4478\n",
      "Epoch 21/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4517 - accuracy: 0.4478 - val_loss: 0.4791 - val_accuracy: 0.4478\n",
      "Epoch 22/50\n",
      "377/377 [==============================] - 5s 15ms/step - loss: 0.4476 - accuracy: 0.4478 - val_loss: 0.4619 - val_accuracy: 0.4478\n",
      "Epoch 23/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4457 - accuracy: 0.4478 - val_loss: 0.4652 - val_accuracy: 0.4478\n",
      "Epoch 24/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4402 - accuracy: 0.4478 - val_loss: 0.4616 - val_accuracy: 0.4478\n",
      "Epoch 25/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4407 - accuracy: 0.4478 - val_loss: 0.4499 - val_accuracy: 0.4478\n",
      "Epoch 26/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4379 - accuracy: 0.4478 - val_loss: 0.4504 - val_accuracy: 0.4478\n",
      "Epoch 27/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4346 - accuracy: 0.4478 - val_loss: 0.4493 - val_accuracy: 0.4478\n",
      "Epoch 28/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4359 - accuracy: 0.4478 - val_loss: 0.4481 - val_accuracy: 0.4478\n",
      "Epoch 29/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4299 - accuracy: 0.4478 - val_loss: 0.4581 - val_accuracy: 0.4478\n",
      "Epoch 30/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4274 - accuracy: 0.4478 - val_loss: 0.4424 - val_accuracy: 0.4478\n",
      "Epoch 31/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4273 - accuracy: 0.4478 - val_loss: 0.4425 - val_accuracy: 0.4478\n",
      "Epoch 32/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4262 - accuracy: 0.4478 - val_loss: 0.4435 - val_accuracy: 0.4478\n",
      "Epoch 33/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4265 - accuracy: 0.4478 - val_loss: 0.4501 - val_accuracy: 0.4478\n",
      "Epoch 34/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4262 - accuracy: 0.4478 - val_loss: 0.4418 - val_accuracy: 0.4478\n",
      "Epoch 35/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4213 - accuracy: 0.4478 - val_loss: 0.4398 - val_accuracy: 0.4478\n",
      "Epoch 36/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4204 - accuracy: 0.4478 - val_loss: 0.4410 - val_accuracy: 0.4478\n",
      "Epoch 37/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4184 - accuracy: 0.4478 - val_loss: 0.4384 - val_accuracy: 0.4478\n",
      "Epoch 38/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4175 - accuracy: 0.4478 - val_loss: 0.4371 - val_accuracy: 0.4478\n",
      "Epoch 39/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4172 - accuracy: 0.4478 - val_loss: 0.4346 - val_accuracy: 0.4478\n",
      "Epoch 40/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4143 - accuracy: 0.4478 - val_loss: 0.4416 - val_accuracy: 0.4478\n",
      "Epoch 41/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4121 - accuracy: 0.4478 - val_loss: 0.4397 - val_accuracy: 0.4478\n",
      "Epoch 42/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4131 - accuracy: 0.4478 - val_loss: 0.4451 - val_accuracy: 0.4478\n",
      "Epoch 43/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4129 - accuracy: 0.4478 - val_loss: 0.4333 - val_accuracy: 0.4478\n",
      "Epoch 44/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4105 - accuracy: 0.4478 - val_loss: 0.4429 - val_accuracy: 0.4478\n",
      "Epoch 45/50\n",
      "377/377 [==============================] - 5s 15ms/step - loss: 0.4093 - accuracy: 0.4478 - val_loss: 0.4411 - val_accuracy: 0.4478\n",
      "Epoch 46/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4074 - accuracy: 0.4478 - val_loss: 0.4402 - val_accuracy: 0.4478\n",
      "Epoch 47/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4073 - accuracy: 0.4478 - val_loss: 0.4340 - val_accuracy: 0.4478\n",
      "Epoch 48/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4068 - accuracy: 0.4478 - val_loss: 0.4336 - val_accuracy: 0.4478\n",
      "Epoch 49/50\n",
      "377/377 [==============================] - 5s 15ms/step - loss: 0.4033 - accuracy: 0.4478 - val_loss: 0.4337 - val_accuracy: 0.4478\n",
      "Epoch 50/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4025 - accuracy: 0.4478 - val_loss: 0.4292 - val_accuracy: 0.4478\n",
      "377/377 [==============================] - 2s 5ms/step\n",
      "Epoch 1/50\n",
      "377/377 [==============================] - 7s 14ms/step - loss: 0.6805 - accuracy: 0.4478 - val_loss: 0.6460 - val_accuracy: 0.4478\n",
      "Epoch 2/50\n",
      "377/377 [==============================] - 6s 16ms/step - loss: 0.6328 - accuracy: 0.4478 - val_loss: 0.6147 - val_accuracy: 0.4478\n",
      "Epoch 3/50\n",
      "377/377 [==============================] - 5s 15ms/step - loss: 0.6060 - accuracy: 0.4478 - val_loss: 0.5942 - val_accuracy: 0.4478\n",
      "Epoch 4/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.5883 - accuracy: 0.4478 - val_loss: 0.5927 - val_accuracy: 0.4478\n",
      "Epoch 5/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.5714 - accuracy: 0.4478 - val_loss: 0.5604 - val_accuracy: 0.4478\n",
      "Epoch 6/50\n",
      "377/377 [==============================] - 5s 15ms/step - loss: 0.5558 - accuracy: 0.4478 - val_loss: 0.5458 - val_accuracy: 0.4478\n",
      "Epoch 7/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.5393 - accuracy: 0.4478 - val_loss: 0.5276 - val_accuracy: 0.4478\n",
      "Epoch 8/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.5230 - accuracy: 0.4478 - val_loss: 0.5147 - val_accuracy: 0.4478\n",
      "Epoch 9/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.5096 - accuracy: 0.4478 - val_loss: 0.5271 - val_accuracy: 0.4478\n",
      "Epoch 10/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4987 - accuracy: 0.4478 - val_loss: 0.4925 - val_accuracy: 0.4478\n",
      "Epoch 11/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4895 - accuracy: 0.4478 - val_loss: 0.4887 - val_accuracy: 0.4478\n",
      "Epoch 12/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4835 - accuracy: 0.4478 - val_loss: 0.4797 - val_accuracy: 0.4478\n",
      "Epoch 13/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4767 - accuracy: 0.4478 - val_loss: 0.4852 - val_accuracy: 0.4478\n",
      "Epoch 14/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4735 - accuracy: 0.4478 - val_loss: 0.4702 - val_accuracy: 0.4478\n",
      "Epoch 15/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4702 - accuracy: 0.4478 - val_loss: 0.4701 - val_accuracy: 0.4478\n",
      "Epoch 16/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4637 - accuracy: 0.4478 - val_loss: 0.4629 - val_accuracy: 0.4478\n",
      "Epoch 17/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4602 - accuracy: 0.4478 - val_loss: 0.4613 - val_accuracy: 0.4478\n",
      "Epoch 18/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4595 - accuracy: 0.4478 - val_loss: 0.4843 - val_accuracy: 0.4478\n",
      "Epoch 19/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4560 - accuracy: 0.4478 - val_loss: 0.4567 - val_accuracy: 0.4478\n",
      "Epoch 20/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4513 - accuracy: 0.4478 - val_loss: 0.4605 - val_accuracy: 0.4478\n",
      "Epoch 21/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4482 - accuracy: 0.4478 - val_loss: 0.4563 - val_accuracy: 0.4478\n",
      "Epoch 22/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4453 - accuracy: 0.4478 - val_loss: 0.4530 - val_accuracy: 0.4478\n",
      "Epoch 23/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4437 - accuracy: 0.4478 - val_loss: 0.4491 - val_accuracy: 0.4478\n",
      "Epoch 24/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4394 - accuracy: 0.4478 - val_loss: 0.4586 - val_accuracy: 0.4478\n",
      "Epoch 25/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4379 - accuracy: 0.4478 - val_loss: 0.4479 - val_accuracy: 0.4478\n",
      "Epoch 26/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4387 - accuracy: 0.4478 - val_loss: 0.4435 - val_accuracy: 0.4478\n",
      "Epoch 27/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4372 - accuracy: 0.4478 - val_loss: 0.4438 - val_accuracy: 0.4478\n",
      "Epoch 28/50\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.4320 - accuracy: 0.4478 - val_loss: 0.4434 - val_accuracy: 0.4478\n",
      "Epoch 29/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4313 - accuracy: 0.4478 - val_loss: 0.4510 - val_accuracy: 0.4478\n",
      "Epoch 30/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4301 - accuracy: 0.4478 - val_loss: 0.4407 - val_accuracy: 0.4478\n",
      "Epoch 31/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4283 - accuracy: 0.4478 - val_loss: 0.4438 - val_accuracy: 0.4478\n",
      "Epoch 32/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4255 - accuracy: 0.4478 - val_loss: 0.4418 - val_accuracy: 0.4478\n",
      "Epoch 33/50\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.4261 - accuracy: 0.4478 - val_loss: 0.4418 - val_accuracy: 0.4478\n",
      "Epoch 34/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4233 - accuracy: 0.4478 - val_loss: 0.4502 - val_accuracy: 0.4478\n",
      "Epoch 35/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4235 - accuracy: 0.4478 - val_loss: 0.4583 - val_accuracy: 0.4478\n",
      "Epoch 36/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4222 - accuracy: 0.4478 - val_loss: 0.4397 - val_accuracy: 0.4478\n",
      "Epoch 37/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4190 - accuracy: 0.4478 - val_loss: 0.4354 - val_accuracy: 0.4478\n",
      "Epoch 38/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4186 - accuracy: 0.4478 - val_loss: 0.4365 - val_accuracy: 0.4478\n",
      "Epoch 39/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4157 - accuracy: 0.4478 - val_loss: 0.4408 - val_accuracy: 0.4478\n",
      "Epoch 40/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4172 - accuracy: 0.4478 - val_loss: 0.4537 - val_accuracy: 0.4478\n",
      "Epoch 41/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4155 - accuracy: 0.4478 - val_loss: 0.4673 - val_accuracy: 0.4478\n",
      "Epoch 42/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4140 - accuracy: 0.4478 - val_loss: 0.4343 - val_accuracy: 0.4478\n",
      "Epoch 43/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4123 - accuracy: 0.4478 - val_loss: 0.4314 - val_accuracy: 0.4478\n",
      "Epoch 44/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4117 - accuracy: 0.4478 - val_loss: 0.4382 - val_accuracy: 0.4478\n",
      "Epoch 45/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4122 - accuracy: 0.4478 - val_loss: 0.4350 - val_accuracy: 0.4478\n",
      "Epoch 46/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4110 - accuracy: 0.4478 - val_loss: 0.4308 - val_accuracy: 0.4478\n",
      "Epoch 47/50\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.4103 - accuracy: 0.4478 - val_loss: 0.4350 - val_accuracy: 0.4478\n",
      "Epoch 48/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4098 - accuracy: 0.4478 - val_loss: 0.4365 - val_accuracy: 0.4478\n",
      "Epoch 49/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4068 - accuracy: 0.4478 - val_loss: 0.4320 - val_accuracy: 0.4478\n",
      "Epoch 50/50\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.4057 - accuracy: 0.4478 - val_loss: 0.4321 - val_accuracy: 0.4478\n",
      "377/377 [==============================] - 2s 5ms/step\n",
      "Accuracy: 0.4477785145888594\n",
      "Precision: 0.4477785145888594\n",
      "Recall: 1.0\n",
      "F1 Score: 0.6185732279858009\n",
      "ROC AUC: 0.5\n",
      "Specificity: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 2 Layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Reshape the data for use in a CNN model\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Define the k-fold cross-validation generator\n",
    "kfold = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "acc_scores = []\n",
    "prec_scores = []\n",
    "rec_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "spec_scores = []\n",
    "\n",
    "# Loop through the cross-validation splits\n",
    "for train, test in kfold.split(X):\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=2, activation='sigmoid', input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='sigmoid'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1 = 2 * ((precision*recall)/(precision+recall))\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Append the evaluation metrics to the lists\n",
    "    acc_scores.append(accuracy)\n",
    "    prec_scores.append(precision)\n",
    "    rec_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(auc)\n",
    "    spec_scores.append(specificity)\n",
    "\n",
    "# Compute the mean of the evaluation metrics\n",
    "mean_acc = np.mean(acc_scores)\n",
    "mean_prec = np.mean(prec_scores)\n",
    "mean_rec = np.mean(rec_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_auc = np.mean(auc_scores)\n",
    "mean_spec = np.mean(spec_scores)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", mean_acc)\n",
    "print(\"Precision:\", mean_prec)\n",
    "print(\"Recall:\", mean_rec)\n",
    "print(\"F1 Score:\", mean_f1)\n",
    "print(\"ROC AUC:\", mean_auc)\n",
    "print(\"Specificity:\", mean_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "41342e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "13f811d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Layers\n",
    "# Reshape the data for use in a CNN model\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a89cbbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), \n",
    "                                                                    tf.keras.metrics.Recall(), tf.keras.metrics.AUC(), \n",
    "                                                                    tf.keras.metrics.TrueNegatives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "16ecb4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "377/377 [==============================] - 10s 21ms/step - loss: 0.5893 - accuracy: 0.6877 - precision_1: 0.6857 - recall_1: 0.5589 - auc_1: 0.7502 - true_negatives_1: 5278.0000 - val_loss: 0.5229 - val_accuracy: 0.7458 - val_precision_1: 0.7217 - val_recall_1: 0.7036 - val_auc_1: 0.8182 - val_true_negatives_1: 5196.0000\n",
      "Epoch 2/30\n",
      "377/377 [==============================] - 7s 19ms/step - loss: 0.4988 - accuracy: 0.7636 - precision_1: 0.7599 - recall_1: 0.6901 - auc_1: 0.8314 - true_negatives_1: 5484.0000 - val_loss: 0.4815 - val_accuracy: 0.7797 - val_precision_1: 0.7990 - val_recall_1: 0.6786 - val_auc_1: 0.8436 - val_true_negatives_1: 5740.0000\n",
      "Epoch 3/30\n",
      "377/377 [==============================] - 7s 19ms/step - loss: 0.4638 - accuracy: 0.7846 - precision_1: 0.7869 - recall_1: 0.7118 - auc_1: 0.8546 - true_negatives_1: 5621.0000 - val_loss: 0.4720 - val_accuracy: 0.7913 - val_precision_1: 0.8847 - val_recall_1: 0.6138 - val_auc_1: 0.8654 - val_true_negatives_1: 6230.0000\n",
      "Epoch 4/30\n",
      "377/377 [==============================] - 7s 18ms/step - loss: 0.4456 - accuracy: 0.7938 - precision_1: 0.8028 - recall_1: 0.7151 - auc_1: 0.8660 - true_negatives_1: 5713.0000 - val_loss: 0.4466 - val_accuracy: 0.7948 - val_precision_1: 0.7764 - val_recall_1: 0.7608 - val_auc_1: 0.8675 - val_true_negatives_1: 5478.0000\n",
      "Epoch 5/30\n",
      "377/377 [==============================] - 6s 17ms/step - loss: 0.4278 - accuracy: 0.8054 - precision_1: 0.8151 - recall_1: 0.7312 - auc_1: 0.8752 - true_negatives_1: 5766.0000 - val_loss: 0.4395 - val_accuracy: 0.7976 - val_precision_1: 0.7930 - val_recall_1: 0.7416 - val_auc_1: 0.8702 - val_true_negatives_1: 5616.0000\n",
      "Epoch 6/30\n",
      "377/377 [==============================] - 7s 19ms/step - loss: 0.4170 - accuracy: 0.8093 - precision_1: 0.8152 - recall_1: 0.7423 - auc_1: 0.8822 - true_negatives_1: 5753.0000 - val_loss: 0.4564 - val_accuracy: 0.7919 - val_precision_1: 0.7586 - val_recall_1: 0.7849 - val_auc_1: 0.8687 - val_true_negatives_1: 5313.0000\n",
      "Epoch 7/30\n",
      "377/377 [==============================] - 7s 18ms/step - loss: 0.4083 - accuracy: 0.8127 - precision_1: 0.8225 - recall_1: 0.7418 - auc_1: 0.8868 - true_negatives_1: 5797.0000 - val_loss: 0.4392 - val_accuracy: 0.7896 - val_precision_1: 0.7557 - val_recall_1: 0.7834 - val_auc_1: 0.8724 - val_true_negatives_1: 5294.0000\n",
      "Epoch 8/30\n",
      "377/377 [==============================] - 7s 18ms/step - loss: 0.3952 - accuracy: 0.8190 - precision_1: 0.8327 - recall_1: 0.7455 - auc_1: 0.8938 - true_negatives_1: 5853.0000 - val_loss: 0.4300 - val_accuracy: 0.8015 - val_precision_1: 0.7910 - val_recall_1: 0.7566 - val_auc_1: 0.8743 - val_true_negatives_1: 5582.0000\n",
      "Epoch 9/30\n",
      "377/377 [==============================] - 7s 19ms/step - loss: 0.3877 - accuracy: 0.8239 - precision_1: 0.8339 - recall_1: 0.7575 - auc_1: 0.8986 - true_negatives_1: 5847.0000 - val_loss: 0.4309 - val_accuracy: 0.8081 - val_precision_1: 0.8658 - val_recall_1: 0.6762 - val_auc_1: 0.8784 - val_true_negatives_1: 6096.0000\n",
      "Epoch 10/30\n",
      "377/377 [==============================] - 8s 20ms/step - loss: 0.3808 - accuracy: 0.8261 - precision_1: 0.8371 - recall_1: 0.7593 - auc_1: 0.9025 - true_negatives_1: 5864.0000 - val_loss: 0.4297 - val_accuracy: 0.8137 - val_precision_1: 0.8675 - val_recall_1: 0.6894 - val_auc_1: 0.8789 - val_true_negatives_1: 6093.0000\n",
      "Epoch 11/30\n",
      "377/377 [==============================] - 7s 19ms/step - loss: 0.3787 - accuracy: 0.8287 - precision_1: 0.8412 - recall_1: 0.7612 - auc_1: 0.9035 - true_negatives_1: 5886.0000 - val_loss: 0.4429 - val_accuracy: 0.8012 - val_precision_1: 0.7774 - val_recall_1: 0.7792 - val_auc_1: 0.8744 - val_true_negatives_1: 5457.0000\n",
      "Epoch 12/30\n",
      "377/377 [==============================] - 7s 19ms/step - loss: 0.3629 - accuracy: 0.8389 - precision_1: 0.8527 - recall_1: 0.7740 - auc_1: 0.9122 - true_negatives_1: 5940.0000 - val_loss: 0.4474 - val_accuracy: 0.7863 - val_precision_1: 0.7417 - val_recall_1: 0.8021 - val_auc_1: 0.8748 - val_true_negatives_1: 5153.0000\n",
      "Epoch 13/30\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.3574 - accuracy: 0.8372 - precision_1: 0.8464 - recall_1: 0.7775 - auc_1: 0.9144 - true_negatives_1: 5900.0000 - val_loss: 0.4285 - val_accuracy: 0.8125 - val_precision_1: 0.8410 - val_recall_1: 0.7168 - val_auc_1: 0.8782 - val_true_negatives_1: 5930.0000\n",
      "Epoch 14/30\n",
      "377/377 [==============================] - 9s 23ms/step - loss: 0.3495 - accuracy: 0.8440 - precision_1: 0.8567 - recall_1: 0.7825 - auc_1: 0.9185 - true_negatives_1: 5955.0000 - val_loss: 0.4242 - val_accuracy: 0.8053 - val_precision_1: 0.8071 - val_recall_1: 0.7427 - val_auc_1: 0.8788 - val_true_negatives_1: 5703.0000\n",
      "Epoch 15/30\n",
      "377/377 [==============================] - 9s 23ms/step - loss: 0.3438 - accuracy: 0.8449 - precision_1: 0.8543 - recall_1: 0.7880 - auc_1: 0.9215 - true_negatives_1: 5936.0000 - val_loss: 0.4410 - val_accuracy: 0.8097 - val_precision_1: 0.8509 - val_recall_1: 0.6971 - val_auc_1: 0.8787 - val_true_negatives_1: 6002.0000\n",
      "Epoch 16/30\n",
      "377/377 [==============================] - 8s 21ms/step - loss: 0.3376 - accuracy: 0.8486 - precision_1: 0.8595 - recall_1: 0.7914 - auc_1: 0.9246 - true_negatives_1: 5963.0000 - val_loss: 0.4333 - val_accuracy: 0.8064 - val_precision_1: 0.8154 - val_recall_1: 0.7336 - val_auc_1: 0.8750 - val_true_negatives_1: 5765.0000\n",
      "Epoch 17/30\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.3287 - accuracy: 0.8534 - precision_1: 0.8650 - recall_1: 0.7971 - auc_1: 0.9284 - true_negatives_1: 5990.0000 - val_loss: 0.4283 - val_accuracy: 0.8037 - val_precision_1: 0.7884 - val_recall_1: 0.7677 - val_auc_1: 0.8807 - val_true_negatives_1: 5549.0000\n",
      "Epoch 18/30\n",
      "377/377 [==============================] - 9s 23ms/step - loss: 0.3265 - accuracy: 0.8545 - precision_1: 0.8640 - recall_1: 0.8012 - auc_1: 0.9299 - true_negatives_1: 5981.0000 - val_loss: 0.4430 - val_accuracy: 0.8123 - val_precision_1: 0.8563 - val_recall_1: 0.6981 - val_auc_1: 0.8801 - val_true_negatives_1: 6029.0000\n",
      "Epoch 19/30\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.3187 - accuracy: 0.8586 - precision_1: 0.8689 - recall_1: 0.8058 - auc_1: 0.9333 - true_negatives_1: 6005.0000 - val_loss: 0.4528 - val_accuracy: 0.7958 - val_precision_1: 0.7949 - val_recall_1: 0.7331 - val_auc_1: 0.8713 - val_true_negatives_1: 5640.0000\n",
      "Epoch 20/30\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.3148 - accuracy: 0.8626 - precision_1: 0.8711 - recall_1: 0.8134 - auc_1: 0.9354 - true_negatives_1: 6012.0000 - val_loss: 0.4491 - val_accuracy: 0.8048 - val_precision_1: 0.8420 - val_recall_1: 0.6944 - val_auc_1: 0.8757 - val_true_negatives_1: 5958.0000\n",
      "Epoch 21/30\n",
      "377/377 [==============================] - 9s 24ms/step - loss: 0.3043 - accuracy: 0.8642 - precision_1: 0.8759 - recall_1: 0.8117 - auc_1: 0.9396 - true_negatives_1: 6041.0000 - val_loss: 0.4627 - val_accuracy: 0.8035 - val_precision_1: 0.8115 - val_recall_1: 0.7308 - val_auc_1: 0.8710 - val_true_negatives_1: 5745.0000\n",
      "Epoch 22/30\n",
      "377/377 [==============================] - 8s 21ms/step - loss: 0.2963 - accuracy: 0.8714 - precision_1: 0.8803 - recall_1: 0.8249 - auc_1: 0.9431 - true_negatives_1: 6056.0000 - val_loss: 0.4618 - val_accuracy: 0.8045 - val_precision_1: 0.8276 - val_recall_1: 0.7116 - val_auc_1: 0.8769 - val_true_negatives_1: 5861.0000\n",
      "Epoch 23/30\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.2943 - accuracy: 0.8728 - precision_1: 0.8797 - recall_1: 0.8295 - auc_1: 0.9445 - true_negatives_1: 6049.0000 - val_loss: 0.4674 - val_accuracy: 0.8085 - val_precision_1: 0.8514 - val_recall_1: 0.6934 - val_auc_1: 0.8759 - val_true_negatives_1: 6008.0000\n",
      "Epoch 24/30\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.2897 - accuracy: 0.8729 - precision_1: 0.8795 - recall_1: 0.8299 - auc_1: 0.9460 - true_negatives_1: 6048.0000 - val_loss: 0.4678 - val_accuracy: 0.7865 - val_precision_1: 0.7528 - val_recall_1: 0.7790 - val_auc_1: 0.8724 - val_true_negatives_1: 5280.0000\n",
      "Epoch 25/30\n",
      "377/377 [==============================] - 8s 21ms/step - loss: 0.2796 - accuracy: 0.8777 - precision_1: 0.8842 - recall_1: 0.8365 - auc_1: 0.9501 - true_negatives_1: 6070.0000 - val_loss: 0.4873 - val_accuracy: 0.7999 - val_precision_1: 0.8017 - val_recall_1: 0.7349 - val_auc_1: 0.8690 - val_true_negatives_1: 5680.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.2775 - accuracy: 0.8781 - precision_1: 0.8828 - recall_1: 0.8391 - auc_1: 0.9507 - true_negatives_1: 6060.0000 - val_loss: 0.4787 - val_accuracy: 0.7984 - val_precision_1: 0.7983 - val_recall_1: 0.7357 - val_auc_1: 0.8731 - val_true_negatives_1: 5658.0000\n",
      "Epoch 27/30\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.2677 - accuracy: 0.8849 - precision_1: 0.8904 - recall_1: 0.8471 - auc_1: 0.9545 - true_negatives_1: 6099.0000 - val_loss: 0.4906 - val_accuracy: 0.7997 - val_precision_1: 0.8131 - val_recall_1: 0.7175 - val_auc_1: 0.8722 - val_true_negatives_1: 5771.0000\n",
      "Epoch 28/30\n",
      "377/377 [==============================] - 8s 21ms/step - loss: 0.2571 - accuracy: 0.8889 - precision_1: 0.8961 - recall_1: 0.8506 - auc_1: 0.9580 - true_negatives_1: 6129.0000 - val_loss: 0.5002 - val_accuracy: 0.7933 - val_precision_1: 0.7742 - val_recall_1: 0.7599 - val_auc_1: 0.8693 - val_true_negatives_1: 5465.0000\n",
      "Epoch 29/30\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.2581 - accuracy: 0.8883 - precision_1: 0.8941 - recall_1: 0.8514 - auc_1: 0.9578 - true_negatives_1: 6117.0000 - val_loss: 0.5028 - val_accuracy: 0.8051 - val_precision_1: 0.8206 - val_recall_1: 0.7229 - val_auc_1: 0.8727 - val_true_negatives_1: 5808.0000\n",
      "Epoch 30/30\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.2507 - accuracy: 0.8941 - precision_1: 0.8999 - recall_1: 0.8589 - auc_1: 0.9600 - true_negatives_1: 6146.0000 - val_loss: 0.5206 - val_accuracy: 0.8002 - val_precision_1: 0.8192 - val_recall_1: 0.7107 - val_auc_1: 0.8679 - val_true_negatives_1: 5815.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bc019f1cd0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a7955cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377/377 [==============================] - 3s 7ms/step - loss: 0.5206 - accuracy: 0.8002 - precision_1: 0.8192 - recall_1: 0.7107 - auc_1: 0.8679 - true_negatives_1: 5815.0000\n",
      "Test loss: 0.5206388831138611\n",
      "Test accuracy: 0.8002321124076843\n",
      "Test precision: 0.8192488551139832\n",
      "Test recall: 0.7106627225875854\n",
      "Test F1-score: 0.46026084625534985\n",
      "Test ROC AUC: 0.8678507804870605\n",
      "Test specificity: 5815.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "loss, accuracy, precision, recall, auc, specificity = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test precision: {precision}\")\n",
    "print(f\"Test recall: {recall}\")\n",
    "print(f\"Test F1-score: {2 * (precision * recall) / (precision + recall+1)}\")\n",
    "print(f\"Test ROC AUC: {auc}\")\n",
    "print(f\"Test specificity: {specificity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e687bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "483/483 [==============================] - 18s 31ms/step - loss: 0.5654 - accuracy: 0.7101 - precision_6: 0.7222 - recall_6: 0.5747 - auc_5: 0.7669 - false_positives_3: 1531.0000 - val_loss: 0.4598 - val_accuracy: 0.7739 - val_precision_6: 0.7972 - val_recall_6: 0.6499 - val_auc_5: 0.8469 - val_false_positives_3: 280.0000\n",
      "Epoch 2/50\n",
      "483/483 [==============================] - 13s 28ms/step - loss: 0.4750 - accuracy: 0.7696 - precision_6: 0.8133 - recall_6: 0.6313 - auc_5: 0.8373 - false_positives_3: 1004.0000 - val_loss: 0.4360 - val_accuracy: 0.8008 - val_precision_6: 0.8278 - val_recall_6: 0.6895 - val_auc_5: 0.8652 - val_false_positives_3: 243.0000\n",
      "Epoch 3/50\n",
      "483/483 [==============================] - 15s 31ms/step - loss: 0.4469 - accuracy: 0.7864 - precision_6: 0.8337 - recall_6: 0.6544 - auc_5: 0.8567 - false_positives_3: 904.0000 - val_loss: 0.4200 - val_accuracy: 0.8034 - val_precision_6: 0.8627 - val_recall_6: 0.6564 - val_auc_5: 0.8751 - val_false_positives_3: 177.0000\n",
      "Epoch 4/50\n",
      "483/483 [==============================] - 12s 26ms/step - loss: 0.4347 - accuracy: 0.7918 - precision_6: 0.8419 - recall_6: 0.6597 - auc_5: 0.8661 - false_positives_3: 858.0000 - val_loss: 0.4088 - val_accuracy: 0.8127 - val_precision_6: 0.8337 - val_recall_6: 0.7161 - val_auc_5: 0.8798 - val_false_positives_3: 242.0000\n",
      "Epoch 5/50\n",
      "483/483 [==============================] - 14s 30ms/step - loss: 0.4201 - accuracy: 0.8010 - precision_6: 0.8509 - recall_6: 0.6748 - auc_5: 0.8745 - false_positives_3: 819.0000 - val_loss: 0.4092 - val_accuracy: 0.8195 - val_precision_6: 0.8484 - val_recall_6: 0.7166 - val_auc_5: 0.8858 - val_false_positives_3: 217.0000\n",
      "Epoch 6/50\n",
      "483/483 [==============================] - 14s 29ms/step - loss: 0.4088 - accuracy: 0.8072 - precision_6: 0.8569 - recall_6: 0.6846 - auc_5: 0.8814 - false_positives_3: 792.0000 - val_loss: 0.4014 - val_accuracy: 0.8177 - val_precision_6: 0.8608 - val_recall_6: 0.6972 - val_auc_5: 0.8847 - val_false_positives_3: 191.0000\n",
      "Epoch 7/50\n",
      "483/483 [==============================] - 13s 26ms/step - loss: 0.4008 - accuracy: 0.8113 - precision_6: 0.8652 - recall_6: 0.6863 - auc_5: 0.8861 - false_positives_3: 741.0000 - val_loss: 0.4049 - val_accuracy: 0.8156 - val_precision_6: 0.9233 - val_recall_6: 0.6322 - val_auc_5: 0.8862 - val_false_positives_3: 89.0000\n",
      "Epoch 8/50\n",
      "483/483 [==============================] - 14s 28ms/step - loss: 0.3940 - accuracy: 0.8148 - precision_6: 0.8735 - recall_6: 0.6866 - auc_5: 0.8909 - false_positives_3: 689.0000 - val_loss: 0.3961 - val_accuracy: 0.8236 - val_precision_6: 0.8998 - val_recall_6: 0.6730 - val_auc_5: 0.8871 - val_false_positives_3: 127.0000\n",
      "Epoch 9/50\n",
      "483/483 [==============================] - 13s 26ms/step - loss: 0.3860 - accuracy: 0.8191 - precision_6: 0.8670 - recall_6: 0.7049 - auc_5: 0.8949 - false_positives_3: 749.0000 - val_loss: 0.3961 - val_accuracy: 0.8187 - val_precision_6: 0.8777 - val_recall_6: 0.6818 - val_auc_5: 0.8864 - val_false_positives_3: 161.0000\n",
      "Epoch 10/50\n",
      "483/483 [==============================] - 14s 30ms/step - loss: 0.3767 - accuracy: 0.8216 - precision_6: 0.8701 - recall_6: 0.7081 - auc_5: 0.9000 - false_positives_3: 732.0000 - val_loss: 0.3884 - val_accuracy: 0.8244 - val_precision_6: 0.8644 - val_recall_6: 0.7113 - val_auc_5: 0.8894 - val_false_positives_3: 189.0000\n",
      "Epoch 11/50\n",
      "483/483 [==============================] - 15s 31ms/step - loss: 0.3697 - accuracy: 0.8272 - precision_6: 0.8810 - recall_6: 0.7108 - auc_5: 0.9040 - false_positives_3: 665.0000 - val_loss: 0.3897 - val_accuracy: 0.8234 - val_precision_6: 0.8656 - val_recall_6: 0.7072 - val_auc_5: 0.8892 - val_false_positives_3: 186.0000\n",
      "Epoch 12/50\n",
      "483/483 [==============================] - 14s 30ms/step - loss: 0.3613 - accuracy: 0.8326 - precision_6: 0.8784 - recall_6: 0.7276 - auc_5: 0.9089 - false_positives_3: 698.0000 - val_loss: 0.4006 - val_accuracy: 0.8218 - val_precision_6: 0.8572 - val_recall_6: 0.7125 - val_auc_5: 0.8887 - val_false_positives_3: 201.0000\n",
      "Epoch 13/50\n",
      "483/483 [==============================] - 15s 31ms/step - loss: 0.3555 - accuracy: 0.8336 - precision_6: 0.8823 - recall_6: 0.7260 - auc_5: 0.9115 - false_positives_3: 671.0000 - val_loss: 0.3893 - val_accuracy: 0.8221 - val_precision_6: 0.8538 - val_recall_6: 0.7172 - val_auc_5: 0.8913 - val_false_positives_3: 208.0000\n",
      "Epoch 14/50\n",
      "483/483 [==============================] - 16s 32ms/step - loss: 0.3464 - accuracy: 0.8390 - precision_6: 0.8844 - recall_6: 0.7375 - auc_5: 0.9176 - false_positives_3: 668.0000 - val_loss: 0.3903 - val_accuracy: 0.8296 - val_precision_6: 0.8321 - val_recall_6: 0.7662 - val_auc_5: 0.8940 - val_false_positives_3: 262.0000\n",
      "Epoch 15/50\n",
      "483/483 [==============================] - 14s 29ms/step - loss: 0.3391 - accuracy: 0.8424 - precision_6: 0.8850 - recall_6: 0.7456 - auc_5: 0.9211 - false_positives_3: 671.0000 - val_loss: 0.3934 - val_accuracy: 0.8218 - val_precision_6: 0.8737 - val_recall_6: 0.6942 - val_auc_5: 0.8874 - val_false_positives_3: 170.0000\n",
      "Epoch 16/50\n",
      "483/483 [==============================] - 14s 29ms/step - loss: 0.3355 - accuracy: 0.8444 - precision_6: 0.8891 - recall_6: 0.7462 - auc_5: 0.9237 - false_positives_3: 645.0000 - val_loss: 0.3904 - val_accuracy: 0.8309 - val_precision_6: 0.8510 - val_recall_6: 0.7450 - val_auc_5: 0.8936 - val_false_positives_3: 221.0000\n",
      "Epoch 17/50\n",
      "483/483 [==============================] - 13s 27ms/step - loss: 0.3189 - accuracy: 0.8528 - precision_6: 0.8951 - recall_6: 0.7611 - auc_5: 0.9310 - false_positives_3: 618.0000 - val_loss: 0.3992 - val_accuracy: 0.8234 - val_precision_6: 0.8329 - val_recall_6: 0.7473 - val_auc_5: 0.8944 - val_false_positives_3: 254.0000\n",
      "Epoch 18/50\n",
      "483/483 [==============================] - 12s 25ms/step - loss: 0.3149 - accuracy: 0.8533 - precision_6: 0.8987 - recall_6: 0.7585 - auc_5: 0.9330 - false_positives_3: 592.0000 - val_loss: 0.3911 - val_accuracy: 0.8329 - val_precision_6: 0.8645 - val_recall_6: 0.7344 - val_auc_5: 0.8928 - val_false_positives_3: 195.0000\n",
      "Epoch 19/50\n",
      "483/483 [==============================] - 14s 29ms/step - loss: 0.3096 - accuracy: 0.8599 - precision_6: 0.9002 - recall_6: 0.7734 - auc_5: 0.9363 - false_positives_3: 594.0000 - val_loss: 0.4015 - val_accuracy: 0.8283 - val_precision_6: 0.8592 - val_recall_6: 0.7279 - val_auc_5: 0.8936 - val_false_positives_3: 202.0000\n",
      "Epoch 20/50\n",
      "483/483 [==============================] - 13s 26ms/step - loss: 0.2944 - accuracy: 0.8642 - precision_6: 0.9033 - recall_6: 0.7809 - auc_5: 0.9419 - false_positives_3: 579.0000 - val_loss: 0.4130 - val_accuracy: 0.8304 - val_precision_6: 0.8527 - val_recall_6: 0.7414 - val_auc_5: 0.8931 - val_false_positives_3: 217.0000\n"
     ]
    }
   ],
   "source": [
    "#4 Layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data for Conv1D input\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC(), tf.keras.metrics.FalsePositives()])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc21f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 1s 8ms/step\n",
      "Accuracy: 0.8118524658101948\n",
      "Precision: 0.8847314423657212\n",
      "Recall: 0.6715529088410445\n",
      "F1 Score: 0.7635416666666667\n",
      "ROC AUC: 0.8872991674219093\n",
      "Specificity: 0.9277336360196746\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model on test set\n",
    "accuracy = accuracy_score(y_test, y_pred.round())\n",
    "precision = precision_score(y_test, y_pred.round())\n",
    "recall = recall_score(y_test, y_pred.round())\n",
    "f1 = f1_score(y_test, y_pred.round())\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred.round()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2a7ffd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data for Conv1D input\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(256, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC(), keras.metrics.SpecificityAtSensitivity(0.5)])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cd6f3fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 2s 12ms/step - loss: 0.4896 - accuracy: 0.8090 - precision_10: 0.8536 - recall_10: 0.6972 - auc_7: 0.8843 - specificity_at_sensitivity_1: 0.9905\n",
      "Test loss: 0.489577978849411\n",
      "Test accuracy: 0.8089514970779419\n",
      "Test precision: 0.8536174893379211\n",
      "Test recall: 0.697205662727356\n",
      "Test F1 score: 0.4666312887176405\n",
      "Test ROC AUC: 0.8843411207199097\n",
      "Test specificity: 0.9995961904359905\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy, precision, recall, auc, fp = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test precision: {precision}\")\n",
    "print(f\"Test recall: {recall}\")\n",
    "print(f\"Test F1 score: {2*((precision*recall)/(precision+recall+1))}\")\n",
    "print(f\"Test ROC AUC: {auc}\")\n",
    "print(f\"Test specificity: {1-fp/(fp+tn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "99432c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy: 0.7104314565658569\n",
      "Fold: 2\n",
      "Accuracy: 0.7165237069129944\n",
      "Fold: 3\n",
      "Accuracy: 0.7117632627487183\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='tanh', input_shape=(X.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=32, kernel_size=2, activation='tanh'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters=32, kernel_size=2, activation='tanh'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters=16, kernel_size=2, activation='tanh'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters=8, kernel_size=2, activation='tanh'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='tanh'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the number of folds\n",
    "num_folds = 3\n",
    "\n",
    "# Define the cross-validation splits\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Iterate through the folds\n",
    "fold = 0\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    fold += 1\n",
    "    print('Fold:', fold)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Reshape the data for the CNN\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Create the model\n",
    "    model = create_model()\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c534d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
