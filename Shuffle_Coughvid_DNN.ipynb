{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1c15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('updated_coughvid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2650c66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc{1}</th>\n",
       "      <th>mfcc{2}</th>\n",
       "      <th>mfcc{3}</th>\n",
       "      <th>mfcc{4}</th>\n",
       "      <th>...</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>ddpJitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>aqpq5Shimmer</th>\n",
       "      <th>apq11Shimmer</th>\n",
       "      <th>ddaShimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065850</td>\n",
       "      <td>0.403767</td>\n",
       "      <td>1316.806414</td>\n",
       "      <td>1373.998076</td>\n",
       "      <td>2637.860622</td>\n",
       "      <td>0.057043</td>\n",
       "      <td>-396.59204</td>\n",
       "      <td>69.540160</td>\n",
       "      <td>2.152846</td>\n",
       "      <td>13.354017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.064553</td>\n",
       "      <td>0.187378</td>\n",
       "      <td>1.718899</td>\n",
       "      <td>0.089158</td>\n",
       "      <td>0.143649</td>\n",
       "      <td>0.352439</td>\n",
       "      <td>0.267473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033997</td>\n",
       "      <td>0.532892</td>\n",
       "      <td>2474.234037</td>\n",
       "      <td>2125.162327</td>\n",
       "      <td>4869.731365</td>\n",
       "      <td>0.186172</td>\n",
       "      <td>-435.21085</td>\n",
       "      <td>45.288998</td>\n",
       "      <td>-12.166409</td>\n",
       "      <td>10.258451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014014</td>\n",
       "      <td>0.018379</td>\n",
       "      <td>0.042043</td>\n",
       "      <td>0.130333</td>\n",
       "      <td>1.313323</td>\n",
       "      <td>0.049385</td>\n",
       "      <td>0.059807</td>\n",
       "      <td>0.110768</td>\n",
       "      <td>0.148156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.370873</td>\n",
       "      <td>2158.381678</td>\n",
       "      <td>2007.817231</td>\n",
       "      <td>4750.294555</td>\n",
       "      <td>0.125032</td>\n",
       "      <td>-412.62552</td>\n",
       "      <td>54.555480</td>\n",
       "      <td>-1.768253</td>\n",
       "      <td>3.977824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.037981</td>\n",
       "      <td>0.100577</td>\n",
       "      <td>0.272967</td>\n",
       "      <td>2.125802</td>\n",
       "      <td>0.132889</td>\n",
       "      <td>0.196880</td>\n",
       "      <td>0.160610</td>\n",
       "      <td>0.398668</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047035</td>\n",
       "      <td>0.479319</td>\n",
       "      <td>2678.491315</td>\n",
       "      <td>2139.232294</td>\n",
       "      <td>5136.450596</td>\n",
       "      <td>0.246256</td>\n",
       "      <td>-393.00226</td>\n",
       "      <td>48.030190</td>\n",
       "      <td>-29.901045</td>\n",
       "      <td>25.478853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>0.031223</td>\n",
       "      <td>0.096085</td>\n",
       "      <td>0.175612</td>\n",
       "      <td>1.617597</td>\n",
       "      <td>0.070595</td>\n",
       "      <td>0.094968</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.211784</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.741700</td>\n",
       "      <td>3316.010424</td>\n",
       "      <td>2345.969321</td>\n",
       "      <td>6007.887783</td>\n",
       "      <td>0.278178</td>\n",
       "      <td>-556.18726</td>\n",
       "      <td>10.974088</td>\n",
       "      <td>-10.207122</td>\n",
       "      <td>6.857212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023268</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.069803</td>\n",
       "      <td>0.138516</td>\n",
       "      <td>1.320449</td>\n",
       "      <td>0.075879</td>\n",
       "      <td>0.081973</td>\n",
       "      <td>0.145956</td>\n",
       "      <td>0.227636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24437</th>\n",
       "      <td>0.030827</td>\n",
       "      <td>0.429203</td>\n",
       "      <td>2974.741815</td>\n",
       "      <td>2265.905377</td>\n",
       "      <td>5435.183318</td>\n",
       "      <td>0.216093</td>\n",
       "      <td>-424.03302</td>\n",
       "      <td>38.712093</td>\n",
       "      <td>-16.247238</td>\n",
       "      <td>12.712377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029542</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>0.088625</td>\n",
       "      <td>0.202130</td>\n",
       "      <td>1.619779</td>\n",
       "      <td>0.075903</td>\n",
       "      <td>0.199575</td>\n",
       "      <td>0.160610</td>\n",
       "      <td>0.227709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24438</th>\n",
       "      <td>0.030711</td>\n",
       "      <td>0.535591</td>\n",
       "      <td>2719.621677</td>\n",
       "      <td>2132.117936</td>\n",
       "      <td>5260.719083</td>\n",
       "      <td>0.198633</td>\n",
       "      <td>-471.09518</td>\n",
       "      <td>28.604359</td>\n",
       "      <td>-8.991700</td>\n",
       "      <td>12.397835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019939</td>\n",
       "      <td>0.022426</td>\n",
       "      <td>0.059816</td>\n",
       "      <td>0.176616</td>\n",
       "      <td>1.347352</td>\n",
       "      <td>0.107413</td>\n",
       "      <td>0.148694</td>\n",
       "      <td>0.117848</td>\n",
       "      <td>0.322239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24439</th>\n",
       "      <td>0.159654</td>\n",
       "      <td>0.389324</td>\n",
       "      <td>2360.664509</td>\n",
       "      <td>1696.391140</td>\n",
       "      <td>4114.219514</td>\n",
       "      <td>0.169187</td>\n",
       "      <td>-206.32933</td>\n",
       "      <td>59.016940</td>\n",
       "      <td>-74.789270</td>\n",
       "      <td>-1.210189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019389</td>\n",
       "      <td>0.020776</td>\n",
       "      <td>0.058168</td>\n",
       "      <td>0.164419</td>\n",
       "      <td>1.429379</td>\n",
       "      <td>0.093083</td>\n",
       "      <td>0.119234</td>\n",
       "      <td>0.175221</td>\n",
       "      <td>0.279249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24440</th>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.451451</td>\n",
       "      <td>2788.705294</td>\n",
       "      <td>1800.585083</td>\n",
       "      <td>4708.568653</td>\n",
       "      <td>0.239603</td>\n",
       "      <td>-469.87784</td>\n",
       "      <td>59.399067</td>\n",
       "      <td>-41.076590</td>\n",
       "      <td>-2.028796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>0.036794</td>\n",
       "      <td>0.098542</td>\n",
       "      <td>0.205180</td>\n",
       "      <td>1.715662</td>\n",
       "      <td>0.105993</td>\n",
       "      <td>0.139264</td>\n",
       "      <td>0.166415</td>\n",
       "      <td>0.317979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24441</th>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>951.673749</td>\n",
       "      <td>828.363544</td>\n",
       "      <td>1897.664311</td>\n",
       "      <td>0.050561</td>\n",
       "      <td>-587.85986</td>\n",
       "      <td>24.133877</td>\n",
       "      <td>-4.841230</td>\n",
       "      <td>9.684202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.037088</td>\n",
       "      <td>0.084477</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>1.495134</td>\n",
       "      <td>0.047783</td>\n",
       "      <td>0.049730</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>0.143348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24442 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse  chroma_stft    spec_cent      spec_bw      rolloff       zcr  \\\n",
       "0      0.065850     0.403767  1316.806414  1373.998076  2637.860622  0.057043   \n",
       "1      0.033997     0.532892  2474.234037  2125.162327  4869.731365  0.186172   \n",
       "2      0.023310     0.370873  2158.381678  2007.817231  4750.294555  0.125032   \n",
       "3      0.047035     0.479319  2678.491315  2139.232294  5136.450596  0.246256   \n",
       "4      0.011785     0.741700  3316.010424  2345.969321  6007.887783  0.278178   \n",
       "...         ...          ...          ...          ...          ...       ...   \n",
       "24437  0.030827     0.429203  2974.741815  2265.905377  5435.183318  0.216093   \n",
       "24438  0.030711     0.535591  2719.621677  2132.117936  5260.719083  0.198633   \n",
       "24439  0.159654     0.389324  2360.664509  1696.391140  4114.219514  0.169187   \n",
       "24440  0.030981     0.451451  2788.705294  1800.585083  4708.568653  0.239603   \n",
       "24441  0.007046     0.167502   951.673749   828.363544  1897.664311  0.050561   \n",
       "\n",
       "         mfcc{1}    mfcc{2}    mfcc{3}    mfcc{4}  ...  rapJitter  ppq5Jitter  \\\n",
       "0     -396.59204  69.540160   2.152846  13.354017  ...   0.021518    0.024053   \n",
       "1     -435.21085  45.288998 -12.166409  10.258451  ...   0.014014    0.018379   \n",
       "2     -412.62552  54.555480  -1.768253   3.977824  ...   0.033526    0.037981   \n",
       "3     -393.00226  48.030190 -29.901045  25.478853  ...   0.032028    0.031223   \n",
       "4     -556.18726  10.974088 -10.207122   6.857212  ...   0.023268    0.026566   \n",
       "...          ...        ...        ...        ...  ...        ...         ...   \n",
       "24437 -424.03302  38.712093 -16.247238  12.712377  ...   0.029542    0.036789   \n",
       "24438 -471.09518  28.604359  -8.991700  12.397835  ...   0.019939    0.022426   \n",
       "24439 -206.32933  59.016940 -74.789270  -1.210189  ...   0.019389    0.020776   \n",
       "24440 -469.87784  59.399067 -41.076590  -2.028796  ...   0.032847    0.036794   \n",
       "24441 -587.85986  24.133877  -4.841230   9.684202  ...   0.028159    0.037088   \n",
       "\n",
       "       ddpJitter  localShimmer  localdbShimmer  apq3Shimmer  aqpq5Shimmer  \\\n",
       "0       0.064553      0.187378        1.718899     0.089158      0.143649   \n",
       "1       0.042043      0.130333        1.313323     0.049385      0.059807   \n",
       "2       0.100577      0.272967        2.125802     0.132889      0.196880   \n",
       "3       0.096085      0.175612        1.617597     0.070595      0.094968   \n",
       "4       0.069803      0.138516        1.320449     0.075879      0.081973   \n",
       "...          ...           ...             ...          ...           ...   \n",
       "24437   0.088625      0.202130        1.619779     0.075903      0.199575   \n",
       "24438   0.059816      0.176616        1.347352     0.107413      0.148694   \n",
       "24439   0.058168      0.164419        1.429379     0.093083      0.119234   \n",
       "24440   0.098542      0.205180        1.715662     0.105993      0.139264   \n",
       "24441   0.084477      0.119565        1.495134     0.047783      0.049730   \n",
       "\n",
       "       apq11Shimmer  ddaShimmer  label  \n",
       "0          0.352439    0.267473      0  \n",
       "1          0.110768    0.148156      1  \n",
       "2          0.160610    0.398668      1  \n",
       "3          0.154762    0.211784      0  \n",
       "4          0.145956    0.227636      0  \n",
       "...             ...         ...    ...  \n",
       "24437      0.160610    0.227709      1  \n",
       "24438      0.117848    0.322239      1  \n",
       "24439      0.175221    0.279249      0  \n",
       "24440      0.166415    0.317979      1  \n",
       "24441      0.021235    0.143348      0  \n",
       "\n",
       "[24442 rows x 80 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170fafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd083f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5386d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4ebd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DNN model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0489e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ceda148",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "995df0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "612/612 [==============================] - 3s 3ms/step - loss: 0.4718 - accuracy: 0.7947 - val_loss: 0.3902 - val_accuracy: 0.8517\n",
      "Epoch 2/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3855 - accuracy: 0.8511 - val_loss: 0.3601 - val_accuracy: 0.8705\n",
      "Epoch 3/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3616 - accuracy: 0.8616 - val_loss: 0.3446 - val_accuracy: 0.8752\n",
      "Epoch 4/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3490 - accuracy: 0.8661 - val_loss: 0.3409 - val_accuracy: 0.8765\n",
      "Epoch 5/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3403 - accuracy: 0.8695 - val_loss: 0.3379 - val_accuracy: 0.8763\n",
      "Epoch 6/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.3358 - accuracy: 0.8706 - val_loss: 0.3349 - val_accuracy: 0.8750\n",
      "Epoch 7/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.3302 - accuracy: 0.8742 - val_loss: 0.3318 - val_accuracy: 0.8783\n",
      "Epoch 8/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.3254 - accuracy: 0.8740 - val_loss: 0.3326 - val_accuracy: 0.8812\n",
      "Epoch 9/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.3224 - accuracy: 0.8767 - val_loss: 0.3325 - val_accuracy: 0.8789\n",
      "Epoch 10/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3199 - accuracy: 0.8776 - val_loss: 0.3293 - val_accuracy: 0.8769\n",
      "Epoch 11/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3184 - accuracy: 0.8770 - val_loss: 0.3290 - val_accuracy: 0.8773\n",
      "Epoch 12/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3158 - accuracy: 0.8780 - val_loss: 0.3269 - val_accuracy: 0.8801\n",
      "Epoch 13/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.3122 - accuracy: 0.8794 - val_loss: 0.3287 - val_accuracy: 0.8752\n",
      "Epoch 14/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3107 - accuracy: 0.8800 - val_loss: 0.3283 - val_accuracy: 0.8775\n",
      "Epoch 15/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.3112 - accuracy: 0.8788 - val_loss: 0.3321 - val_accuracy: 0.8738\n",
      "Epoch 16/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3071 - accuracy: 0.8814 - val_loss: 0.3279 - val_accuracy: 0.8812\n",
      "Epoch 17/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3060 - accuracy: 0.8809 - val_loss: 0.3265 - val_accuracy: 0.8803\n",
      "Epoch 18/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3049 - accuracy: 0.8809 - val_loss: 0.3335 - val_accuracy: 0.8775\n",
      "Epoch 19/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.3031 - accuracy: 0.8813 - val_loss: 0.3334 - val_accuracy: 0.8771\n",
      "Epoch 20/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3025 - accuracy: 0.8830 - val_loss: 0.3292 - val_accuracy: 0.8789\n",
      "Epoch 21/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.3002 - accuracy: 0.8827 - val_loss: 0.3328 - val_accuracy: 0.8777\n",
      "Epoch 22/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.2992 - accuracy: 0.8832 - val_loss: 0.3315 - val_accuracy: 0.8769\n",
      "Epoch 23/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2987 - accuracy: 0.8842 - val_loss: 0.3348 - val_accuracy: 0.8740\n",
      "Epoch 24/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.2970 - accuracy: 0.8842 - val_loss: 0.3296 - val_accuracy: 0.8785\n",
      "Epoch 25/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.2966 - accuracy: 0.8849 - val_loss: 0.3309 - val_accuracy: 0.8752\n",
      "Epoch 26/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.2949 - accuracy: 0.8840 - val_loss: 0.3298 - val_accuracy: 0.8793\n",
      "Epoch 27/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.2949 - accuracy: 0.8851 - val_loss: 0.3369 - val_accuracy: 0.8699\n",
      "Epoch 28/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.2960 - accuracy: 0.8843 - val_loss: 0.3317 - val_accuracy: 0.8765\n",
      "Epoch 29/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.2914 - accuracy: 0.8861 - val_loss: 0.3347 - val_accuracy: 0.8734\n",
      "Epoch 30/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2902 - accuracy: 0.8858 - val_loss: 0.3329 - val_accuracy: 0.8756\n",
      "Epoch 31/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.2911 - accuracy: 0.8859 - val_loss: 0.3349 - val_accuracy: 0.8795\n",
      "Epoch 32/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2915 - accuracy: 0.8856 - val_loss: 0.3347 - val_accuracy: 0.8779\n",
      "Epoch 33/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2898 - accuracy: 0.8857 - val_loss: 0.3331 - val_accuracy: 0.8785\n",
      "Epoch 34/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2878 - accuracy: 0.8855 - val_loss: 0.3360 - val_accuracy: 0.8758\n",
      "Epoch 35/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2880 - accuracy: 0.8871 - val_loss: 0.3358 - val_accuracy: 0.8773\n",
      "Epoch 36/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2879 - accuracy: 0.8872 - val_loss: 0.3381 - val_accuracy: 0.8787\n",
      "Epoch 37/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2859 - accuracy: 0.8868 - val_loss: 0.3359 - val_accuracy: 0.8744\n",
      "Epoch 38/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2853 - accuracy: 0.8882 - val_loss: 0.3361 - val_accuracy: 0.8754\n",
      "Epoch 39/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2851 - accuracy: 0.8862 - val_loss: 0.3370 - val_accuracy: 0.8734\n",
      "Epoch 40/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2860 - accuracy: 0.8870 - val_loss: 0.3369 - val_accuracy: 0.8740\n",
      "Epoch 41/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2835 - accuracy: 0.8898 - val_loss: 0.3359 - val_accuracy: 0.8748\n",
      "Epoch 42/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2830 - accuracy: 0.8891 - val_loss: 0.3380 - val_accuracy: 0.8760\n",
      "Epoch 43/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2831 - accuracy: 0.8884 - val_loss: 0.3432 - val_accuracy: 0.8697\n",
      "Epoch 44/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2815 - accuracy: 0.8891 - val_loss: 0.3462 - val_accuracy: 0.8664\n",
      "Epoch 45/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2831 - accuracy: 0.8877 - val_loss: 0.3403 - val_accuracy: 0.8744\n",
      "Epoch 46/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2800 - accuracy: 0.8890 - val_loss: 0.3411 - val_accuracy: 0.8760\n",
      "Epoch 47/50\n",
      "612/612 [==============================] - 2s 2ms/step - loss: 0.2815 - accuracy: 0.8886 - val_loss: 0.3436 - val_accuracy: 0.8699\n",
      "Epoch 48/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.2800 - accuracy: 0.8905 - val_loss: 0.3418 - val_accuracy: 0.8715\n",
      "Epoch 49/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2813 - accuracy: 0.8882 - val_loss: 0.3430 - val_accuracy: 0.8789\n",
      "Epoch 50/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2796 - accuracy: 0.8894 - val_loss: 0.3439 - val_accuracy: 0.8673\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b31cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 0s 2ms/step - loss: 0.3439 - accuracy: 0.8673\n",
      "Test accuracy: 0.8672530055046082\n",
      "153/153 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Save predictions and corresponding file names to a CSV file\n",
    "results = pd.DataFrame({'label': y_test, 'prediction': predictions.flatten()})\n",
    "results.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad81a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.8672530169768868\n",
      "Precision: 0.9147025813692481\n",
      "Recall: 0.76633756464504\n",
      "F1 Score: 0.8339728830903045\n",
      "ROC AUC: 0.8556524897808835\n",
      "Specificity: 0.944967414916727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"Specificity: {specificity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a89b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "612/612 [==============================] - 7s 6ms/step - loss: 0.4177 - accuracy: 0.8303 - precision: 0.8691 - recall: 0.7198 - auc: 0.8654 - false_positives: 926.0000 - val_loss: 0.3635 - val_accuracy: 0.8640 - val_precision: 0.9632 - val_recall: 0.7146 - val_auc: 0.8835 - val_false_positives: 58.0000\n",
      "Epoch 2/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3452 - accuracy: 0.8660 - precision: 0.9397 - recall: 0.7407 - auc: 0.8907 - false_positives: 406.0000 - val_loss: 0.3310 - val_accuracy: 0.8744 - val_precision: 0.9621 - val_recall: 0.7405 - val_auc: 0.8918 - val_false_positives: 62.0000\n",
      "Epoch 3/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3259 - accuracy: 0.8727 - precision: 0.9564 - recall: 0.7423 - auc: 0.8995 - false_positives: 289.0000 - val_loss: 0.3228 - val_accuracy: 0.8758 - val_precision: 0.9668 - val_recall: 0.7400 - val_auc: 0.8900 - val_false_positives: 54.0000\n",
      "Epoch 4/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3154 - accuracy: 0.8785 - precision: 0.9667 - recall: 0.7477 - auc: 0.9054 - false_positives: 220.0000 - val_loss: 0.3260 - val_accuracy: 0.8791 - val_precision: 0.9788 - val_recall: 0.7381 - val_auc: 0.8912 - val_false_positives: 34.0000\n",
      "Epoch 5/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.3100 - accuracy: 0.8790 - precision: 0.9699 - recall: 0.7463 - auc: 0.9085 - false_positives: 198.0000 - val_loss: 0.3228 - val_accuracy: 0.8783 - val_precision: 0.9752 - val_recall: 0.7391 - val_auc: 0.8903 - val_false_positives: 40.0000\n",
      "Epoch 6/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2994 - accuracy: 0.8828 - precision: 0.9735 - recall: 0.7523 - auc: 0.9149 - false_positives: 175.0000 - val_loss: 0.3282 - val_accuracy: 0.8754 - val_precision: 0.9539 - val_recall: 0.7499 - val_auc: 0.8913 - val_false_positives: 77.0000\n",
      "Epoch 7/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2933 - accuracy: 0.8845 - precision: 0.9752 - recall: 0.7548 - auc: 0.9190 - false_positives: 164.0000 - val_loss: 0.3243 - val_accuracy: 0.8783 - val_precision: 0.9659 - val_recall: 0.7466 - val_auc: 0.8919 - val_false_positives: 56.0000\n",
      "Epoch 8/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2857 - accuracy: 0.8865 - precision: 0.9743 - recall: 0.7601 - auc: 0.9244 - false_positives: 171.0000 - val_loss: 0.3329 - val_accuracy: 0.8785 - val_precision: 0.9746 - val_recall: 0.7400 - val_auc: 0.8866 - val_false_positives: 41.0000\n",
      "Epoch 9/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2787 - accuracy: 0.8885 - precision: 0.9745 - recall: 0.7647 - auc: 0.9285 - false_positives: 171.0000 - val_loss: 0.3329 - val_accuracy: 0.8781 - val_precision: 0.9676 - val_recall: 0.7447 - val_auc: 0.8842 - val_false_positives: 53.0000\n",
      "Epoch 10/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2703 - accuracy: 0.8903 - precision: 0.9750 - recall: 0.7685 - auc: 0.9348 - false_positives: 168.0000 - val_loss: 0.3293 - val_accuracy: 0.8781 - val_precision: 0.9688 - val_recall: 0.7438 - val_auc: 0.8861 - val_false_positives: 51.0000\n",
      "Epoch 11/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2640 - accuracy: 0.8950 - precision: 0.9762 - recall: 0.7785 - auc: 0.9386 - false_positives: 162.0000 - val_loss: 0.3682 - val_accuracy: 0.8734 - val_precision: 0.9707 - val_recall: 0.7311 - val_auc: 0.8805 - val_false_positives: 47.0000\n",
      "Epoch 12/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2567 - accuracy: 0.8964 - precision: 0.9742 - recall: 0.7836 - auc: 0.9431 - false_positives: 177.0000 - val_loss: 0.3576 - val_accuracy: 0.8724 - val_precision: 0.9552 - val_recall: 0.7414 - val_auc: 0.8825 - val_false_positives: 74.0000\n",
      "Epoch 13/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2470 - accuracy: 0.9001 - precision: 0.9751 - recall: 0.7916 - auc: 0.9480 - false_positives: 173.0000 - val_loss: 0.3535 - val_accuracy: 0.8769 - val_precision: 0.9580 - val_recall: 0.7499 - val_auc: 0.8842 - val_false_positives: 70.0000\n",
      "Epoch 14/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2400 - accuracy: 0.9034 - precision: 0.9732 - recall: 0.8008 - auc: 0.9521 - false_positives: 188.0000 - val_loss: 0.3671 - val_accuracy: 0.8732 - val_precision: 0.9472 - val_recall: 0.7504 - val_auc: 0.8816 - val_false_positives: 89.0000\n",
      "Epoch 15/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2306 - accuracy: 0.9073 - precision: 0.9751 - recall: 0.8085 - auc: 0.9564 - false_positives: 176.0000 - val_loss: 0.3653 - val_accuracy: 0.8640 - val_precision: 0.9075 - val_recall: 0.7654 - val_auc: 0.8846 - val_false_positives: 166.0000\n",
      "Epoch 16/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2216 - accuracy: 0.9098 - precision: 0.9737 - recall: 0.8156 - auc: 0.9604 - false_positives: 188.0000 - val_loss: 0.3918 - val_accuracy: 0.8513 - val_precision: 0.8688 - val_recall: 0.7753 - val_auc: 0.8830 - val_false_positives: 249.0000\n",
      "Epoch 17/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.2142 - accuracy: 0.9133 - precision: 0.9728 - recall: 0.8245 - auc: 0.9632 - false_positives: 197.0000 - val_loss: 0.3923 - val_accuracy: 0.8589 - val_precision: 0.8916 - val_recall: 0.7692 - val_auc: 0.8804 - val_false_positives: 199.0000\n",
      "Epoch 18/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2073 - accuracy: 0.9154 - precision: 0.9698 - recall: 0.8321 - auc: 0.9663 - false_positives: 221.0000 - val_loss: 0.4173 - val_accuracy: 0.8548 - val_precision: 0.8763 - val_recall: 0.7757 - val_auc: 0.8816 - val_false_positives: 233.0000\n",
      "Epoch 19/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2003 - accuracy: 0.9192 - precision: 0.9733 - recall: 0.8381 - auc: 0.9693 - false_positives: 196.0000 - val_loss: 0.4142 - val_accuracy: 0.8576 - val_precision: 0.9063 - val_recall: 0.7504 - val_auc: 0.8763 - val_false_positives: 165.0000\n",
      "Epoch 20/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1893 - accuracy: 0.9235 - precision: 0.9738 - recall: 0.8477 - auc: 0.9725 - false_positives: 195.0000 - val_loss: 0.4279 - val_accuracy: 0.8585 - val_precision: 0.8872 - val_recall: 0.7729 - val_auc: 0.8828 - val_false_positives: 209.0000\n",
      "Epoch 21/50\n",
      "612/612 [==============================] - 3s 5ms/step - loss: 0.1820 - accuracy: 0.9278 - precision: 0.9750 - recall: 0.8568 - auc: 0.9752 - false_positives: 188.0000 - val_loss: 0.4475 - val_accuracy: 0.8552 - val_precision: 0.8760 - val_recall: 0.7772 - val_auc: 0.8810 - val_false_positives: 234.0000\n",
      "Epoch 22/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1777 - accuracy: 0.9293 - precision: 0.9705 - recall: 0.8643 - auc: 0.9766 - false_positives: 224.0000 - val_loss: 0.4566 - val_accuracy: 0.8609 - val_precision: 0.9008 - val_recall: 0.7645 - val_auc: 0.8774 - val_false_positives: 179.0000\n",
      "Epoch 23/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1659 - accuracy: 0.9348 - precision: 0.9755 - recall: 0.8727 - auc: 0.9800 - false_positives: 187.0000 - val_loss: 0.4806 - val_accuracy: 0.8362 - val_precision: 0.8289 - val_recall: 0.7856 - val_auc: 0.8776 - val_false_positives: 345.0000\n",
      "Epoch 24/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1614 - accuracy: 0.9359 - precision: 0.9726 - recall: 0.8780 - auc: 0.9816 - false_positives: 211.0000 - val_loss: 0.4651 - val_accuracy: 0.8437 - val_precision: 0.8493 - val_recall: 0.7790 - val_auc: 0.8798 - val_false_positives: 294.0000\n",
      "Epoch 25/50\n",
      "612/612 [==============================] - 3s 5ms/step - loss: 0.1549 - accuracy: 0.9382 - precision: 0.9721 - recall: 0.8839 - auc: 0.9829 - false_positives: 217.0000 - val_loss: 0.5083 - val_accuracy: 0.8497 - val_precision: 0.8730 - val_recall: 0.7659 - val_auc: 0.8733 - val_false_positives: 237.0000\n",
      "Epoch 26/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1449 - accuracy: 0.9439 - precision: 0.9751 - recall: 0.8945 - auc: 0.9853 - false_positives: 195.0000 - val_loss: 0.5166 - val_accuracy: 0.8523 - val_precision: 0.8759 - val_recall: 0.7696 - val_auc: 0.8757 - val_false_positives: 232.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.1481 - accuracy: 0.9410 - precision: 0.9684 - recall: 0.8940 - auc: 0.9845 - false_positives: 249.0000 - val_loss: 0.5504 - val_accuracy: 0.8486 - val_precision: 0.8767 - val_recall: 0.7588 - val_auc: 0.8725 - val_false_positives: 227.0000\n",
      "Epoch 28/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1331 - accuracy: 0.9482 - precision: 0.9749 - recall: 0.9047 - auc: 0.9879 - false_positives: 199.0000 - val_loss: 0.5566 - val_accuracy: 0.8632 - val_precision: 0.9137 - val_recall: 0.7569 - val_auc: 0.8743 - val_false_positives: 152.0000\n",
      "Epoch 29/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1321 - accuracy: 0.9492 - precision: 0.9752 - recall: 0.9068 - auc: 0.9881 - false_positives: 197.0000 - val_loss: 0.5365 - val_accuracy: 0.8419 - val_precision: 0.8511 - val_recall: 0.7715 - val_auc: 0.8761 - val_false_positives: 287.0000\n",
      "Epoch 30/50\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.1525 - accuracy: 0.9409 - precision: 0.9649 - recall: 0.8973 - auc: 0.9833 - false_positives: 279.0000 - val_loss: 0.5706 - val_accuracy: 0.8319 - val_precision: 0.8287 - val_recall: 0.7734 - val_auc: 0.8737 - val_false_positives: 340.0000\n",
      "Epoch 31/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1216 - accuracy: 0.9544 - precision: 0.9744 - recall: 0.9198 - auc: 0.9902 - false_positives: 206.0000 - val_loss: 0.5966 - val_accuracy: 0.8351 - val_precision: 0.8281 - val_recall: 0.7837 - val_auc: 0.8745 - val_false_positives: 346.0000\n",
      "Epoch 32/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1125 - accuracy: 0.9577 - precision: 0.9771 - recall: 0.9247 - auc: 0.9917 - false_positives: 185.0000 - val_loss: 0.6062 - val_accuracy: 0.8542 - val_precision: 0.8897 - val_recall: 0.7588 - val_auc: 0.8724 - val_false_positives: 200.0000\n",
      "Epoch 33/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1043 - accuracy: 0.9605 - precision: 0.9787 - recall: 0.9299 - auc: 0.9932 - false_positives: 173.0000 - val_loss: 0.6088 - val_accuracy: 0.8429 - val_precision: 0.8530 - val_recall: 0.7720 - val_auc: 0.8745 - val_false_positives: 283.0000\n",
      "Epoch 34/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1057 - accuracy: 0.9618 - precision: 0.9790 - recall: 0.9327 - auc: 0.9927 - false_positives: 171.0000 - val_loss: 0.6375 - val_accuracy: 0.8495 - val_precision: 0.8678 - val_recall: 0.7715 - val_auc: 0.8742 - val_false_positives: 250.0000\n",
      "Epoch 35/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1012 - accuracy: 0.9608 - precision: 0.9774 - recall: 0.9319 - auc: 0.9936 - false_positives: 184.0000 - val_loss: 0.6634 - val_accuracy: 0.8364 - val_precision: 0.8346 - val_recall: 0.7781 - val_auc: 0.8723 - val_false_positives: 328.0000\n",
      "Epoch 36/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1010 - accuracy: 0.9618 - precision: 0.9772 - recall: 0.9343 - auc: 0.9935 - false_positives: 186.0000 - val_loss: 0.6757 - val_accuracy: 0.8439 - val_precision: 0.8586 - val_recall: 0.7677 - val_auc: 0.8744 - val_false_positives: 269.0000\n",
      "Epoch 37/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0920 - accuracy: 0.9669 - precision: 0.9811 - recall: 0.9424 - auc: 0.9948 - false_positives: 155.0000 - val_loss: 0.6738 - val_accuracy: 0.8323 - val_precision: 0.8263 - val_recall: 0.7781 - val_auc: 0.8751 - val_false_positives: 348.0000\n",
      "Epoch 38/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.9684 - precision: 0.9836 - recall: 0.9434 - auc: 0.9953 - false_positives: 134.0000 - val_loss: 0.7010 - val_accuracy: 0.8437 - val_precision: 0.8537 - val_recall: 0.7734 - val_auc: 0.8717 - val_false_positives: 282.0000\n",
      "Epoch 39/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0928 - accuracy: 0.9672 - precision: 0.9787 - recall: 0.9456 - auc: 0.9943 - false_positives: 176.0000 - val_loss: 0.7520 - val_accuracy: 0.8092 - val_precision: 0.7774 - val_recall: 0.7866 - val_auc: 0.8691 - val_false_positives: 479.0000\n",
      "Epoch 40/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0854 - accuracy: 0.9692 - precision: 0.9795 - recall: 0.9494 - auc: 0.9956 - false_positives: 170.0000 - val_loss: 0.7432 - val_accuracy: 0.8251 - val_precision: 0.8075 - val_recall: 0.7851 - val_auc: 0.8728 - val_false_positives: 398.0000\n",
      "Epoch 41/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0705 - accuracy: 0.9750 - precision: 0.9855 - recall: 0.9569 - auc: 0.9971 - false_positives: 120.0000 - val_loss: 0.7418 - val_accuracy: 0.8378 - val_precision: 0.8389 - val_recall: 0.7762 - val_auc: 0.8756 - val_false_positives: 317.0000\n",
      "Epoch 42/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0727 - accuracy: 0.9751 - precision: 0.9844 - recall: 0.9583 - auc: 0.9968 - false_positives: 130.0000 - val_loss: 0.8273 - val_accuracy: 0.8460 - val_precision: 0.8686 - val_recall: 0.7612 - val_auc: 0.8697 - val_false_positives: 245.0000\n",
      "Epoch 43/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0878 - accuracy: 0.9708 - precision: 0.9787 - recall: 0.9540 - auc: 0.9954 - false_positives: 177.0000 - val_loss: 0.7763 - val_accuracy: 0.8370 - val_precision: 0.8325 - val_recall: 0.7828 - val_auc: 0.8759 - val_false_positives: 335.0000\n",
      "Epoch 44/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.0785 - accuracy: 0.9717 - precision: 0.9796 - recall: 0.9552 - auc: 0.9961 - false_positives: 170.0000 - val_loss: 0.8073 - val_accuracy: 0.8341 - val_precision: 0.8307 - val_recall: 0.7772 - val_auc: 0.8729 - val_false_positives: 337.0000\n",
      "Epoch 45/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0669 - accuracy: 0.9775 - precision: 0.9848 - recall: 0.9634 - auc: 0.9972 - false_positives: 127.0000 - val_loss: 0.8389 - val_accuracy: 0.8376 - val_precision: 0.8385 - val_recall: 0.7762 - val_auc: 0.8710 - val_false_positives: 318.0000\n",
      "Epoch 46/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0546 - accuracy: 0.9836 - precision: 0.9902 - recall: 0.9721 - auc: 0.9986 - false_positives: 82.0000 - val_loss: 0.8359 - val_accuracy: 0.8333 - val_precision: 0.8264 - val_recall: 0.7809 - val_auc: 0.8723 - val_false_positives: 349.0000\n",
      "Epoch 47/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.0605 - accuracy: 0.9791 - precision: 0.9857 - recall: 0.9663 - auc: 0.9977 - false_positives: 120.0000 - val_loss: 0.8819 - val_accuracy: 0.8335 - val_precision: 0.8297 - val_recall: 0.7767 - val_auc: 0.8713 - val_false_positives: 339.0000\n",
      "Epoch 48/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.0694 - accuracy: 0.9756 - precision: 0.9810 - recall: 0.9628 - auc: 0.9968 - false_positives: 159.0000 - val_loss: 0.8887 - val_accuracy: 0.8390 - val_precision: 0.8490 - val_recall: 0.7663 - val_auc: 0.8718 - val_false_positives: 290.0000\n",
      "Epoch 49/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0804 - accuracy: 0.9723 - precision: 0.9802 - recall: 0.9559 - auc: 0.9952 - false_positives: 165.0000 - val_loss: 0.9264 - val_accuracy: 0.8370 - val_precision: 0.8359 - val_recall: 0.7781 - val_auc: 0.8704 - val_false_positives: 325.0000\n",
      "Epoch 50/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0554 - accuracy: 0.9824 - precision: 0.9867 - recall: 0.9728 - auc: 0.9981 - false_positives: 112.0000 - val_loss: 0.9123 - val_accuracy: 0.8280 - val_precision: 0.8127 - val_recall: 0.7856 - val_auc: 0.8684 - val_false_positives: 385.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ce5fb35580>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC(), keras.metrics.FalsePositives()])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aed7e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 0s 3ms/step - loss: 0.9123 - accuracy: 0.8280 - precision: 0.8127 - recall: 0.7856 - auc: 0.8684 - false_positives: 385.0000\n",
      "Test loss: 0.9123197793960571\n",
      "Test accuracy: 0.827981173992157\n",
      "Test precision: 0.8127431869506836\n",
      "Test recall: 0.7856135368347168\n",
      "Test F1 score: 0.49146604374510067\n",
      "Test ROC AUC: 0.8684041500091553\n",
      "Test specificity: 0.8714524207011686\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy, precision, recall, auc, fp = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test precision: {precision}\")\n",
    "print(f\"Test recall: {recall}\")\n",
    "print(f\"Test F1 score: {2*((precision*recall)/(precision+recall+1))}\")\n",
    "print(f\"Test ROC AUC: {auc}\")\n",
    "print(f\"Test specificity: {1-fp/(fp+tn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53e24a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "612/612 [==============================] - 6s 5ms/step - loss: 0.4366 - accuracy: 0.8150 - precision_1: 0.8567 - recall_1: 0.6922 - auc_1: 0.8575 - false_positives_1: 989.0000 - val_loss: 0.3602 - val_accuracy: 0.8597 - val_precision_1: 0.9167 - val_recall_1: 0.7452 - val_auc_1: 0.8823 - val_false_positives_1: 144.0000\n",
      "Epoch 2/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.3492 - accuracy: 0.8641 - precision_1: 0.9415 - recall_1: 0.7346 - auc_1: 0.8885 - false_positives_1: 390.0000 - val_loss: 0.3462 - val_accuracy: 0.8673 - val_precision_1: 0.9409 - val_recall_1: 0.7414 - val_auc_1: 0.8835 - val_false_positives_1: 99.0000\n",
      "Epoch 3/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.3276 - accuracy: 0.8732 - precision_1: 0.9609 - recall_1: 0.7397 - auc_1: 0.8966 - false_positives_1: 257.0000 - val_loss: 0.3253 - val_accuracy: 0.8769 - val_precision_1: 0.9757 - val_recall_1: 0.7353 - val_auc_1: 0.8887 - val_false_positives_1: 39.0000\n",
      "Epoch 4/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.3182 - accuracy: 0.8768 - precision_1: 0.9690 - recall_1: 0.7417 - auc_1: 0.9003 - false_positives_1: 203.0000 - val_loss: 0.3206 - val_accuracy: 0.8781 - val_precision_1: 0.9653 - val_recall_1: 0.7466 - val_auc_1: 0.8922 - val_false_positives_1: 57.0000\n",
      "Epoch 5/50\n",
      "612/612 [==============================] - 3s 5ms/step - loss: 0.3126 - accuracy: 0.8783 - precision_1: 0.9717 - recall_1: 0.7430 - auc_1: 0.9039 - false_positives_1: 185.0000 - val_loss: 0.3180 - val_accuracy: 0.8799 - val_precision_1: 0.9777 - val_recall_1: 0.7409 - val_auc_1: 0.8915 - val_false_positives_1: 36.0000\n",
      "Epoch 6/50\n",
      "612/612 [==============================] - 3s 5ms/step - loss: 0.3056 - accuracy: 0.8812 - precision_1: 0.9773 - recall_1: 0.7453 - auc_1: 0.9091 - false_positives_1: 148.0000 - val_loss: 0.3231 - val_accuracy: 0.8779 - val_precision_1: 0.9728 - val_recall_1: 0.7400 - val_auc_1: 0.8895 - val_false_positives_1: 44.0000\n",
      "Epoch 7/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.3121 - accuracy: 0.8769 - precision_1: 0.9782 - recall_1: 0.7346 - auc_1: 0.9066 - false_positives_1: 140.0000 - val_loss: 0.3193 - val_accuracy: 0.8795 - val_precision_1: 0.9706 - val_recall_1: 0.7457 - val_auc_1: 0.8910 - val_false_positives_1: 48.0000\n",
      "Epoch 8/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2961 - accuracy: 0.8842 - precision_1: 0.9800 - recall_1: 0.7503 - auc_1: 0.9148 - false_positives_1: 131.0000 - val_loss: 0.3218 - val_accuracy: 0.8797 - val_precision_1: 0.9812 - val_recall_1: 0.7377 - val_auc_1: 0.8880 - val_false_positives_1: 30.0000\n",
      "Epoch 9/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.2913 - accuracy: 0.8864 - precision_1: 0.9820 - recall_1: 0.7538 - auc_1: 0.9192 - false_positives_1: 118.0000 - val_loss: 0.3313 - val_accuracy: 0.8728 - val_precision_1: 0.9536 - val_recall_1: 0.7438 - val_auc_1: 0.8890 - val_false_positives_1: 77.0000\n",
      "Epoch 10/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2888 - accuracy: 0.8865 - precision_1: 0.9811 - recall_1: 0.7546 - auc_1: 0.9212 - false_positives_1: 124.0000 - val_loss: 0.3294 - val_accuracy: 0.8765 - val_precision_1: 0.9640 - val_recall_1: 0.7438 - val_auc_1: 0.8869 - val_false_positives_1: 59.0000\n",
      "Epoch 11/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2837 - accuracy: 0.8889 - precision_1: 0.9821 - recall_1: 0.7595 - auc_1: 0.9241 - false_positives_1: 118.0000 - val_loss: 0.3304 - val_accuracy: 0.8777 - val_precision_1: 0.9653 - val_recall_1: 0.7457 - val_auc_1: 0.8859 - val_false_positives_1: 57.0000\n",
      "Epoch 12/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.2778 - accuracy: 0.8891 - precision_1: 0.9819 - recall_1: 0.7601 - auc_1: 0.9275 - false_positives_1: 120.0000 - val_loss: 0.3387 - val_accuracy: 0.8763 - val_precision_1: 0.9590 - val_recall_1: 0.7475 - val_auc_1: 0.8857 - val_false_positives_1: 68.0000\n",
      "Epoch 13/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2760 - accuracy: 0.8894 - precision_1: 0.9778 - recall_1: 0.7641 - auc_1: 0.9307 - false_positives_1: 148.0000 - val_loss: 0.3365 - val_accuracy: 0.8805 - val_precision_1: 0.9801 - val_recall_1: 0.7405 - val_auc_1: 0.8848 - val_false_positives_1: 32.0000\n",
      "Epoch 14/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2700 - accuracy: 0.8915 - precision_1: 0.9813 - recall_1: 0.7662 - auc_1: 0.9334 - false_positives_1: 125.0000 - val_loss: 0.3498 - val_accuracy: 0.8734 - val_precision_1: 0.9504 - val_recall_1: 0.7480 - val_auc_1: 0.8833 - val_false_positives_1: 83.0000\n",
      "Epoch 15/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2665 - accuracy: 0.8924 - precision_1: 0.9780 - recall_1: 0.7711 - auc_1: 0.9353 - false_positives_1: 148.0000 - val_loss: 0.3550 - val_accuracy: 0.8650 - val_precision_1: 0.9189 - val_recall_1: 0.7565 - val_auc_1: 0.8831 - val_false_positives_1: 142.0000\n",
      "Epoch 16/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2615 - accuracy: 0.8942 - precision_1: 0.9793 - recall_1: 0.7743 - auc_1: 0.9386 - false_positives_1: 140.0000 - val_loss: 0.3571 - val_accuracy: 0.8701 - val_precision_1: 0.9399 - val_recall_1: 0.7494 - val_auc_1: 0.8825 - val_false_positives_1: 102.0000\n",
      "Epoch 17/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2588 - accuracy: 0.8948 - precision_1: 0.9752 - recall_1: 0.7789 - auc_1: 0.9407 - false_positives_1: 169.0000 - val_loss: 0.3796 - val_accuracy: 0.8699 - val_precision_1: 0.9403 - val_recall_1: 0.7485 - val_auc_1: 0.8784 - val_false_positives_1: 101.0000\n",
      "Epoch 18/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2578 - accuracy: 0.8966 - precision_1: 0.9767 - recall_1: 0.7820 - auc_1: 0.9420 - false_positives_1: 159.0000 - val_loss: 0.3558 - val_accuracy: 0.8687 - val_precision_1: 0.9212 - val_recall_1: 0.7635 - val_auc_1: 0.8811 - val_false_positives_1: 139.0000\n",
      "Epoch 19/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2520 - accuracy: 0.8971 - precision_1: 0.9751 - recall_1: 0.7845 - auc_1: 0.9446 - false_positives_1: 171.0000 - val_loss: 0.3689 - val_accuracy: 0.8666 - val_precision_1: 0.9174 - val_recall_1: 0.7621 - val_auc_1: 0.8806 - val_false_positives_1: 146.0000\n",
      "Epoch 20/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2469 - accuracy: 0.8975 - precision_1: 0.9729 - recall_1: 0.7873 - auc_1: 0.9475 - false_positives_1: 187.0000 - val_loss: 0.3785 - val_accuracy: 0.8660 - val_precision_1: 0.9324 - val_recall_1: 0.7461 - val_auc_1: 0.8755 - val_false_positives_1: 115.0000\n",
      "Epoch 21/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2478 - accuracy: 0.8978 - precision_1: 0.9711 - recall_1: 0.7895 - auc_1: 0.9472 - false_positives_1: 201.0000 - val_loss: 0.3841 - val_accuracy: 0.8668 - val_precision_1: 0.9246 - val_recall_1: 0.7555 - val_auc_1: 0.8803 - val_false_positives_1: 131.0000\n",
      "Epoch 22/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2390 - accuracy: 0.9013 - precision_1: 0.9696 - recall_1: 0.7992 - auc_1: 0.9521 - false_positives_1: 214.0000 - val_loss: 0.3801 - val_accuracy: 0.8621 - val_precision_1: 0.9121 - val_recall_1: 0.7560 - val_auc_1: 0.8769 - val_false_positives_1: 155.0000\n",
      "Epoch 23/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2340 - accuracy: 0.9023 - precision_1: 0.9709 - recall_1: 0.8003 - auc_1: 0.9543 - false_positives_1: 205.0000 - val_loss: 0.3998 - val_accuracy: 0.8625 - val_precision_1: 0.9099 - val_recall_1: 0.7593 - val_auc_1: 0.8751 - val_false_positives_1: 160.0000\n",
      "Epoch 24/50\n",
      "612/612 [==============================] - 3s 5ms/step - loss: 0.2307 - accuracy: 0.9056 - precision_1: 0.9704 - recall_1: 0.8086 - auc_1: 0.9556 - false_positives_1: 211.0000 - val_loss: 0.4146 - val_accuracy: 0.8609 - val_precision_1: 0.9081 - val_recall_1: 0.7569 - val_auc_1: 0.8777 - val_false_positives_1: 163.0000\n",
      "Epoch 25/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2291 - accuracy: 0.9041 - precision_1: 0.9654 - recall_1: 0.8094 - auc_1: 0.9576 - false_positives_1: 248.0000 - val_loss: 0.4347 - val_accuracy: 0.8482 - val_precision_1: 0.8654 - val_recall_1: 0.7710 - val_auc_1: 0.8785 - val_false_positives_1: 255.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2287 - accuracy: 0.9047 - precision_1: 0.9641 - recall_1: 0.8121 - auc_1: 0.9570 - false_positives_1: 258.0000 - val_loss: 0.4235 - val_accuracy: 0.8533 - val_precision_1: 0.8844 - val_recall_1: 0.7626 - val_auc_1: 0.8780 - val_false_positives_1: 212.0000\n",
      "Epoch 27/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2202 - accuracy: 0.9075 - precision_1: 0.9678 - recall_1: 0.8155 - auc_1: 0.9607 - false_positives_1: 232.0000 - val_loss: 0.4271 - val_accuracy: 0.8470 - val_precision_1: 0.8685 - val_recall_1: 0.7640 - val_auc_1: 0.8759 - val_false_positives_1: 246.0000\n",
      "Epoch 28/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2172 - accuracy: 0.9082 - precision_1: 0.9624 - recall_1: 0.8219 - auc_1: 0.9627 - false_positives_1: 274.0000 - val_loss: 0.4474 - val_accuracy: 0.8484 - val_precision_1: 0.8714 - val_recall_1: 0.7645 - val_auc_1: 0.8747 - val_false_positives_1: 240.0000\n",
      "Epoch 29/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2126 - accuracy: 0.9107 - precision_1: 0.9655 - recall_1: 0.8251 - auc_1: 0.9639 - false_positives_1: 252.0000 - val_loss: 0.4442 - val_accuracy: 0.8585 - val_precision_1: 0.8958 - val_recall_1: 0.7635 - val_auc_1: 0.8784 - val_false_positives_1: 189.0000\n",
      "Epoch 30/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2092 - accuracy: 0.9119 - precision_1: 0.9646 - recall_1: 0.8287 - auc_1: 0.9650 - false_positives_1: 260.0000 - val_loss: 0.4356 - val_accuracy: 0.8417 - val_precision_1: 0.8555 - val_recall_1: 0.7654 - val_auc_1: 0.8734 - val_false_positives_1: 275.0000\n",
      "Epoch 31/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.2087 - accuracy: 0.9123 - precision_1: 0.9622 - recall_1: 0.8319 - auc_1: 0.9658 - false_positives_1: 279.0000 - val_loss: 0.4630 - val_accuracy: 0.8490 - val_precision_1: 0.8764 - val_recall_1: 0.7602 - val_auc_1: 0.8741 - val_false_positives_1: 228.0000\n",
      "Epoch 32/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2020 - accuracy: 0.9149 - precision_1: 0.9607 - recall_1: 0.8396 - auc_1: 0.9680 - false_positives_1: 293.0000 - val_loss: 0.4681 - val_accuracy: 0.8499 - val_precision_1: 0.8800 - val_recall_1: 0.7583 - val_auc_1: 0.8750 - val_false_positives_1: 220.0000\n",
      "Epoch 33/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1995 - accuracy: 0.9180 - precision_1: 0.9641 - recall_1: 0.8437 - auc_1: 0.9688 - false_positives_1: 268.0000 - val_loss: 0.4755 - val_accuracy: 0.8503 - val_precision_1: 0.8818 - val_recall_1: 0.7574 - val_auc_1: 0.8717 - val_false_positives_1: 216.0000\n",
      "Epoch 34/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1942 - accuracy: 0.9198 - precision_1: 0.9623 - recall_1: 0.8497 - auc_1: 0.9710 - false_positives_1: 284.0000 - val_loss: 0.4933 - val_accuracy: 0.8464 - val_precision_1: 0.8751 - val_recall_1: 0.7546 - val_auc_1: 0.8678 - val_false_positives_1: 229.0000\n",
      "Epoch 35/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1932 - accuracy: 0.9213 - precision_1: 0.9637 - recall_1: 0.8520 - auc_1: 0.9711 - false_positives_1: 274.0000 - val_loss: 0.5055 - val_accuracy: 0.8462 - val_precision_1: 0.8682 - val_recall_1: 0.7621 - val_auc_1: 0.8697 - val_false_positives_1: 246.0000\n",
      "Epoch 36/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1954 - accuracy: 0.9204 - precision_1: 0.9580 - recall_1: 0.8553 - auc_1: 0.9715 - false_positives_1: 320.0000 - val_loss: 0.5093 - val_accuracy: 0.8501 - val_precision_1: 0.8800 - val_recall_1: 0.7588 - val_auc_1: 0.8726 - val_false_positives_1: 220.0000\n",
      "Epoch 37/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1841 - accuracy: 0.9237 - precision_1: 0.9616 - recall_1: 0.8597 - auc_1: 0.9742 - false_positives_1: 293.0000 - val_loss: 0.5186 - val_accuracy: 0.8462 - val_precision_1: 0.8747 - val_recall_1: 0.7546 - val_auc_1: 0.8670 - val_false_positives_1: 230.0000\n",
      "Epoch 38/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1866 - accuracy: 0.9247 - precision_1: 0.9632 - recall_1: 0.8604 - auc_1: 0.9734 - false_positives_1: 281.0000 - val_loss: 0.5338 - val_accuracy: 0.8507 - val_precision_1: 0.8745 - val_recall_1: 0.7668 - val_auc_1: 0.8710 - val_false_positives_1: 234.0000\n",
      "Epoch 39/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1815 - accuracy: 0.9245 - precision_1: 0.9609 - recall_1: 0.8623 - auc_1: 0.9752 - false_positives_1: 300.0000 - val_loss: 0.5198 - val_accuracy: 0.8313 - val_precision_1: 0.8278 - val_recall_1: 0.7729 - val_auc_1: 0.8682 - val_false_positives_1: 342.0000\n",
      "Epoch 40/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1776 - accuracy: 0.9259 - precision_1: 0.9608 - recall_1: 0.8657 - auc_1: 0.9764 - false_positives_1: 302.0000 - val_loss: 0.5366 - val_accuracy: 0.8409 - val_precision_1: 0.8548 - val_recall_1: 0.7640 - val_auc_1: 0.8713 - val_false_positives_1: 276.0000\n",
      "Epoch 41/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1740 - accuracy: 0.9287 - precision_1: 0.9598 - recall_1: 0.8732 - auc_1: 0.9774 - false_positives_1: 312.0000 - val_loss: 0.5406 - val_accuracy: 0.8274 - val_precision_1: 0.8238 - val_recall_1: 0.7673 - val_auc_1: 0.8684 - val_false_positives_1: 349.0000\n",
      "Epoch 42/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1719 - accuracy: 0.9288 - precision_1: 0.9600 - recall_1: 0.8734 - auc_1: 0.9779 - false_positives_1: 311.0000 - val_loss: 0.5530 - val_accuracy: 0.8186 - val_precision_1: 0.7981 - val_recall_1: 0.7804 - val_auc_1: 0.8690 - val_false_positives_1: 420.0000\n",
      "Epoch 43/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1690 - accuracy: 0.9310 - precision_1: 0.9582 - recall_1: 0.8803 - auc_1: 0.9788 - false_positives_1: 328.0000 - val_loss: 0.5686 - val_accuracy: 0.8425 - val_precision_1: 0.8588 - val_recall_1: 0.7635 - val_auc_1: 0.8695 - val_false_positives_1: 267.0000\n",
      "Epoch 44/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1705 - accuracy: 0.9309 - precision_1: 0.9591 - recall_1: 0.8792 - auc_1: 0.9794 - false_positives_1: 320.0000 - val_loss: 0.5711 - val_accuracy: 0.8223 - val_precision_1: 0.8053 - val_recall_1: 0.7800 - val_auc_1: 0.8701 - val_false_positives_1: 401.0000\n",
      "Epoch 45/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1699 - accuracy: 0.9312 - precision_1: 0.9599 - recall_1: 0.8793 - auc_1: 0.9789 - false_positives_1: 314.0000 - val_loss: 0.6277 - val_accuracy: 0.8325 - val_precision_1: 0.8417 - val_recall_1: 0.7574 - val_auc_1: 0.8648 - val_false_positives_1: 303.0000\n",
      "Epoch 46/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1645 - accuracy: 0.9336 - precision_1: 0.9592 - recall_1: 0.8856 - auc_1: 0.9802 - false_positives_1: 322.0000 - val_loss: 0.5937 - val_accuracy: 0.8351 - val_precision_1: 0.8399 - val_recall_1: 0.7673 - val_auc_1: 0.8667 - val_false_positives_1: 311.0000\n",
      "Epoch 47/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1574 - accuracy: 0.9352 - precision_1: 0.9615 - recall_1: 0.8870 - auc_1: 0.9815 - false_positives_1: 303.0000 - val_loss: 0.6020 - val_accuracy: 0.8319 - val_precision_1: 0.8351 - val_recall_1: 0.7645 - val_auc_1: 0.8667 - val_false_positives_1: 321.0000\n",
      "Epoch 48/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1625 - accuracy: 0.9352 - precision_1: 0.9591 - recall_1: 0.8895 - auc_1: 0.9806 - false_positives_1: 324.0000 - val_loss: 0.6366 - val_accuracy: 0.8323 - val_precision_1: 0.8289 - val_recall_1: 0.7743 - val_auc_1: 0.8638 - val_false_positives_1: 340.0000\n",
      "Epoch 49/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1604 - accuracy: 0.9355 - precision_1: 0.9598 - recall_1: 0.8896 - auc_1: 0.9813 - false_positives_1: 318.0000 - val_loss: 0.5917 - val_accuracy: 0.8237 - val_precision_1: 0.8090 - val_recall_1: 0.7786 - val_auc_1: 0.8700 - val_false_positives_1: 391.0000\n",
      "Epoch 50/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1517 - accuracy: 0.9390 - precision_1: 0.9617 - recall_1: 0.8961 - auc_1: 0.9833 - false_positives_1: 305.0000 - val_loss: 0.6287 - val_accuracy: 0.8229 - val_precision_1: 0.8080 - val_recall_1: 0.7776 - val_auc_1: 0.8683 - val_false_positives_1: 393.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ce5f9a7be0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(units=64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    keras.layers.Dense(units=32, activation='relu'),\n",
    "    keras.layers.Dense(units=16, activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC(), keras.metrics.FalsePositives()])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43239a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 0s 3ms/step - loss: 0.6287 - accuracy: 0.8229 - precision_1: 0.8080 - recall_1: 0.7776 - auc_1: 0.8683 - false_positives_1: 393.0000\n",
      "Test loss: 0.6286953687667847\n",
      "Test accuracy: 0.8228676915168762\n",
      "Test precision: 0.8080117106437683\n",
      "Test recall: 0.7776210904121399\n",
      "Test F1 score: 0.48601405987732865\n",
      "Test ROC AUC: 0.8682659268379211\n",
      "Test specificity: 0.8691308691308691\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy, precision, recall, auc, fp = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test precision: {precision}\")\n",
    "print(f\"Test recall: {recall}\")\n",
    "print(f\"Test F1 score: {2*((precision*recall)/(precision+recall+1))}\")\n",
    "print(f\"Test ROC AUC: {auc}\")\n",
    "print(f\"Test specificity: {1-fp/(fp+tn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40271995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "612/612 [==============================] - 6s 5ms/step - loss: 0.4067 - accuracy: 0.8311 - precision_3: 0.8810 - recall_3: 0.7091 - auc_3: 0.8665 - false_positives_3: 818.0000 - false_negatives_1: 2485.0000 - val_loss: 0.3433 - val_accuracy: 0.8691 - val_precision_3: 0.9673 - val_recall_3: 0.7236 - val_auc_3: 0.8836 - val_false_positives_3: 52.0000 - val_false_negatives_1: 588.0000\n",
      "Epoch 2/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.3376 - accuracy: 0.8686 - precision_3: 0.9571 - recall_3: 0.7319 - auc_3: 0.8916 - false_positives_3: 280.0000 - false_negatives_1: 2290.0000 - val_loss: 0.3219 - val_accuracy: 0.8779 - val_precision_3: 0.9705 - val_recall_3: 0.7419 - val_auc_3: 0.8900 - val_false_positives_3: 48.0000 - val_false_negatives_1: 549.0000\n",
      "Epoch 3/50\n",
      "612/612 [==============================] - 3s 6ms/step - loss: 0.3212 - accuracy: 0.8749 - precision_3: 0.9672 - recall_3: 0.7387 - auc_3: 0.8996 - false_positives_3: 214.0000 - false_negatives_1: 2232.0000 - val_loss: 0.3299 - val_accuracy: 0.8787 - val_precision_3: 0.9812 - val_recall_3: 0.7353 - val_auc_3: 0.8888 - val_false_positives_3: 30.0000 - val_false_negatives_1: 563.0000\n",
      "Epoch 4/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.3219 - accuracy: 0.8759 - precision_3: 0.9706 - recall_3: 0.7382 - auc_3: 0.9006 - false_positives_3: 191.0000 - false_negatives_1: 2236.0000 - val_loss: 0.3294 - val_accuracy: 0.8736 - val_precision_3: 0.9815 - val_recall_3: 0.7231 - val_auc_3: 0.8888 - val_false_positives_3: 29.0000 - val_false_negatives_1: 589.0000\n",
      "Epoch 5/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.3041 - accuracy: 0.8819 - precision_3: 0.9781 - recall_3: 0.7464 - auc_3: 0.9079 - false_positives_3: 143.0000 - false_negatives_1: 2166.0000 - val_loss: 0.3163 - val_accuracy: 0.8805 - val_precision_3: 0.9754 - val_recall_3: 0.7442 - val_auc_3: 0.8929 - val_false_positives_3: 40.0000 - val_false_negatives_1: 544.0000\n",
      "Epoch 6/50\n",
      "612/612 [==============================] - 3s 5ms/step - loss: 0.2985 - accuracy: 0.8852 - precision_3: 0.9802 - recall_3: 0.7525 - auc_3: 0.9114 - false_positives_3: 130.0000 - false_negatives_1: 2114.0000 - val_loss: 0.3434 - val_accuracy: 0.8679 - val_precision_3: 0.9273 - val_recall_3: 0.7555 - val_auc_3: 0.8871 - val_false_positives_3: 126.0000 - val_false_negatives_1: 520.0000\n",
      "Epoch 7/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2982 - accuracy: 0.8843 - precision_3: 0.9773 - recall_3: 0.7525 - auc_3: 0.9131 - false_positives_3: 149.0000 - false_negatives_1: 2114.0000 - val_loss: 0.3282 - val_accuracy: 0.8783 - val_precision_3: 0.9654 - val_recall_3: 0.7471 - val_auc_3: 0.8891 - val_false_positives_3: 57.0000 - val_false_negatives_1: 538.0000\n",
      "Epoch 8/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.2922 - accuracy: 0.8866 - precision_3: 0.9803 - recall_3: 0.7556 - auc_3: 0.9178 - false_positives_3: 130.0000 - false_negatives_1: 2087.0000 - val_loss: 0.3244 - val_accuracy: 0.8734 - val_precision_3: 0.9537 - val_recall_3: 0.7452 - val_auc_3: 0.8898 - val_false_positives_3: 77.0000 - val_false_negatives_1: 542.0000\n",
      "Epoch 9/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.2866 - accuracy: 0.8887 - precision_3: 0.9784 - recall_3: 0.7620 - auc_3: 0.9213 - false_positives_3: 144.0000 - false_negatives_1: 2033.0000 - val_loss: 0.3329 - val_accuracy: 0.8765 - val_precision_3: 0.9574 - val_recall_3: 0.7494 - val_auc_3: 0.8855 - val_false_positives_3: 71.0000 - val_false_negatives_1: 533.0000\n",
      "Epoch 10/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.2772 - accuracy: 0.8906 - precision_3: 0.9790 - recall_3: 0.7660 - auc_3: 0.9272 - false_positives_3: 140.0000 - false_negatives_1: 1999.0000 - val_loss: 0.3346 - val_accuracy: 0.8769 - val_precision_3: 0.9641 - val_recall_3: 0.7447 - val_auc_3: 0.8855 - val_false_positives_3: 59.0000 - val_false_negatives_1: 543.0000\n",
      "Epoch 11/50\n",
      "612/612 [==============================] - 3s 5ms/step - loss: 0.2717 - accuracy: 0.8937 - precision_3: 0.9778 - recall_3: 0.7743 - auc_3: 0.9308 - false_positives_3: 150.0000 - false_negatives_1: 1928.0000 - val_loss: 0.3374 - val_accuracy: 0.8750 - val_precision_3: 0.9605 - val_recall_3: 0.7433 - val_auc_3: 0.8843 - val_false_positives_3: 65.0000 - val_false_negatives_1: 546.0000\n",
      "Epoch 12/50\n",
      "612/612 [==============================] - 3s 5ms/step - loss: 0.2640 - accuracy: 0.8967 - precision_3: 0.9800 - recall_3: 0.7794 - auc_3: 0.9351 - false_positives_3: 136.0000 - false_negatives_1: 1884.0000 - val_loss: 0.3491 - val_accuracy: 0.8754 - val_precision_3: 0.9512 - val_recall_3: 0.7522 - val_auc_3: 0.8838 - val_false_positives_3: 82.0000 - val_false_negatives_1: 527.0000\n",
      "Epoch 13/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2567 - accuracy: 0.8979 - precision_3: 0.9754 - recall_3: 0.7861 - auc_3: 0.9406 - false_positives_3: 169.0000 - false_negatives_1: 1827.0000 - val_loss: 0.3617 - val_accuracy: 0.8699 - val_precision_3: 0.9224 - val_recall_3: 0.7654 - val_auc_3: 0.8819 - val_false_positives_3: 137.0000 - val_false_negatives_1: 499.0000\n",
      "Epoch 14/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2511 - accuracy: 0.9011 - precision_3: 0.9731 - recall_3: 0.7957 - auc_3: 0.9449 - false_positives_3: 188.0000 - false_negatives_1: 1745.0000 - val_loss: 0.3620 - val_accuracy: 0.8740 - val_precision_3: 0.9554 - val_recall_3: 0.7452 - val_auc_3: 0.8799 - val_false_positives_3: 74.0000 - val_false_negatives_1: 542.0000\n",
      "Epoch 15/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2421 - accuracy: 0.9031 - precision_3: 0.9707 - recall_3: 0.8025 - auc_3: 0.9498 - false_positives_3: 207.0000 - false_negatives_1: 1687.0000 - val_loss: 0.3679 - val_accuracy: 0.8693 - val_precision_3: 0.9346 - val_recall_3: 0.7522 - val_auc_3: 0.8785 - val_false_positives_3: 112.0000 - val_false_negatives_1: 527.0000\n",
      "Epoch 16/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.2302 - accuracy: 0.9048 - precision_3: 0.9688 - recall_3: 0.8080 - auc_3: 0.9555 - false_positives_3: 222.0000 - false_negatives_1: 1640.0000 - val_loss: 0.3843 - val_accuracy: 0.8668 - val_precision_3: 0.9160 - val_recall_3: 0.7640 - val_auc_3: 0.8819 - val_false_positives_3: 149.0000 - val_false_negatives_1: 502.0000\n",
      "Epoch 17/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2229 - accuracy: 0.9082 - precision_3: 0.9658 - recall_3: 0.8190 - auc_3: 0.9585 - false_positives_3: 248.0000 - false_negatives_1: 1546.0000 - val_loss: 0.4301 - val_accuracy: 0.8517 - val_precision_3: 0.8773 - val_recall_3: 0.7663 - val_auc_3: 0.8739 - val_false_positives_3: 228.0000 - val_false_negatives_1: 497.0000\n",
      "Epoch 18/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2212 - accuracy: 0.9101 - precision_3: 0.9635 - recall_3: 0.8255 - auc_3: 0.9598 - false_positives_3: 267.0000 - false_negatives_1: 1490.0000 - val_loss: 0.4218 - val_accuracy: 0.8452 - val_precision_3: 0.8624 - val_recall_3: 0.7663 - val_auc_3: 0.8746 - val_false_positives_3: 260.0000 - val_false_negatives_1: 497.0000\n",
      "Epoch 19/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2074 - accuracy: 0.9162 - precision_3: 0.9647 - recall_3: 0.8389 - auc_3: 0.9649 - false_positives_3: 262.0000 - false_negatives_1: 1376.0000 - val_loss: 0.4206 - val_accuracy: 0.8538 - val_precision_3: 0.8796 - val_recall_3: 0.7692 - val_auc_3: 0.8769 - val_false_positives_3: 224.0000 - val_false_negatives_1: 491.0000\n",
      "Epoch 20/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.2007 - accuracy: 0.9176 - precision_3: 0.9626 - recall_3: 0.8442 - auc_3: 0.9673 - false_positives_3: 280.0000 - false_negatives_1: 1331.0000 - val_loss: 0.4802 - val_accuracy: 0.8368 - val_precision_3: 0.8354 - val_recall_3: 0.7781 - val_auc_3: 0.8744 - val_false_positives_3: 326.0000 - val_false_negatives_1: 472.0000\n",
      "Epoch 21/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1955 - accuracy: 0.9213 - precision_3: 0.9647 - recall_3: 0.8511 - auc_3: 0.9696 - false_positives_3: 266.0000 - false_negatives_1: 1272.0000 - val_loss: 0.4688 - val_accuracy: 0.8445 - val_precision_3: 0.8584 - val_recall_3: 0.7696 - val_auc_3: 0.8756 - val_false_positives_3: 270.0000 - val_false_negatives_1: 490.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1795 - accuracy: 0.9289 - precision_3: 0.9695 - recall_3: 0.8643 - auc_3: 0.9742 - false_positives_3: 232.0000 - false_negatives_1: 1159.0000 - val_loss: 0.5051 - val_accuracy: 0.8515 - val_precision_3: 0.8801 - val_recall_3: 0.7626 - val_auc_3: 0.8750 - val_false_positives_3: 221.0000 - val_false_negatives_1: 505.0000\n",
      "Epoch 23/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.1828 - accuracy: 0.9248 - precision_3: 0.9596 - recall_3: 0.8642 - auc_3: 0.9741 - false_positives_3: 311.0000 - false_negatives_1: 1160.0000 - val_loss: 0.5021 - val_accuracy: 0.8427 - val_precision_3: 0.8540 - val_recall_3: 0.7701 - val_auc_3: 0.8724 - val_false_positives_3: 280.0000 - val_false_negatives_1: 489.0000\n",
      "Epoch 24/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1665 - accuracy: 0.9338 - precision_3: 0.9676 - recall_3: 0.8778 - auc_3: 0.9789 - false_positives_3: 251.0000 - false_negatives_1: 1044.0000 - val_loss: 0.5796 - val_accuracy: 0.8450 - val_precision_3: 0.8582 - val_recall_3: 0.7710 - val_auc_3: 0.8732 - val_false_positives_3: 271.0000 - val_false_negatives_1: 487.0000\n",
      "Epoch 25/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1618 - accuracy: 0.9368 - precision_3: 0.9683 - recall_3: 0.8842 - auc_3: 0.9797 - false_positives_3: 247.0000 - false_negatives_1: 989.0000 - val_loss: 0.5439 - val_accuracy: 0.8513 - val_precision_3: 0.8739 - val_recall_3: 0.7692 - val_auc_3: 0.8755 - val_false_positives_3: 236.0000 - val_false_negatives_1: 491.0000\n",
      "Epoch 26/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1559 - accuracy: 0.9381 - precision_3: 0.9692 - recall_3: 0.8865 - auc_3: 0.9813 - false_positives_3: 241.0000 - false_negatives_1: 969.0000 - val_loss: 0.5654 - val_accuracy: 0.8155 - val_precision_3: 0.7880 - val_recall_3: 0.7880 - val_auc_3: 0.8726 - val_false_positives_3: 451.0000 - val_false_negatives_1: 451.0000\n",
      "Epoch 27/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.1557 - accuracy: 0.9379 - precision_3: 0.9638 - recall_3: 0.8912 - auc_3: 0.9816 - false_positives_3: 286.0000 - false_negatives_1: 929.0000 - val_loss: 0.5664 - val_accuracy: 0.8382 - val_precision_3: 0.8401 - val_recall_3: 0.7757 - val_auc_3: 0.8708 - val_false_positives_3: 314.0000 - val_false_negatives_1: 477.0000\n",
      "Epoch 28/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.1438 - accuracy: 0.9420 - precision_3: 0.9697 - recall_3: 0.8951 - auc_3: 0.9843 - false_positives_3: 239.0000 - false_negatives_1: 896.0000 - val_loss: 0.6015 - val_accuracy: 0.8261 - val_precision_3: 0.8101 - val_recall_3: 0.7842 - val_auc_3: 0.8708 - val_false_positives_3: 391.0000 - val_false_negatives_1: 459.0000\n",
      "Epoch 29/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1361 - accuracy: 0.9474 - precision_3: 0.9707 - recall_3: 0.9069 - auc_3: 0.9864 - false_positives_3: 234.0000 - false_negatives_1: 795.0000 - val_loss: 0.6424 - val_accuracy: 0.8278 - val_precision_3: 0.8182 - val_recall_3: 0.7767 - val_auc_3: 0.8693 - val_false_positives_3: 367.0000 - val_false_negatives_1: 475.0000\n",
      "Epoch 30/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1299 - accuracy: 0.9471 - precision_3: 0.9689 - recall_3: 0.9081 - auc_3: 0.9875 - false_positives_3: 249.0000 - false_negatives_1: 785.0000 - val_loss: 0.6427 - val_accuracy: 0.8345 - val_precision_3: 0.8436 - val_recall_3: 0.7607 - val_auc_3: 0.8679 - val_false_positives_3: 300.0000 - val_false_negatives_1: 509.0000\n",
      "Epoch 31/50\n",
      "612/612 [==============================] - 3s 4ms/step - loss: 0.1247 - accuracy: 0.9517 - precision_3: 0.9720 - recall_3: 0.9157 - auc_3: 0.9885 - false_positives_3: 225.0000 - false_negatives_1: 720.0000 - val_loss: 0.6884 - val_accuracy: 0.8233 - val_precision_3: 0.8150 - val_recall_3: 0.7682 - val_auc_3: 0.8599 - val_false_positives_3: 371.0000 - val_false_negatives_1: 493.0000\n",
      "Epoch 32/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1258 - accuracy: 0.9495 - precision_3: 0.9698 - recall_3: 0.9129 - auc_3: 0.9886 - false_positives_3: 243.0000 - false_negatives_1: 744.0000 - val_loss: 0.7489 - val_accuracy: 0.8384 - val_precision_3: 0.8458 - val_recall_3: 0.7687 - val_auc_3: 0.8672 - val_false_positives_3: 298.0000 - val_false_negatives_1: 492.0000\n",
      "Epoch 33/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.1219 - accuracy: 0.9530 - precision_3: 0.9705 - recall_3: 0.9204 - auc_3: 0.9889 - false_positives_3: 239.0000 - false_negatives_1: 680.0000 - val_loss: 0.7194 - val_accuracy: 0.8450 - val_precision_3: 0.8601 - val_recall_3: 0.7687 - val_auc_3: 0.8695 - val_false_positives_3: 266.0000 - val_false_negatives_1: 492.0000\n",
      "Epoch 34/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1153 - accuracy: 0.9544 - precision_3: 0.9713 - recall_3: 0.9228 - auc_3: 0.9902 - false_positives_3: 233.0000 - false_negatives_1: 659.0000 - val_loss: 0.7199 - val_accuracy: 0.8415 - val_precision_3: 0.8492 - val_recall_3: 0.7729 - val_auc_3: 0.8692 - val_false_positives_3: 292.0000 - val_false_negatives_1: 483.0000\n",
      "Epoch 35/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1100 - accuracy: 0.9564 - precision_3: 0.9752 - recall_3: 0.9238 - auc_3: 0.9912 - false_positives_3: 201.0000 - false_negatives_1: 651.0000 - val_loss: 0.8111 - val_accuracy: 0.8223 - val_precision_3: 0.8050 - val_recall_3: 0.7804 - val_auc_3: 0.8646 - val_false_positives_3: 402.0000 - val_false_negatives_1: 467.0000\n",
      "Epoch 36/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1113 - accuracy: 0.9554 - precision_3: 0.9699 - recall_3: 0.9267 - auc_3: 0.9909 - false_positives_3: 246.0000 - false_negatives_1: 626.0000 - val_loss: 0.7871 - val_accuracy: 0.8165 - val_precision_3: 0.7912 - val_recall_3: 0.7856 - val_auc_3: 0.8652 - val_false_positives_3: 441.0000 - val_false_negatives_1: 456.0000\n",
      "Epoch 37/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1183 - accuracy: 0.9566 - precision_3: 0.9721 - recall_3: 0.9273 - auc_3: 0.9900 - false_positives_3: 227.0000 - false_negatives_1: 621.0000 - val_loss: 0.8281 - val_accuracy: 0.8171 - val_precision_3: 0.7926 - val_recall_3: 0.7851 - val_auc_3: 0.8631 - val_false_positives_3: 437.0000 - val_false_negatives_1: 457.0000\n",
      "Epoch 38/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1010 - accuracy: 0.9602 - precision_3: 0.9733 - recall_3: 0.9344 - auc_3: 0.9926 - false_positives_3: 219.0000 - false_negatives_1: 560.0000 - val_loss: 0.8260 - val_accuracy: 0.8139 - val_precision_3: 0.7853 - val_recall_3: 0.7875 - val_auc_3: 0.8634 - val_false_positives_3: 458.0000 - val_false_negatives_1: 452.0000\n",
      "Epoch 39/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.1105 - accuracy: 0.9561 - precision_3: 0.9690 - recall_3: 0.9292 - auc_3: 0.9913 - false_positives_3: 254.0000 - false_negatives_1: 605.0000 - val_loss: 0.7854 - val_accuracy: 0.8276 - val_precision_3: 0.8132 - val_recall_3: 0.7837 - val_auc_3: 0.8683 - val_false_positives_3: 383.0000 - val_false_negatives_1: 460.0000\n",
      "Epoch 40/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.0977 - accuracy: 0.9633 - precision_3: 0.9736 - recall_3: 0.9415 - auc_3: 0.9936 - false_positives_3: 218.0000 - false_negatives_1: 500.0000 - val_loss: 0.7929 - val_accuracy: 0.8218 - val_precision_3: 0.8034 - val_recall_3: 0.7819 - val_auc_3: 0.8667 - val_false_positives_3: 407.0000 - val_false_negatives_1: 464.0000\n",
      "Epoch 41/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.0898 - accuracy: 0.9661 - precision_3: 0.9765 - recall_3: 0.9452 - auc_3: 0.9942 - false_positives_3: 194.0000 - false_negatives_1: 468.0000 - val_loss: 0.8191 - val_accuracy: 0.8317 - val_precision_3: 0.8215 - val_recall_3: 0.7833 - val_auc_3: 0.8650 - val_false_positives_3: 362.0000 - val_false_negatives_1: 461.0000\n",
      "Epoch 42/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.0817 - accuracy: 0.9688 - precision_3: 0.9773 - recall_3: 0.9506 - auc_3: 0.9951 - false_positives_3: 189.0000 - false_negatives_1: 422.0000 - val_loss: 0.8370 - val_accuracy: 0.8343 - val_precision_3: 0.8334 - val_recall_3: 0.7739 - val_auc_3: 0.8674 - val_false_positives_3: 329.0000 - val_false_negatives_1: 481.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0891 - accuracy: 0.9653 - precision_3: 0.9745 - recall_3: 0.9453 - auc_3: 0.9942 - false_positives_3: 211.0000 - false_negatives_1: 467.0000 - val_loss: 0.8799 - val_accuracy: 0.8180 - val_precision_3: 0.7947 - val_recall_3: 0.7842 - val_auc_3: 0.8632 - val_false_positives_3: 431.0000 - val_false_negatives_1: 459.0000\n",
      "Epoch 44/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0952 - accuracy: 0.9645 - precision_3: 0.9739 - recall_3: 0.9439 - auc_3: 0.9930 - false_positives_3: 216.0000 - false_negatives_1: 479.0000 - val_loss: 0.8695 - val_accuracy: 0.8263 - val_precision_3: 0.8154 - val_recall_3: 0.7767 - val_auc_3: 0.8652 - val_false_positives_3: 374.0000 - val_false_negatives_1: 475.0000\n",
      "Epoch 45/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9686 - precision_3: 0.9775 - recall_3: 0.9500 - auc_3: 0.9953 - false_positives_3: 187.0000 - false_negatives_1: 427.0000 - val_loss: 0.8914 - val_accuracy: 0.8302 - val_precision_3: 0.8304 - val_recall_3: 0.7663 - val_auc_3: 0.8621 - val_false_positives_3: 333.0000 - val_false_negatives_1: 497.0000\n",
      "Epoch 46/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0878 - accuracy: 0.9669 - precision_3: 0.9753 - recall_3: 0.9481 - auc_3: 0.9941 - false_positives_3: 205.0000 - false_negatives_1: 443.0000 - val_loss: 0.9404 - val_accuracy: 0.8147 - val_precision_3: 0.7903 - val_recall_3: 0.7814 - val_auc_3: 0.8579 - val_false_positives_3: 441.0000 - val_false_negatives_1: 465.0000\n",
      "Epoch 47/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0717 - accuracy: 0.9718 - precision_3: 0.9784 - recall_3: 0.9566 - auc_3: 0.9963 - false_positives_3: 180.0000 - false_negatives_1: 371.0000 - val_loss: 0.9520 - val_accuracy: 0.8243 - val_precision_3: 0.8142 - val_recall_3: 0.7724 - val_auc_3: 0.8613 - val_false_positives_3: 375.0000 - val_false_negatives_1: 484.0000\n",
      "Epoch 48/50\n",
      "612/612 [==============================] - 2s 3ms/step - loss: 0.0895 - accuracy: 0.9674 - precision_3: 0.9741 - recall_3: 0.9507 - auc_3: 0.9942 - false_positives_3: 216.0000 - false_negatives_1: 421.0000 - val_loss: 0.9222 - val_accuracy: 0.7995 - val_precision_3: 0.7528 - val_recall_3: 0.8030 - val_auc_3: 0.8609 - val_false_positives_3: 561.0000 - val_false_negatives_1: 419.0000\n",
      "Epoch 49/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.0796 - accuracy: 0.9700 - precision_3: 0.9770 - recall_3: 0.9538 - auc_3: 0.9956 - false_positives_3: 192.0000 - false_negatives_1: 395.0000 - val_loss: 1.0118 - val_accuracy: 0.8208 - val_precision_3: 0.8029 - val_recall_3: 0.7795 - val_auc_3: 0.8582 - val_false_positives_3: 407.0000 - val_false_negatives_1: 469.0000\n",
      "Epoch 50/50\n",
      "612/612 [==============================] - 2s 4ms/step - loss: 0.0769 - accuracy: 0.9705 - precision_3: 0.9759 - recall_3: 0.9561 - auc_3: 0.9956 - false_positives_3: 202.0000 - false_negatives_1: 375.0000 - val_loss: 0.9558 - val_accuracy: 0.8227 - val_precision_3: 0.8049 - val_recall_3: 0.7819 - val_auc_3: 0.8620 - val_false_positives_3: 403.0000 - val_false_negatives_1: 464.0000\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, optimizers, metrics\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', metrics.Precision(), metrics.Recall(), metrics.AUC(), metrics.FalsePositives(), metrics.FalseNegatives()])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3aef786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8226631212926979\n",
      "Precision: 0.8049370764762827\n",
      "Recall: 0.7818523742360132\n",
      "F1 Score: 0.7932268065823992\n",
      "ROC AUC: 0.8737550584924629\n",
      "Specificity: 0.8540912382331644\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model on test set\n",
    "accuracy = accuracy_score(y_test, y_pred.round())\n",
    "precision = precision_score(y_test, y_pred.round())\n",
    "recall = recall_score(y_test, y_pred.round())\n",
    "f1 = f1_score(y_test, y_pred.round())\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred.round()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f79e328a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "612/612 [==============================] - 9s 9ms/step - loss: 0.4161 - accuracy: 0.8280 - precision_4: 0.8280 - recall_4: 0.8280 - auc_4: 0.8879 - specificity_at_sensitivity: 0.9454 - val_loss: 0.3425 - val_accuracy: 0.8677 - val_precision_4: 0.8677 - val_recall_4: 0.8677 - val_auc_4: 0.9185 - val_specificity_at_sensitivity: 0.9609\n",
      "Epoch 2/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.3458 - accuracy: 0.8642 - precision_4: 0.8642 - recall_4: 0.8642 - auc_4: 0.9192 - specificity_at_sensitivity: 0.9644 - val_loss: 0.3351 - val_accuracy: 0.8730 - val_precision_4: 0.8730 - val_recall_4: 0.8730 - val_auc_4: 0.9216 - val_specificity_at_sensitivity: 0.9611\n",
      "Epoch 3/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.3264 - accuracy: 0.8738 - precision_4: 0.8738 - recall_4: 0.8738 - auc_4: 0.9273 - specificity_at_sensitivity: 0.9685 - val_loss: 0.3213 - val_accuracy: 0.8791 - val_precision_4: 0.8791 - val_recall_4: 0.8791 - val_auc_4: 0.9267 - val_specificity_at_sensitivity: 0.9669\n",
      "Epoch 4/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.3169 - accuracy: 0.8779 - precision_4: 0.8779 - recall_4: 0.8779 - auc_4: 0.9319 - specificity_at_sensitivity: 0.9747 - val_loss: 0.3224 - val_accuracy: 0.8763 - val_precision_4: 0.8763 - val_recall_4: 0.8763 - val_auc_4: 0.9270 - val_specificity_at_sensitivity: 0.9667\n",
      "Epoch 5/50\n",
      "612/612 [==============================] - 5s 7ms/step - loss: 0.3093 - accuracy: 0.8801 - precision_4: 0.8801 - recall_4: 0.8801 - auc_4: 0.9357 - specificity_at_sensitivity: 0.9761 - val_loss: 0.3177 - val_accuracy: 0.8801 - val_precision_4: 0.8801 - val_recall_4: 0.8801 - val_auc_4: 0.9277 - val_specificity_at_sensitivity: 0.9663\n",
      "Epoch 6/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.3026 - accuracy: 0.8824 - precision_4: 0.8824 - recall_4: 0.8824 - auc_4: 0.9391 - specificity_at_sensitivity: 0.9784 - val_loss: 0.3211 - val_accuracy: 0.8777 - val_precision_4: 0.8777 - val_recall_4: 0.8777 - val_auc_4: 0.9271 - val_specificity_at_sensitivity: 0.9679\n",
      "Epoch 7/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.2960 - accuracy: 0.8837 - precision_4: 0.8837 - recall_4: 0.8837 - auc_4: 0.9425 - specificity_at_sensitivity: 0.9811 - val_loss: 0.3262 - val_accuracy: 0.8793 - val_precision_4: 0.8793 - val_recall_4: 0.8793 - val_auc_4: 0.9276 - val_specificity_at_sensitivity: 0.9697\n",
      "Epoch 8/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.2900 - accuracy: 0.8852 - precision_4: 0.8852 - recall_4: 0.8852 - auc_4: 0.9454 - specificity_at_sensitivity: 0.9837 - val_loss: 0.3224 - val_accuracy: 0.8795 - val_precision_4: 0.8795 - val_recall_4: 0.8795 - val_auc_4: 0.9278 - val_specificity_at_sensitivity: 0.9685\n",
      "Epoch 9/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.2800 - accuracy: 0.8881 - precision_4: 0.8881 - recall_4: 0.8881 - auc_4: 0.9497 - specificity_at_sensitivity: 0.9877 - val_loss: 0.3382 - val_accuracy: 0.8777 - val_precision_4: 0.8777 - val_recall_4: 0.8777 - val_auc_4: 0.9269 - val_specificity_at_sensitivity: 0.9687\n",
      "Epoch 10/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.2717 - accuracy: 0.8909 - precision_4: 0.8909 - recall_4: 0.8909 - auc_4: 0.9530 - specificity_at_sensitivity: 0.9901 - val_loss: 0.3350 - val_accuracy: 0.8760 - val_precision_4: 0.8760 - val_recall_4: 0.8760 - val_auc_4: 0.9261 - val_specificity_at_sensitivity: 0.9691\n",
      "Epoch 11/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.2601 - accuracy: 0.8961 - precision_4: 0.8961 - recall_4: 0.8961 - auc_4: 0.9573 - specificity_at_sensitivity: 0.9917 - val_loss: 0.3791 - val_accuracy: 0.8724 - val_precision_4: 0.8724 - val_recall_4: 0.8724 - val_auc_4: 0.9236 - val_specificity_at_sensitivity: 0.9681\n",
      "Epoch 12/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.2501 - accuracy: 0.8997 - precision_4: 0.8997 - recall_4: 0.8997 - auc_4: 0.9615 - specificity_at_sensitivity: 0.9940 - val_loss: 0.3791 - val_accuracy: 0.8490 - val_precision_4: 0.8490 - val_recall_4: 0.8490 - val_auc_4: 0.9177 - val_specificity_at_sensitivity: 0.9654\n",
      "Epoch 13/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.2400 - accuracy: 0.9012 - precision_4: 0.9012 - recall_4: 0.9012 - auc_4: 0.9642 - specificity_at_sensitivity: 0.9958 - val_loss: 0.3771 - val_accuracy: 0.8707 - val_precision_4: 0.8707 - val_recall_4: 0.8707 - val_auc_4: 0.9230 - val_specificity_at_sensitivity: 0.9656\n",
      "Epoch 14/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.2243 - accuracy: 0.9086 - precision_4: 0.9086 - recall_4: 0.9086 - auc_4: 0.9687 - specificity_at_sensitivity: 0.9965 - val_loss: 0.3989 - val_accuracy: 0.8562 - val_precision_4: 0.8562 - val_recall_4: 0.8562 - val_auc_4: 0.9152 - val_specificity_at_sensitivity: 0.9636\n",
      "Epoch 15/50\n",
      "612/612 [==============================] - 5s 7ms/step - loss: 0.2118 - accuracy: 0.9147 - precision_4: 0.9147 - recall_4: 0.9147 - auc_4: 0.9723 - specificity_at_sensitivity: 0.9978 - val_loss: 0.4660 - val_accuracy: 0.8495 - val_precision_4: 0.8495 - val_recall_4: 0.8495 - val_auc_4: 0.9077 - val_specificity_at_sensitivity: 0.9570\n",
      "Epoch 16/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.1992 - accuracy: 0.9182 - precision_4: 0.9182 - recall_4: 0.9182 - auc_4: 0.9755 - specificity_at_sensitivity: 0.9985 - val_loss: 0.4331 - val_accuracy: 0.8501 - val_precision_4: 0.8501 - val_recall_4: 0.8501 - val_auc_4: 0.9105 - val_specificity_at_sensitivity: 0.9622\n",
      "Epoch 17/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.1852 - accuracy: 0.9266 - precision_4: 0.9266 - recall_4: 0.9266 - auc_4: 0.9788 - specificity_at_sensitivity: 0.9985 - val_loss: 0.4700 - val_accuracy: 0.8425 - val_precision_4: 0.8425 - val_recall_4: 0.8425 - val_auc_4: 0.9092 - val_specificity_at_sensitivity: 0.9618\n",
      "Epoch 18/50\n",
      "612/612 [==============================] - 4s 6ms/step - loss: 0.1764 - accuracy: 0.9294 - precision_4: 0.9294 - recall_4: 0.9294 - auc_4: 0.9808 - specificity_at_sensitivity: 0.9986 - val_loss: 0.4681 - val_accuracy: 0.8595 - val_precision_4: 0.8595 - val_recall_4: 0.8595 - val_auc_4: 0.9121 - val_specificity_at_sensitivity: 0.9644\n",
      "Epoch 19/50\n",
      "612/612 [==============================] - 4s 6ms/step - loss: 0.1641 - accuracy: 0.9351 - precision_4: 0.9351 - recall_4: 0.9351 - auc_4: 0.9834 - specificity_at_sensitivity: 0.9990 - val_loss: 0.4932 - val_accuracy: 0.8460 - val_precision_4: 0.8460 - val_recall_4: 0.8460 - val_auc_4: 0.9072 - val_specificity_at_sensitivity: 0.9593\n",
      "Epoch 20/50\n",
      "612/612 [==============================] - 4s 6ms/step - loss: 0.1499 - accuracy: 0.9392 - precision_4: 0.9392 - recall_4: 0.9392 - auc_4: 0.9861 - specificity_at_sensitivity: 0.9995 - val_loss: 0.5346 - val_accuracy: 0.8480 - val_precision_4: 0.8480 - val_recall_4: 0.8480 - val_auc_4: 0.9072 - val_specificity_at_sensitivity: 0.9585\n",
      "Epoch 21/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.1447 - accuracy: 0.9416 - precision_4: 0.9416 - recall_4: 0.9416 - auc_4: 0.9871 - specificity_at_sensitivity: 0.9992 - val_loss: 0.5194 - val_accuracy: 0.8415 - val_precision_4: 0.8415 - val_recall_4: 0.8415 - val_auc_4: 0.9065 - val_specificity_at_sensitivity: 0.9620\n",
      "Epoch 22/50\n",
      "612/612 [==============================] - 4s 6ms/step - loss: 0.1349 - accuracy: 0.9461 - precision_4: 0.9461 - recall_4: 0.9461 - auc_4: 0.9885 - specificity_at_sensitivity: 0.9994 - val_loss: 0.6074 - val_accuracy: 0.8382 - val_precision_4: 0.8382 - val_recall_4: 0.8382 - val_auc_4: 0.8999 - val_specificity_at_sensitivity: 0.9628\n",
      "Epoch 23/50\n",
      "612/612 [==============================] - 4s 6ms/step - loss: 0.1246 - accuracy: 0.9522 - precision_4: 0.9522 - recall_4: 0.9522 - auc_4: 0.9904 - specificity_at_sensitivity: 0.9994 - val_loss: 0.6816 - val_accuracy: 0.8585 - val_precision_4: 0.8585 - val_recall_4: 0.8585 - val_auc_4: 0.9037 - val_specificity_at_sensitivity: 0.9554\n",
      "Epoch 24/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.1158 - accuracy: 0.9552 - precision_4: 0.9552 - recall_4: 0.9552 - auc_4: 0.9916 - specificity_at_sensitivity: 0.9994 - val_loss: 0.7302 - val_accuracy: 0.8415 - val_precision_4: 0.8415 - val_recall_4: 0.8415 - val_auc_4: 0.8975 - val_specificity_at_sensitivity: 0.9517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.1354 - accuracy: 0.9478 - precision_4: 0.9478 - recall_4: 0.9478 - auc_4: 0.9885 - specificity_at_sensitivity: 0.9990 - val_loss: 0.6432 - val_accuracy: 0.8286 - val_precision_4: 0.8286 - val_recall_4: 0.8286 - val_auc_4: 0.8926 - val_specificity_at_sensitivity: 0.9564\n",
      "Epoch 26/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.0994 - accuracy: 0.9622 - precision_4: 0.9622 - recall_4: 0.9622 - auc_4: 0.9937 - specificity_at_sensitivity: 0.9995 - val_loss: 0.7756 - val_accuracy: 0.8390 - val_precision_4: 0.8390 - val_recall_4: 0.8390 - val_auc_4: 0.8947 - val_specificity_at_sensitivity: 0.9489\n",
      "Epoch 27/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.0974 - accuracy: 0.9609 - precision_4: 0.9609 - recall_4: 0.9609 - auc_4: 0.9939 - specificity_at_sensitivity: 0.9995 - val_loss: 0.7008 - val_accuracy: 0.8458 - val_precision_4: 0.8458 - val_recall_4: 0.8458 - val_auc_4: 0.9005 - val_specificity_at_sensitivity: 0.9552\n",
      "Epoch 28/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.0935 - accuracy: 0.9643 - precision_4: 0.9643 - recall_4: 0.9643 - auc_4: 0.9943 - specificity_at_sensitivity: 0.9994 - val_loss: 0.7478 - val_accuracy: 0.8304 - val_precision_4: 0.8304 - val_recall_4: 0.8304 - val_auc_4: 0.8884 - val_specificity_at_sensitivity: 0.9538\n",
      "Epoch 29/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.0845 - accuracy: 0.9674 - precision_4: 0.9674 - recall_4: 0.9674 - auc_4: 0.9952 - specificity_at_sensitivity: 0.9994 - val_loss: 0.7742 - val_accuracy: 0.8349 - val_precision_4: 0.8349 - val_recall_4: 0.8349 - val_auc_4: 0.8963 - val_specificity_at_sensitivity: 0.9519\n",
      "Epoch 30/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.0875 - accuracy: 0.9676 - precision_4: 0.9676 - recall_4: 0.9676 - auc_4: 0.9955 - specificity_at_sensitivity: 0.9996 - val_loss: 0.7417 - val_accuracy: 0.8351 - val_precision_4: 0.8351 - val_recall_4: 0.8351 - val_auc_4: 0.8920 - val_specificity_at_sensitivity: 0.9483\n",
      "Epoch 31/50\n",
      "612/612 [==============================] - 5s 7ms/step - loss: 0.0800 - accuracy: 0.9693 - precision_4: 0.9693 - recall_4: 0.9693 - auc_4: 0.9957 - specificity_at_sensitivity: 0.9993 - val_loss: 0.8280 - val_accuracy: 0.8294 - val_precision_4: 0.8294 - val_recall_4: 0.8294 - val_auc_4: 0.8878 - val_specificity_at_sensitivity: 0.9480\n",
      "Epoch 32/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.0699 - accuracy: 0.9728 - precision_4: 0.9728 - recall_4: 0.9728 - auc_4: 0.9966 - specificity_at_sensitivity: 0.9995 - val_loss: 0.8232 - val_accuracy: 0.8364 - val_precision_4: 0.8364 - val_recall_4: 0.8364 - val_auc_4: 0.8880 - val_specificity_at_sensitivity: 0.9417\n",
      "Epoch 33/50\n",
      "612/612 [==============================] - 4s 7ms/step - loss: 0.0822 - accuracy: 0.9691 - precision_4: 0.9691 - recall_4: 0.9691 - auc_4: 0.9954 - specificity_at_sensitivity: 0.9992 - val_loss: 0.8398 - val_accuracy: 0.8237 - val_precision_4: 0.8237 - val_recall_4: 0.8237 - val_auc_4: 0.8843 - val_specificity_at_sensitivity: 0.9435\n",
      "Epoch 34/50\n",
      "612/612 [==============================] - 5s 7ms/step - loss: 0.0663 - accuracy: 0.9762 - precision_4: 0.9762 - recall_4: 0.9762 - auc_4: 0.9967 - specificity_at_sensitivity: 0.9992 - val_loss: 0.8971 - val_accuracy: 0.8376 - val_precision_4: 0.8376 - val_recall_4: 0.8376 - val_auc_4: 0.8914 - val_specificity_at_sensitivity: 0.9388\n",
      "Epoch 35/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.0656 - accuracy: 0.9740 - precision_4: 0.9740 - recall_4: 0.9740 - auc_4: 0.9970 - specificity_at_sensitivity: 0.9995 - val_loss: 0.8333 - val_accuracy: 0.8384 - val_precision_4: 0.8384 - val_recall_4: 0.8384 - val_auc_4: 0.8919 - val_specificity_at_sensitivity: 0.9442\n",
      "Epoch 36/50\n",
      "612/612 [==============================] - 5s 7ms/step - loss: 0.0765 - accuracy: 0.9736 - precision_4: 0.9736 - recall_4: 0.9736 - auc_4: 0.9960 - specificity_at_sensitivity: 0.9992 - val_loss: 0.7753 - val_accuracy: 0.8407 - val_precision_4: 0.8407 - val_recall_4: 0.8407 - val_auc_4: 0.8955 - val_specificity_at_sensitivity: 0.9493\n",
      "Epoch 37/50\n",
      "612/612 [==============================] - 5s 7ms/step - loss: 0.0557 - accuracy: 0.9782 - precision_4: 0.9782 - recall_4: 0.9782 - auc_4: 0.9977 - specificity_at_sensitivity: 0.9995 - val_loss: 1.0315 - val_accuracy: 0.8454 - val_precision_4: 0.8454 - val_recall_4: 0.8454 - val_auc_4: 0.8860 - val_specificity_at_sensitivity: 0.9243\n",
      "Epoch 38/50\n",
      "612/612 [==============================] - 5s 7ms/step - loss: 0.0632 - accuracy: 0.9773 - precision_4: 0.9773 - recall_4: 0.9773 - auc_4: 0.9972 - specificity_at_sensitivity: 0.9993 - val_loss: 0.8138 - val_accuracy: 0.8396 - val_precision_4: 0.8396 - val_recall_4: 0.8396 - val_auc_4: 0.8925 - val_specificity_at_sensitivity: 0.9413\n",
      "Epoch 39/50\n",
      "612/612 [==============================] - 5s 7ms/step - loss: 0.0588 - accuracy: 0.9789 - precision_4: 0.9789 - recall_4: 0.9789 - auc_4: 0.9974 - specificity_at_sensitivity: 0.9994 - val_loss: 0.9402 - val_accuracy: 0.8317 - val_precision_4: 0.8317 - val_recall_4: 0.8317 - val_auc_4: 0.8844 - val_specificity_at_sensitivity: 0.9309\n",
      "Epoch 40/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.0560 - accuracy: 0.9795 - precision_4: 0.9795 - recall_4: 0.9795 - auc_4: 0.9975 - specificity_at_sensitivity: 0.9993 - val_loss: 0.9342 - val_accuracy: 0.8358 - val_precision_4: 0.8358 - val_recall_4: 0.8358 - val_auc_4: 0.8869 - val_specificity_at_sensitivity: 0.9337\n",
      "Epoch 41/50\n",
      "612/612 [==============================] - 5s 9ms/step - loss: 0.0580 - accuracy: 0.9793 - precision_4: 0.9793 - recall_4: 0.9793 - auc_4: 0.9976 - specificity_at_sensitivity: 0.9995 - val_loss: 0.8325 - val_accuracy: 0.8319 - val_precision_4: 0.8319 - val_recall_4: 0.8319 - val_auc_4: 0.8881 - val_specificity_at_sensitivity: 0.9450\n",
      "Epoch 42/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.0536 - accuracy: 0.9807 - precision_4: 0.9807 - recall_4: 0.9807 - auc_4: 0.9979 - specificity_at_sensitivity: 0.9996 - val_loss: 0.9931 - val_accuracy: 0.8245 - val_precision_4: 0.8245 - val_recall_4: 0.8245 - val_auc_4: 0.8820 - val_specificity_at_sensitivity: 0.9313\n",
      "Epoch 43/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.0495 - accuracy: 0.9818 - precision_4: 0.9818 - recall_4: 0.9818 - auc_4: 0.9980 - specificity_at_sensitivity: 0.9993 - val_loss: 0.8461 - val_accuracy: 0.8255 - val_precision_4: 0.8255 - val_recall_4: 0.8255 - val_auc_4: 0.8841 - val_specificity_at_sensitivity: 0.9433\n",
      "Epoch 44/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.0469 - accuracy: 0.9846 - precision_4: 0.9846 - recall_4: 0.9846 - auc_4: 0.9982 - specificity_at_sensitivity: 0.9994 - val_loss: 0.9945 - val_accuracy: 0.8190 - val_precision_4: 0.8190 - val_recall_4: 0.8190 - val_auc_4: 0.8776 - val_specificity_at_sensitivity: 0.9309\n",
      "Epoch 45/50\n",
      "612/612 [==============================] - 5s 9ms/step - loss: 0.0496 - accuracy: 0.9825 - precision_4: 0.9825 - recall_4: 0.9825 - auc_4: 0.9979 - specificity_at_sensitivity: 0.9992 - val_loss: 0.9058 - val_accuracy: 0.8290 - val_precision_4: 0.8290 - val_recall_4: 0.8290 - val_auc_4: 0.8824 - val_specificity_at_sensitivity: 0.9358\n",
      "Epoch 46/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.0475 - accuracy: 0.9824 - precision_4: 0.9824 - recall_4: 0.9824 - auc_4: 0.9984 - specificity_at_sensitivity: 0.9996 - val_loss: 0.9640 - val_accuracy: 0.8339 - val_precision_4: 0.8339 - val_recall_4: 0.8339 - val_auc_4: 0.8858 - val_specificity_at_sensitivity: 0.9315\n",
      "Epoch 47/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.0415 - accuracy: 0.9854 - precision_4: 0.9854 - recall_4: 0.9854 - auc_4: 0.9985 - specificity_at_sensitivity: 0.9995 - val_loss: 0.8872 - val_accuracy: 0.8313 - val_precision_4: 0.8313 - val_recall_4: 0.8313 - val_auc_4: 0.8850 - val_specificity_at_sensitivity: 0.9380\n",
      "Epoch 48/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.0436 - accuracy: 0.9840 - precision_4: 0.9840 - recall_4: 0.9840 - auc_4: 0.9987 - specificity_at_sensitivity: 0.9997 - val_loss: 1.1296 - val_accuracy: 0.8364 - val_precision_4: 0.8364 - val_recall_4: 0.8364 - val_auc_4: 0.8822 - val_specificity_at_sensitivity: 0.9192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "612/612 [==============================] - 5s 7ms/step - loss: 0.0506 - accuracy: 0.9834 - precision_4: 0.9834 - recall_4: 0.9834 - auc_4: 0.9978 - specificity_at_sensitivity: 0.9992 - val_loss: 0.9689 - val_accuracy: 0.8355 - val_precision_4: 0.8355 - val_recall_4: 0.8355 - val_auc_4: 0.8838 - val_specificity_at_sensitivity: 0.9270\n",
      "Epoch 50/50\n",
      "612/612 [==============================] - 5s 8ms/step - loss: 0.0469 - accuracy: 0.9821 - precision_4: 0.9821 - recall_4: 0.9821 - auc_4: 0.9982 - specificity_at_sensitivity: 0.9994 - val_loss: 1.1455 - val_accuracy: 0.8499 - val_precision_4: 0.8499 - val_recall_4: 0.8499 - val_auc_4: 0.8887 - val_specificity_at_sensitivity: 0.9180\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC(), keras.metrics.SpecificityAtSensitivity(0.5)])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bc24714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.849867048476171\n",
      "Precision: 0.851862332006429\n",
      "Recall: 0.849867048476171\n",
      "F1 Score: 0.8483547550611351\n",
      "ROC AUC: 0.8402650893464157\n",
      "Specificity: 0.9141926140477915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred,average='weighted')\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Convert y_test and y_pred to one-hot encoded format\n",
    "n_classes = len(np.unique(y_train))\n",
    "y_test_oh = label_binarize(y_test, classes=np.arange(n_classes))\n",
    "y_pred_oh = label_binarize(y_pred, classes=np.arange(n_classes))\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test_oh, y_pred_oh)\n",
    "precision = precision_score(y_test_oh, y_pred_oh, average='weighted')\n",
    "recall = recall_score(y_test_oh, y_pred_oh, average='weighted')\n",
    "f1 = f1_score(y_test_oh, y_pred_oh, average='weighted')\n",
    "roc_auc = roc_auc_score(y_test_oh, y_pred_oh, average='weighted')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_oh.argmax(axis=1), y_pred_oh.argmax(axis=1)).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"Specificity: {specificity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a6c1ca4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_14\" is incompatible with the layer: expected shape=(None, 28, 28, 1), found shape=(None, 79, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_53468\\2763463992.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# Train the DNN model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# Make predictions on the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_14\" is incompatible with the layer: expected shape=(None, 28, 28, 1), found shape=(None, 79, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Define the DNN model\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the K-fold cross-validator\n",
    "k = 5\n",
    "fold_accuracy = []\n",
    "fold_precision = []\n",
    "fold_recall = []\n",
    "fold_f1 = []\n",
    "fold_roc_auc = []\n",
    "fold_specificity = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform K-fold cross-validation\n",
    "for train_index, test_index in skf.split(x_train, y_train):\n",
    "    x_train_fold, x_test_fold = x_train[train_index], x_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # Create the DNN model\n",
    "    model = create_model()\n",
    "\n",
    "    # Train the DNN model\n",
    "    model.fit(x_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = np.argmax(model.predict(x_test_fold), axis=-1)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test_fold, y_pred)\n",
    "    precision = precision_score(y_test_fold, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_fold, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_fold, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_test_fold, y_pred, average='weighted')\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_fold, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Append the performance metrics for the current fold\n",
    "    fold_accuracy.append(accuracy)\n",
    "    fold_precision.append(precision)\n",
    "    fold_recall.append(recall)\n",
    "    fold_f1.append(f1)\n",
    "    fold_roc_auc.append(roc_auc)\n",
    "    fold_specificity.append(specificity)\n",
    "\n",
    "# Print the mean performance metrics across all folds\n",
    "print(\"Mean Accuracy:\", np.mean(fold_accuracy))\n",
    "print(\"Mean Precision:\", np.mean(fold_precision))\n",
    "print(\"Mean Recall:\", np.mean(fold_recall))\n",
    "print(\"Mean F1 Score:\", np.mean(fold_f1))\n",
    "print(\"Mean ROC AUC Score:\", np.mean(fold_roc_auc))\n",
    "print(\"Mean Specificity:\", np.mean(fold_specificity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edee36eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Epoch 1/30\n",
      "489/489 [==============================] - 3s 2ms/step - loss: 0.4875 - accuracy: 0.7844\n",
      "Epoch 2/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3791 - accuracy: 0.8507\n",
      "Epoch 3/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3499 - accuracy: 0.8642\n",
      "Epoch 4/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3345 - accuracy: 0.8695\n",
      "Epoch 5/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3248 - accuracy: 0.8727\n",
      "Epoch 6/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3175 - accuracy: 0.8757\n",
      "Epoch 7/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3129 - accuracy: 0.8770\n",
      "Epoch 8/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3076 - accuracy: 0.8794\n",
      "Epoch 9/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3021 - accuracy: 0.8808\n",
      "Epoch 10/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2983 - accuracy: 0.8828\n",
      "Epoch 11/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2948 - accuracy: 0.8845\n",
      "Epoch 12/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2909 - accuracy: 0.8868\n",
      "Epoch 13/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2894 - accuracy: 0.8860\n",
      "Epoch 14/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2863 - accuracy: 0.8875\n",
      "Epoch 15/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2844 - accuracy: 0.8890\n",
      "Epoch 16/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2831 - accuracy: 0.8898\n",
      "Epoch 17/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2804 - accuracy: 0.8903\n",
      "Epoch 18/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2773 - accuracy: 0.8904\n",
      "Epoch 19/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2770 - accuracy: 0.8916\n",
      "Epoch 20/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2735 - accuracy: 0.8927\n",
      "Epoch 21/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2717 - accuracy: 0.8938\n",
      "Epoch 22/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2707 - accuracy: 0.8945\n",
      "Epoch 23/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2692 - accuracy: 0.8949\n",
      "Epoch 24/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2689 - accuracy: 0.8942\n",
      "Epoch 25/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2648 - accuracy: 0.8962\n",
      "Epoch 26/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2644 - accuracy: 0.8975\n",
      "Epoch 27/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2624 - accuracy: 0.8963\n",
      "Epoch 28/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2591 - accuracy: 0.8994\n",
      "Epoch 29/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2589 - accuracy: 0.8984\n",
      "Epoch 30/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2553 - accuracy: 0.8998\n",
      "123/123 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8609051393505497\n",
      "Precision: 0.8708233904332204\n",
      "Recall: 0.8609051393505497\n",
      "F1-Score: 0.8581764445251755\n",
      "ROC-AUC Score: 0.848392448709692\n",
      "Specificity: 0.9569005043558001\n",
      "Fold: 2\n",
      "Epoch 1/30\n",
      "489/489 [==============================] - 3s 3ms/step - loss: 0.4849 - accuracy: 0.7874\n",
      "Epoch 2/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3927 - accuracy: 0.8423\n",
      "Epoch 3/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3616 - accuracy: 0.8570\n",
      "Epoch 4/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3449 - accuracy: 0.8636\n",
      "Epoch 5/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3331 - accuracy: 0.8678\n",
      "Epoch 6/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3248 - accuracy: 0.8717\n",
      "Epoch 7/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3173 - accuracy: 0.8746\n",
      "Epoch 8/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3118 - accuracy: 0.8764\n",
      "Epoch 9/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3080 - accuracy: 0.8792\n",
      "Epoch 10/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3043 - accuracy: 0.8814\n",
      "Epoch 11/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3015 - accuracy: 0.8807\n",
      "Epoch 12/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2973 - accuracy: 0.8831\n",
      "Epoch 13/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2957 - accuracy: 0.8831\n",
      "Epoch 14/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2934 - accuracy: 0.8833\n",
      "Epoch 15/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2898 - accuracy: 0.8847\n",
      "Epoch 16/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2886 - accuracy: 0.8854\n",
      "Epoch 17/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2858 - accuracy: 0.8865\n",
      "Epoch 18/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2837 - accuracy: 0.8873\n",
      "Epoch 19/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2808 - accuracy: 0.8878\n",
      "Epoch 20/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2795 - accuracy: 0.8877\n",
      "Epoch 21/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2769 - accuracy: 0.8898\n",
      "Epoch 22/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2765 - accuracy: 0.8881\n",
      "Epoch 23/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2746 - accuracy: 0.8902\n",
      "Epoch 24/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2728 - accuracy: 0.8900\n",
      "Epoch 25/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2697 - accuracy: 0.8914\n",
      "Epoch 26/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2686 - accuracy: 0.8918\n",
      "Epoch 27/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2679 - accuracy: 0.8916\n",
      "Epoch 28/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2647 - accuracy: 0.8933\n",
      "Epoch 29/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2628 - accuracy: 0.8943\n",
      "Epoch 30/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2627 - accuracy: 0.8937\n",
      "123/123 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8734339043722833\n",
      "Precision: 0.8788750037304737\n",
      "Recall: 0.8734339043722833\n",
      "F1-Score: 0.8720263799191963\n",
      "ROC-AUC Score: 0.865943216224117\n",
      "Specificity: 0.9457690509583918\n",
      "Fold: 3\n",
      "Epoch 1/30\n",
      "489/489 [==============================] - 3s 2ms/step - loss: 0.4919 - accuracy: 0.7860\n",
      "Epoch 2/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3795 - accuracy: 0.8488\n",
      "Epoch 3/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.3485 - accuracy: 0.8641\n",
      "Epoch 4/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3326 - accuracy: 0.8696\n",
      "Epoch 5/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3236 - accuracy: 0.8741\n",
      "Epoch 6/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.3169 - accuracy: 0.8766\n",
      "Epoch 7/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.3115 - accuracy: 0.8783\n",
      "Epoch 8/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3078 - accuracy: 0.8783\n",
      "Epoch 9/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3055 - accuracy: 0.8806\n",
      "Epoch 10/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.3003 - accuracy: 0.8829\n",
      "Epoch 11/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2983 - accuracy: 0.8827\n",
      "Epoch 12/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2942 - accuracy: 0.8840\n",
      "Epoch 13/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2915 - accuracy: 0.8856\n",
      "Epoch 14/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2905 - accuracy: 0.8847\n",
      "Epoch 15/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2868 - accuracy: 0.8867\n",
      "Epoch 16/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2854 - accuracy: 0.8865\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2821 - accuracy: 0.8883\n",
      "Epoch 18/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2799 - accuracy: 0.8886\n",
      "Epoch 19/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2787 - accuracy: 0.8898\n",
      "Epoch 20/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2761 - accuracy: 0.8909\n",
      "Epoch 21/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2740 - accuracy: 0.8900\n",
      "Epoch 22/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2746 - accuracy: 0.8902\n",
      "Epoch 23/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2711 - accuracy: 0.8927\n",
      "Epoch 24/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2685 - accuracy: 0.8944\n",
      "Epoch 25/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2664 - accuracy: 0.8939\n",
      "Epoch 26/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2640 - accuracy: 0.8948\n",
      "Epoch 27/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2623 - accuracy: 0.8963\n",
      "Epoch 28/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2609 - accuracy: 0.8966\n",
      "Epoch 29/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2629 - accuracy: 0.8946\n",
      "Epoch 30/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2590 - accuracy: 0.8962\n",
      "123/123 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8688315008949118\n",
      "Precision: 0.8808602379461904\n",
      "Recall: 0.8688315008949118\n",
      "F1-Score: 0.8656948438677708\n",
      "ROC-AUC Score: 0.8531805426876672\n",
      "Specificity: 0.9706678700361011\n",
      "Fold: 4\n",
      "Epoch 1/30\n",
      "489/489 [==============================] - 3s 3ms/step - loss: 0.4776 - accuracy: 0.7867\n",
      "Epoch 2/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3760 - accuracy: 0.8500\n",
      "Epoch 3/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3489 - accuracy: 0.8616\n",
      "Epoch 4/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3324 - accuracy: 0.8697\n",
      "Epoch 5/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3241 - accuracy: 0.8720\n",
      "Epoch 6/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3166 - accuracy: 0.8768\n",
      "Epoch 7/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3108 - accuracy: 0.8785\n",
      "Epoch 8/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3054 - accuracy: 0.8806\n",
      "Epoch 9/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3023 - accuracy: 0.8805\n",
      "Epoch 10/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2978 - accuracy: 0.8830\n",
      "Epoch 11/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2943 - accuracy: 0.8841\n",
      "Epoch 12/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2919 - accuracy: 0.8844\n",
      "Epoch 13/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2889 - accuracy: 0.8865\n",
      "Epoch 14/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2856 - accuracy: 0.8876\n",
      "Epoch 15/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2831 - accuracy: 0.8877\n",
      "Epoch 16/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2808 - accuracy: 0.8888\n",
      "Epoch 17/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2784 - accuracy: 0.8887\n",
      "Epoch 18/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2750 - accuracy: 0.8915\n",
      "Epoch 19/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2738 - accuracy: 0.8917\n",
      "Epoch 20/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2700 - accuracy: 0.8931\n",
      "Epoch 21/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2689 - accuracy: 0.8925\n",
      "Epoch 22/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2659 - accuracy: 0.8948\n",
      "Epoch 23/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2640 - accuracy: 0.8964\n",
      "Epoch 24/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2620 - accuracy: 0.8962\n",
      "Epoch 25/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2597 - accuracy: 0.8977\n",
      "Epoch 26/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2605 - accuracy: 0.8970\n",
      "Epoch 27/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2556 - accuracy: 0.8991\n",
      "Epoch 28/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2537 - accuracy: 0.8993\n",
      "Epoch 29/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2516 - accuracy: 0.9018\n",
      "Epoch 30/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2514 - accuracy: 0.8998\n",
      "123/123 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8593350383631714\n",
      "Precision: 0.8673694554669097\n",
      "Recall: 0.8593350383631714\n",
      "F1-Score: 0.8562586630175568\n",
      "ROC-AUC Score: 0.8427008431602084\n",
      "Specificity: 0.9533125833703868\n",
      "Fold: 5\n",
      "Epoch 1/30\n",
      "489/489 [==============================] - 3s 2ms/step - loss: 0.5002 - accuracy: 0.7741\n",
      "Epoch 2/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3929 - accuracy: 0.8417\n",
      "Epoch 3/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3591 - accuracy: 0.8592\n",
      "Epoch 4/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3390 - accuracy: 0.8671\n",
      "Epoch 5/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3269 - accuracy: 0.8723\n",
      "Epoch 6/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.3192 - accuracy: 0.8766\n",
      "Epoch 7/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3128 - accuracy: 0.8775\n",
      "Epoch 8/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3075 - accuracy: 0.8792\n",
      "Epoch 9/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.3019 - accuracy: 0.8827\n",
      "Epoch 10/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2994 - accuracy: 0.8840\n",
      "Epoch 11/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2951 - accuracy: 0.8860\n",
      "Epoch 12/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2947 - accuracy: 0.8853\n",
      "Epoch 13/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2902 - accuracy: 0.8858\n",
      "Epoch 14/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2870 - accuracy: 0.8888\n",
      "Epoch 15/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2865 - accuracy: 0.8879\n",
      "Epoch 16/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2839 - accuracy: 0.8884\n",
      "Epoch 17/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2820 - accuracy: 0.8913\n",
      "Epoch 18/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2786 - accuracy: 0.8911\n",
      "Epoch 19/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2789 - accuracy: 0.8911\n",
      "Epoch 20/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2755 - accuracy: 0.8909\n",
      "Epoch 21/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2756 - accuracy: 0.8918\n",
      "Epoch 22/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2714 - accuracy: 0.8942\n",
      "Epoch 23/30\n",
      "489/489 [==============================] - 2s 3ms/step - loss: 0.2710 - accuracy: 0.8936\n",
      "Epoch 24/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2691 - accuracy: 0.8946\n",
      "Epoch 25/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2666 - accuracy: 0.8948\n",
      "Epoch 26/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2660 - accuracy: 0.8943\n",
      "Epoch 27/30\n",
      "489/489 [==============================] - 1s 2ms/step - loss: 0.2637 - accuracy: 0.8956\n",
      "Epoch 28/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2621 - accuracy: 0.8966\n",
      "Epoch 29/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2636 - accuracy: 0.8952\n",
      "Epoch 30/30\n",
      "489/489 [==============================] - 1s 3ms/step - loss: 0.2588 - accuracy: 0.8957\n",
      "123/123 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.8572890025575448\n",
      "Precision: 0.8644800348728158\n",
      "Recall: 0.8572890025575448\n",
      "F1-Score: 0.8545276820277484\n",
      "ROC-AUC Score: 0.8427154345430051\n",
      "Specificity: 0.9474629546475078\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Convert y_train and y_test to one-hot encoded arrays\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Define number of folds\n",
    "n_splits = 5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "# Initialize list to store accuracy for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores=[]\n",
    "\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X_train, y_train)):\n",
    "\n",
    "    # Print current fold\n",
    "    print(f'Fold: {fold+1}')\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[test_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "    # Define the DNN model architecture\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model on the current fold\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=30, batch_size=32)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    y_pred = np.argmax(model.predict(X_val_fold), axis=-1)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(np.argmax(y_val_fold, axis=-1), y_pred)\n",
    "    precision = precision_score(np.argmax(y_val_fold, axis=-1), y_pred, average='weighted')\n",
    "    recall = recall_score(np.argmax(y_val_fold, axis=-1), y_pred, average='weighted')\n",
    "    f1 = f1_score(np.argmax(y_val_fold, axis=-1), y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(np.argmax(y_val_fold, axis=-1), y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(np.argmax(y_val_fold, axis=-1), y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1-Score: {f1}')\n",
    "    print(f'ROC-AUC Score: {roc_auc}')\n",
    "    print(f'Specificity: {specificity}')\n",
    "\n",
    "    # Add accuracy to the list of accuracy scores\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e66733e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy: 0.8734339043722833\n"
     ]
    }
   ],
   "source": [
    "# Print the mean accuracy score across all folds\n",
    "print(f'Max Accuracy: {max(accuracy_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b32be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2f388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c4fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69788b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
