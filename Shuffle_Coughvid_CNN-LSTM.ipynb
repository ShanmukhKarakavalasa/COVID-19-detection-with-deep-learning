{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044b9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('updated_coughvid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76231e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc{1}</th>\n",
       "      <th>mfcc{2}</th>\n",
       "      <th>mfcc{3}</th>\n",
       "      <th>mfcc{4}</th>\n",
       "      <th>...</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>ddpJitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>aqpq5Shimmer</th>\n",
       "      <th>apq11Shimmer</th>\n",
       "      <th>ddaShimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065850</td>\n",
       "      <td>0.403767</td>\n",
       "      <td>1316.806414</td>\n",
       "      <td>1373.998076</td>\n",
       "      <td>2637.860622</td>\n",
       "      <td>0.057043</td>\n",
       "      <td>-396.59204</td>\n",
       "      <td>69.540160</td>\n",
       "      <td>2.152846</td>\n",
       "      <td>13.354017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.064553</td>\n",
       "      <td>0.187378</td>\n",
       "      <td>1.718899</td>\n",
       "      <td>0.089158</td>\n",
       "      <td>0.143649</td>\n",
       "      <td>0.352439</td>\n",
       "      <td>0.267473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033997</td>\n",
       "      <td>0.532892</td>\n",
       "      <td>2474.234037</td>\n",
       "      <td>2125.162327</td>\n",
       "      <td>4869.731365</td>\n",
       "      <td>0.186172</td>\n",
       "      <td>-435.21085</td>\n",
       "      <td>45.288998</td>\n",
       "      <td>-12.166409</td>\n",
       "      <td>10.258451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014014</td>\n",
       "      <td>0.018379</td>\n",
       "      <td>0.042043</td>\n",
       "      <td>0.130333</td>\n",
       "      <td>1.313323</td>\n",
       "      <td>0.049385</td>\n",
       "      <td>0.059807</td>\n",
       "      <td>0.110768</td>\n",
       "      <td>0.148156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.370873</td>\n",
       "      <td>2158.381678</td>\n",
       "      <td>2007.817231</td>\n",
       "      <td>4750.294555</td>\n",
       "      <td>0.125032</td>\n",
       "      <td>-412.62552</td>\n",
       "      <td>54.555480</td>\n",
       "      <td>-1.768253</td>\n",
       "      <td>3.977824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.037981</td>\n",
       "      <td>0.100577</td>\n",
       "      <td>0.272967</td>\n",
       "      <td>2.125802</td>\n",
       "      <td>0.132889</td>\n",
       "      <td>0.196880</td>\n",
       "      <td>0.160610</td>\n",
       "      <td>0.398668</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047035</td>\n",
       "      <td>0.479319</td>\n",
       "      <td>2678.491315</td>\n",
       "      <td>2139.232294</td>\n",
       "      <td>5136.450596</td>\n",
       "      <td>0.246256</td>\n",
       "      <td>-393.00226</td>\n",
       "      <td>48.030190</td>\n",
       "      <td>-29.901045</td>\n",
       "      <td>25.478853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>0.031223</td>\n",
       "      <td>0.096085</td>\n",
       "      <td>0.175612</td>\n",
       "      <td>1.617597</td>\n",
       "      <td>0.070595</td>\n",
       "      <td>0.094968</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.211784</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.741700</td>\n",
       "      <td>3316.010424</td>\n",
       "      <td>2345.969321</td>\n",
       "      <td>6007.887783</td>\n",
       "      <td>0.278178</td>\n",
       "      <td>-556.18726</td>\n",
       "      <td>10.974088</td>\n",
       "      <td>-10.207122</td>\n",
       "      <td>6.857212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023268</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.069803</td>\n",
       "      <td>0.138516</td>\n",
       "      <td>1.320449</td>\n",
       "      <td>0.075879</td>\n",
       "      <td>0.081973</td>\n",
       "      <td>0.145956</td>\n",
       "      <td>0.227636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24437</th>\n",
       "      <td>0.030827</td>\n",
       "      <td>0.429203</td>\n",
       "      <td>2974.741815</td>\n",
       "      <td>2265.905377</td>\n",
       "      <td>5435.183318</td>\n",
       "      <td>0.216093</td>\n",
       "      <td>-424.03302</td>\n",
       "      <td>38.712093</td>\n",
       "      <td>-16.247238</td>\n",
       "      <td>12.712377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029542</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>0.088625</td>\n",
       "      <td>0.202130</td>\n",
       "      <td>1.619779</td>\n",
       "      <td>0.075903</td>\n",
       "      <td>0.199575</td>\n",
       "      <td>0.160610</td>\n",
       "      <td>0.227709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24438</th>\n",
       "      <td>0.030711</td>\n",
       "      <td>0.535591</td>\n",
       "      <td>2719.621677</td>\n",
       "      <td>2132.117936</td>\n",
       "      <td>5260.719083</td>\n",
       "      <td>0.198633</td>\n",
       "      <td>-471.09518</td>\n",
       "      <td>28.604359</td>\n",
       "      <td>-8.991700</td>\n",
       "      <td>12.397835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019939</td>\n",
       "      <td>0.022426</td>\n",
       "      <td>0.059816</td>\n",
       "      <td>0.176616</td>\n",
       "      <td>1.347352</td>\n",
       "      <td>0.107413</td>\n",
       "      <td>0.148694</td>\n",
       "      <td>0.117848</td>\n",
       "      <td>0.322239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24439</th>\n",
       "      <td>0.159654</td>\n",
       "      <td>0.389324</td>\n",
       "      <td>2360.664509</td>\n",
       "      <td>1696.391140</td>\n",
       "      <td>4114.219514</td>\n",
       "      <td>0.169187</td>\n",
       "      <td>-206.32933</td>\n",
       "      <td>59.016940</td>\n",
       "      <td>-74.789270</td>\n",
       "      <td>-1.210189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019389</td>\n",
       "      <td>0.020776</td>\n",
       "      <td>0.058168</td>\n",
       "      <td>0.164419</td>\n",
       "      <td>1.429379</td>\n",
       "      <td>0.093083</td>\n",
       "      <td>0.119234</td>\n",
       "      <td>0.175221</td>\n",
       "      <td>0.279249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24440</th>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.451451</td>\n",
       "      <td>2788.705294</td>\n",
       "      <td>1800.585083</td>\n",
       "      <td>4708.568653</td>\n",
       "      <td>0.239603</td>\n",
       "      <td>-469.87784</td>\n",
       "      <td>59.399067</td>\n",
       "      <td>-41.076590</td>\n",
       "      <td>-2.028796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>0.036794</td>\n",
       "      <td>0.098542</td>\n",
       "      <td>0.205180</td>\n",
       "      <td>1.715662</td>\n",
       "      <td>0.105993</td>\n",
       "      <td>0.139264</td>\n",
       "      <td>0.166415</td>\n",
       "      <td>0.317979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24441</th>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.167502</td>\n",
       "      <td>951.673749</td>\n",
       "      <td>828.363544</td>\n",
       "      <td>1897.664311</td>\n",
       "      <td>0.050561</td>\n",
       "      <td>-587.85986</td>\n",
       "      <td>24.133877</td>\n",
       "      <td>-4.841230</td>\n",
       "      <td>9.684202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.037088</td>\n",
       "      <td>0.084477</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>1.495134</td>\n",
       "      <td>0.047783</td>\n",
       "      <td>0.049730</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>0.143348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24442 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse  chroma_stft    spec_cent      spec_bw      rolloff       zcr  \\\n",
       "0      0.065850     0.403767  1316.806414  1373.998076  2637.860622  0.057043   \n",
       "1      0.033997     0.532892  2474.234037  2125.162327  4869.731365  0.186172   \n",
       "2      0.023310     0.370873  2158.381678  2007.817231  4750.294555  0.125032   \n",
       "3      0.047035     0.479319  2678.491315  2139.232294  5136.450596  0.246256   \n",
       "4      0.011785     0.741700  3316.010424  2345.969321  6007.887783  0.278178   \n",
       "...         ...          ...          ...          ...          ...       ...   \n",
       "24437  0.030827     0.429203  2974.741815  2265.905377  5435.183318  0.216093   \n",
       "24438  0.030711     0.535591  2719.621677  2132.117936  5260.719083  0.198633   \n",
       "24439  0.159654     0.389324  2360.664509  1696.391140  4114.219514  0.169187   \n",
       "24440  0.030981     0.451451  2788.705294  1800.585083  4708.568653  0.239603   \n",
       "24441  0.007046     0.167502   951.673749   828.363544  1897.664311  0.050561   \n",
       "\n",
       "         mfcc{1}    mfcc{2}    mfcc{3}    mfcc{4}  ...  rapJitter  ppq5Jitter  \\\n",
       "0     -396.59204  69.540160   2.152846  13.354017  ...   0.021518    0.024053   \n",
       "1     -435.21085  45.288998 -12.166409  10.258451  ...   0.014014    0.018379   \n",
       "2     -412.62552  54.555480  -1.768253   3.977824  ...   0.033526    0.037981   \n",
       "3     -393.00226  48.030190 -29.901045  25.478853  ...   0.032028    0.031223   \n",
       "4     -556.18726  10.974088 -10.207122   6.857212  ...   0.023268    0.026566   \n",
       "...          ...        ...        ...        ...  ...        ...         ...   \n",
       "24437 -424.03302  38.712093 -16.247238  12.712377  ...   0.029542    0.036789   \n",
       "24438 -471.09518  28.604359  -8.991700  12.397835  ...   0.019939    0.022426   \n",
       "24439 -206.32933  59.016940 -74.789270  -1.210189  ...   0.019389    0.020776   \n",
       "24440 -469.87784  59.399067 -41.076590  -2.028796  ...   0.032847    0.036794   \n",
       "24441 -587.85986  24.133877  -4.841230   9.684202  ...   0.028159    0.037088   \n",
       "\n",
       "       ddpJitter  localShimmer  localdbShimmer  apq3Shimmer  aqpq5Shimmer  \\\n",
       "0       0.064553      0.187378        1.718899     0.089158      0.143649   \n",
       "1       0.042043      0.130333        1.313323     0.049385      0.059807   \n",
       "2       0.100577      0.272967        2.125802     0.132889      0.196880   \n",
       "3       0.096085      0.175612        1.617597     0.070595      0.094968   \n",
       "4       0.069803      0.138516        1.320449     0.075879      0.081973   \n",
       "...          ...           ...             ...          ...           ...   \n",
       "24437   0.088625      0.202130        1.619779     0.075903      0.199575   \n",
       "24438   0.059816      0.176616        1.347352     0.107413      0.148694   \n",
       "24439   0.058168      0.164419        1.429379     0.093083      0.119234   \n",
       "24440   0.098542      0.205180        1.715662     0.105993      0.139264   \n",
       "24441   0.084477      0.119565        1.495134     0.047783      0.049730   \n",
       "\n",
       "       apq11Shimmer  ddaShimmer  label  \n",
       "0          0.352439    0.267473      0  \n",
       "1          0.110768    0.148156      1  \n",
       "2          0.160610    0.398668      1  \n",
       "3          0.154762    0.211784      0  \n",
       "4          0.145956    0.227636      0  \n",
       "...             ...         ...    ...  \n",
       "24437      0.160610    0.227709      1  \n",
       "24438      0.117848    0.322239      1  \n",
       "24439      0.175221    0.279249      0  \n",
       "24440      0.166415    0.317979      1  \n",
       "24441      0.021235    0.143348      0  \n",
       "\n",
       "[24442 rows x 80 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289820e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, Activation, Multiply, Permute, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdef142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e88abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the features to fit the CNN-LSTM model\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (X.shape[1], X.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4191eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of filters and kernel size for the convolutional layers\n",
    "num_filters = 64\n",
    "kernel_size = 3\n",
    "\n",
    "# Define the number of units for the LSTM layer\n",
    "num_units = 128\n",
    "\n",
    "# Define the dropout rate\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf782dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # Compute the score by multiplying the input with a trainable weight vector\n",
    "    attention_weights = Dense(1, activation='tanh')(inputs)\n",
    "    # Reshape the score to a vector\n",
    "    attention_weights = Flatten()(attention_weights)\n",
    "    # Apply a softmax activation function to the score vector\n",
    "    attention_weights = Activation('softmax')(attention_weights)\n",
    "    # Reshape the score vector to a tensor\n",
    "    attention_weights = Reshape((-1, 1))(attention_weights)\n",
    "    # Multiply the input with the attention weights\n",
    "    multiplied = Multiply()([inputs, attention_weights])\n",
    "    # Compute the output of the attention mechanism\n",
    "    output = K.sum(multiplied, axis=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe917a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv1 = Conv1D(num_filters, kernel_size, activation='relu')(input_layer)\n",
    "maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "conv2 = Conv1D(num_filters, kernel_size, activation='relu')(maxpool1)\n",
    "maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "lstm_layer = LSTM(num_units, return_sequences=True)(maxpool2)\n",
    "attention_layer = attention_3d_block(lstm_layer)\n",
    "dropout_layer = Dropout(dropout_rate)(attention_layer)\n",
    "flatten_layer = Flatten()(dropout_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(flatten_layer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eced8ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52709c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "612/612 [==============================] - 16s 21ms/step - loss: 0.6596 - accuracy: 0.6049 - val_loss: 0.5749 - val_accuracy: 0.7040\n",
      "Epoch 2/20\n",
      "612/612 [==============================] - 11s 18ms/step - loss: 0.5377 - accuracy: 0.7461 - val_loss: 0.5100 - val_accuracy: 0.7689\n",
      "Epoch 3/20\n",
      "612/612 [==============================] - 13s 21ms/step - loss: 0.4921 - accuracy: 0.7759 - val_loss: 0.4714 - val_accuracy: 0.8004\n",
      "Epoch 4/20\n",
      "612/612 [==============================] - 13s 21ms/step - loss: 0.4605 - accuracy: 0.7986 - val_loss: 0.5031 - val_accuracy: 0.7609\n",
      "Epoch 5/20\n",
      "612/612 [==============================] - 12s 19ms/step - loss: 0.4311 - accuracy: 0.8190 - val_loss: 0.4673 - val_accuracy: 0.8036\n",
      "Epoch 6/20\n",
      "612/612 [==============================] - 11s 18ms/step - loss: 0.4205 - accuracy: 0.8244 - val_loss: 0.4076 - val_accuracy: 0.8337\n",
      "Epoch 7/20\n",
      "612/612 [==============================] - 14s 22ms/step - loss: 0.4195 - accuracy: 0.8233 - val_loss: 0.4092 - val_accuracy: 0.8308\n",
      "Epoch 8/20\n",
      "612/612 [==============================] - 14s 22ms/step - loss: 0.4049 - accuracy: 0.8315 - val_loss: 0.3902 - val_accuracy: 0.8400\n",
      "Epoch 9/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.3971 - accuracy: 0.8351 - val_loss: 0.3845 - val_accuracy: 0.8411\n",
      "Epoch 10/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.3875 - accuracy: 0.8416 - val_loss: 0.4458 - val_accuracy: 0.7969\n",
      "Epoch 11/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.3831 - accuracy: 0.8424 - val_loss: 0.4082 - val_accuracy: 0.8243\n",
      "Epoch 12/20\n",
      "612/612 [==============================] - 14s 22ms/step - loss: 0.3787 - accuracy: 0.8460 - val_loss: 0.3734 - val_accuracy: 0.8488\n",
      "Epoch 13/20\n",
      "612/612 [==============================] - 15s 25ms/step - loss: 0.3730 - accuracy: 0.8505 - val_loss: 0.4164 - val_accuracy: 0.8196\n",
      "Epoch 14/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.3710 - accuracy: 0.8497 - val_loss: 0.3957 - val_accuracy: 0.8400\n",
      "Epoch 15/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.3633 - accuracy: 0.8544 - val_loss: 0.3535 - val_accuracy: 0.8607\n",
      "Epoch 16/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.3660 - accuracy: 0.8529 - val_loss: 0.3528 - val_accuracy: 0.8609\n",
      "Epoch 17/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.3608 - accuracy: 0.8561 - val_loss: 0.3532 - val_accuracy: 0.8619\n",
      "Epoch 18/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.3564 - accuracy: 0.8579 - val_loss: 0.3565 - val_accuracy: 0.8570\n",
      "Epoch 19/20\n",
      "612/612 [==============================] - 15s 25ms/step - loss: 0.3494 - accuracy: 0.8617 - val_loss: 0.3447 - val_accuracy: 0.8677\n",
      "Epoch 20/20\n",
      "612/612 [==============================] - 13s 21ms/step - loss: 0.3476 - accuracy: 0.8628 - val_loss: 0.3544 - val_accuracy: 0.8625\n",
      "764/764 [==============================] - 6s 8ms/step - loss: 0.3422 - accuracy: 0.8641\n",
      "Test accuracy: 0.8641273379325867\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "history = model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "648514f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv1 = Conv1D(num_filters, kernel_size, activation='relu')(input_layer)\n",
    "maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "conv2 = Conv1D(num_filters, kernel_size, activation='relu')(maxpool1)\n",
    "maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "conv3 = Conv1D(num_filters, kernel_size, activation='relu')(maxpool2)\n",
    "maxpool3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "lstm_layer = LSTM(num_units, return_sequences=True)(maxpool3)\n",
    "lstm_layer2 = LSTM(num_units, return_sequences=True)(lstm_layer)\n",
    "attention_layer = attention_3d_block(lstm_layer2)\n",
    "dropout_layer = Dropout(dropout_rate)(attention_layer)\n",
    "flatten_layer = Flatten()(dropout_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(flatten_layer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66e57485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "857b58b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "612/612 [==============================] - 25s 26ms/step - loss: 0.6489 - accuracy: 0.6076 - val_loss: 0.5349 - val_accuracy: 0.7492\n",
      "Epoch 2/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.4866 - accuracy: 0.7851 - val_loss: 0.4424 - val_accuracy: 0.8130\n",
      "Epoch 3/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.4345 - accuracy: 0.8163 - val_loss: 0.3914 - val_accuracy: 0.8386\n",
      "Epoch 4/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.4087 - accuracy: 0.8331 - val_loss: 0.3882 - val_accuracy: 0.8437\n",
      "Epoch 5/20\n",
      "612/612 [==============================] - 15s 25ms/step - loss: 0.3973 - accuracy: 0.8397 - val_loss: 0.4037 - val_accuracy: 0.8323\n",
      "Epoch 6/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.3824 - accuracy: 0.8450 - val_loss: 0.3618 - val_accuracy: 0.8556\n",
      "Epoch 7/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.3719 - accuracy: 0.8502 - val_loss: 0.3693 - val_accuracy: 0.8560\n",
      "Epoch 8/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.3660 - accuracy: 0.8538 - val_loss: 0.3528 - val_accuracy: 0.8615\n",
      "Epoch 9/20\n",
      "612/612 [==============================] - 13s 22ms/step - loss: 0.3617 - accuracy: 0.8552 - val_loss: 0.3497 - val_accuracy: 0.8617\n",
      "Epoch 10/20\n",
      "612/612 [==============================] - 17s 28ms/step - loss: 0.3543 - accuracy: 0.8609 - val_loss: 0.3642 - val_accuracy: 0.8550\n",
      "Epoch 11/20\n",
      "612/612 [==============================] - 15s 25ms/step - loss: 0.3504 - accuracy: 0.8642 - val_loss: 0.3437 - val_accuracy: 0.8650\n",
      "Epoch 12/20\n",
      "612/612 [==============================] - 17s 28ms/step - loss: 0.3429 - accuracy: 0.8657 - val_loss: 0.3355 - val_accuracy: 0.8695\n",
      "Epoch 13/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.3457 - accuracy: 0.8653 - val_loss: 0.3456 - val_accuracy: 0.8662\n",
      "Epoch 14/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.3409 - accuracy: 0.8681 - val_loss: 0.3371 - val_accuracy: 0.8695\n",
      "Epoch 15/20\n",
      "612/612 [==============================] - 16s 26ms/step - loss: 0.3376 - accuracy: 0.8699 - val_loss: 0.3435 - val_accuracy: 0.8668\n",
      "Epoch 16/20\n",
      "612/612 [==============================] - 15s 25ms/step - loss: 0.3373 - accuracy: 0.8708 - val_loss: 0.3512 - val_accuracy: 0.8574\n",
      "Epoch 17/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.3609 - accuracy: 0.8553 - val_loss: 0.3416 - val_accuracy: 0.8666\n",
      "Epoch 18/20\n",
      "612/612 [==============================] - 16s 26ms/step - loss: 0.3331 - accuracy: 0.8726 - val_loss: 0.3247 - val_accuracy: 0.8756\n",
      "Epoch 19/20\n",
      "612/612 [==============================] - 14s 23ms/step - loss: 0.3323 - accuracy: 0.8724 - val_loss: 0.3667 - val_accuracy: 0.8605\n",
      "Epoch 20/20\n",
      "612/612 [==============================] - 15s 24ms/step - loss: 0.3290 - accuracy: 0.8737 - val_loss: 0.3404 - val_accuracy: 0.8726\n",
      "764/764 [==============================] - 5s 7ms/step - loss: 0.3245 - accuracy: 0.8753\n",
      "Test accuracy: 0.8752557039260864\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "history = model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac15d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 8s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X, y, verbose=0)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Convert the predicted probabilities to labels\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred_labels)\n",
    "precision = precision_score(y, y_pred_labels)\n",
    "recall = recall_score(y, y_pred_labels)\n",
    "f1_score = f1_score(y, y_pred_labels)\n",
    "roc_auc = roc_auc_score(y, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred_labels).ravel()\n",
    "specificity = tn / (tn + fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eca08d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8752557073889207\n",
      "Precision:  0.9407613097304177\n",
      "Recall:  0.7621859767529059\n",
      "f1_score 0.8421107141007717\n",
      "AUC:  0.8938091233767842\n",
      "Specificity:  0.9628285174967329\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('f1_score',f1_score)\n",
    "print('AUC: ', roc_auc)\n",
    "print('Specificity: ', specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bbcda1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Epoch 1/30\n",
      "510/510 [==============================] - 18s 20ms/step - loss: 0.6389 - accuracy: 0.6249\n",
      "Epoch 2/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.4979 - accuracy: 0.7762\n",
      "Epoch 3/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.4615 - accuracy: 0.7985\n",
      "Epoch 4/30\n",
      "510/510 [==============================] - 13s 26ms/step - loss: 0.4311 - accuracy: 0.8198\n",
      "Epoch 5/30\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.4073 - accuracy: 0.8334\n",
      "Epoch 6/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3941 - accuracy: 0.8416\n",
      "Epoch 7/30\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 0.3814 - accuracy: 0.8482\n",
      "Epoch 8/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.3755 - accuracy: 0.8514\n",
      "Epoch 9/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3695 - accuracy: 0.8541\n",
      "Epoch 10/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.3603 - accuracy: 0.8582\n",
      "Epoch 11/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3583 - accuracy: 0.8585\n",
      "Epoch 12/30\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 0.3525 - accuracy: 0.8613\n",
      "Epoch 13/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3502 - accuracy: 0.8641\n",
      "Epoch 14/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.3432 - accuracy: 0.8663\n",
      "Epoch 15/30\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 0.3466 - accuracy: 0.8652\n",
      "Epoch 16/30\n",
      "510/510 [==============================] - 13s 26ms/step - loss: 0.3447 - accuracy: 0.8659\n",
      "Epoch 17/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.3364 - accuracy: 0.8712\n",
      "Epoch 18/30\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 0.3404 - accuracy: 0.8671\n",
      "Epoch 19/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3396 - accuracy: 0.8684\n",
      "Epoch 20/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3347 - accuracy: 0.8701\n",
      "Epoch 21/30\n",
      "510/510 [==============================] - 10s 21ms/step - loss: 0.3310 - accuracy: 0.8719\n",
      "Epoch 22/30\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 0.3283 - accuracy: 0.8747\n",
      "Epoch 23/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3285 - accuracy: 0.8738\n",
      "Epoch 24/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3274 - accuracy: 0.8730\n",
      "Epoch 25/30\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.3242 - accuracy: 0.8764\n",
      "Epoch 26/30\n",
      "510/510 [==============================] - 14s 27ms/step - loss: 0.3247 - accuracy: 0.8757\n",
      "Epoch 27/30\n",
      "510/510 [==============================] - 14s 27ms/step - loss: 0.3225 - accuracy: 0.8763\n",
      "Epoch 28/30\n",
      "510/510 [==============================] - 14s 28ms/step - loss: 0.3234 - accuracy: 0.8768\n",
      "Epoch 29/30\n",
      "510/510 [==============================] - 14s 27ms/step - loss: 0.3222 - accuracy: 0.8769\n",
      "Epoch 30/30\n",
      "510/510 [==============================] - 14s 28ms/step - loss: 0.3143 - accuracy: 0.8785\n",
      "255/255 [==============================] - 5s 10ms/step\n",
      "Confusion Matrix:\n",
      "[[4237  355]\n",
      " [ 819 2737]]\n",
      "\n",
      "Fold: 2\n",
      "Epoch 1/30\n",
      "510/510 [==============================] - 28s 25ms/step - loss: 0.6298 - accuracy: 0.6359\n",
      "Epoch 2/30\n",
      "510/510 [==============================] - 15s 29ms/step - loss: 0.4912 - accuracy: 0.7829\n",
      "Epoch 3/30\n",
      "510/510 [==============================] - 15s 29ms/step - loss: 0.4418 - accuracy: 0.8137\n",
      "Epoch 4/30\n",
      "510/510 [==============================] - 15s 30ms/step - loss: 0.4084 - accuracy: 0.8311\n",
      "Epoch 5/30\n",
      "510/510 [==============================] - 15s 29ms/step - loss: 0.3959 - accuracy: 0.8387\n",
      "Epoch 6/30\n",
      "510/510 [==============================] - 16s 31ms/step - loss: 0.3869 - accuracy: 0.8433\n",
      "Epoch 7/30\n",
      "510/510 [==============================] - 16s 32ms/step - loss: 0.3757 - accuracy: 0.8519\n",
      "Epoch 8/30\n",
      "510/510 [==============================] - 15s 30ms/step - loss: 0.3660 - accuracy: 0.8571\n",
      "Epoch 9/30\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.3595 - accuracy: 0.8580\n",
      "Epoch 10/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3593 - accuracy: 0.8564\n",
      "Epoch 11/30\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 0.3523 - accuracy: 0.8625\n",
      "Epoch 12/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.3500 - accuracy: 0.8631\n",
      "Epoch 13/30\n",
      "510/510 [==============================] - 14s 28ms/step - loss: 0.3504 - accuracy: 0.8631\n",
      "Epoch 14/30\n",
      "510/510 [==============================] - 14s 27ms/step - loss: 0.3483 - accuracy: 0.8635\n",
      "Epoch 15/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3399 - accuracy: 0.8687\n",
      "Epoch 16/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3390 - accuracy: 0.8679\n",
      "Epoch 17/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3328 - accuracy: 0.8717\n",
      "Epoch 18/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3352 - accuracy: 0.8714\n",
      "Epoch 19/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.3306 - accuracy: 0.8723\n",
      "Epoch 20/30\n",
      "510/510 [==============================] - 16s 31ms/step - loss: 0.3306 - accuracy: 0.8707\n",
      "Epoch 21/30\n",
      "510/510 [==============================] - 13s 26ms/step - loss: 0.3284 - accuracy: 0.8729\n",
      "Epoch 22/30\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 0.3297 - accuracy: 0.8717\n",
      "Epoch 23/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3236 - accuracy: 0.8744\n",
      "Epoch 24/30\n",
      "510/510 [==============================] - 10s 19ms/step - loss: 0.3245 - accuracy: 0.8748\n",
      "Epoch 25/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.3236 - accuracy: 0.8754\n",
      "Epoch 26/30\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.3215 - accuracy: 0.8771\n",
      "Epoch 27/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3206 - accuracy: 0.8759\n",
      "Epoch 28/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3179 - accuracy: 0.8774\n",
      "Epoch 29/30\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 0.3153 - accuracy: 0.8787\n",
      "Epoch 30/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.3150 - accuracy: 0.8794\n",
      "255/255 [==============================] - 4s 9ms/step\n",
      "Confusion Matrix:\n",
      "[[4449  142]\n",
      " [ 931 2625]]\n",
      "\n",
      "Fold: 3\n",
      "Epoch 1/30\n",
      "510/510 [==============================] - 17s 19ms/step - loss: 0.6181 - accuracy: 0.6555\n",
      "Epoch 2/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.4819 - accuracy: 0.7826\n",
      "Epoch 3/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.4368 - accuracy: 0.8154\n",
      "Epoch 4/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.4231 - accuracy: 0.8198\n",
      "Epoch 5/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3947 - accuracy: 0.8404\n",
      "Epoch 6/30\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 0.3894 - accuracy: 0.8429\n",
      "Epoch 7/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3774 - accuracy: 0.8509\n",
      "Epoch 8/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3715 - accuracy: 0.8517\n",
      "Epoch 9/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3660 - accuracy: 0.8544\n",
      "Epoch 10/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.3628 - accuracy: 0.8570\n",
      "Epoch 11/30\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 0.3600 - accuracy: 0.8554\n",
      "Epoch 12/30\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 0.3485 - accuracy: 0.8637\n",
      "Epoch 13/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3489 - accuracy: 0.8624\n",
      "Epoch 14/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3447 - accuracy: 0.8652\n",
      "Epoch 15/30\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 0.3395 - accuracy: 0.8668\n",
      "Epoch 16/30\n",
      "510/510 [==============================] - 9s 19ms/step - loss: 0.3379 - accuracy: 0.8687\n",
      "Epoch 17/30\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.3370 - accuracy: 0.8682\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3380 - accuracy: 0.8674\n",
      "Epoch 19/30\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 0.3325 - accuracy: 0.8707\n",
      "Epoch 20/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3308 - accuracy: 0.8722\n",
      "Epoch 21/30\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 0.3319 - accuracy: 0.8717\n",
      "Epoch 22/30\n",
      "510/510 [==============================] - 14s 27ms/step - loss: 0.3247 - accuracy: 0.8750\n",
      "Epoch 23/30\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.3277 - accuracy: 0.8731\n",
      "Epoch 24/30\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.3216 - accuracy: 0.8752\n",
      "Epoch 25/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3248 - accuracy: 0.8735\n",
      "Epoch 26/30\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 0.3200 - accuracy: 0.8771\n",
      "Epoch 27/30\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 0.3169 - accuracy: 0.8787\n",
      "Epoch 28/30\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.3179 - accuracy: 0.8773\n",
      "Epoch 29/30\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.3123 - accuracy: 0.8797\n",
      "Epoch 30/30\n",
      "510/510 [==============================] - 10s 19ms/step - loss: 0.3149 - accuracy: 0.8781\n",
      "255/255 [==============================] - 2s 5ms/step\n",
      "Confusion Matrix:\n",
      "[[4546   45]\n",
      " [ 988 2568]]\n",
      "\n",
      "accuracy 0.865805166909615\n",
      "precision 0.9388822927631734\n",
      "recall 0.7433445819272592\n",
      "f1_score 0.8287524844349909\n",
      "roc_auc 0.8519989194527883\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding, Bidirectional, Conv1D, MaxPooling1D, Flatten, Activation, multiply, Lambda\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Define the metrics to be calculated\n",
    "metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': [], 'roc_auc': []}\n",
    "\n",
    "# Perform 3-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "for fold, (train, test) in enumerate(kfold.split(X, y)):\n",
    "    print('Fold:', fold+1)\n",
    "    \n",
    "    # Define the model for this fold\n",
    "    conv1 = Conv1D(num_filters, kernel_size, activation='relu')(input_layer)\n",
    "    maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    conv2 = Conv1D(num_filters, kernel_size, activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    conv3 = Conv1D(num_filters, kernel_size, activation='relu')(maxpool2)\n",
    "    maxpool3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "    lstm_layer = LSTM(num_units, return_sequences=True)(maxpool3)\n",
    "    lstm_layer2 = LSTM(num_units, return_sequences=True)(lstm_layer)\n",
    "    attention_layer = attention_3d_block(lstm_layer2)\n",
    "    dropout_layer = Dropout(dropout_rate)(attention_layer)\n",
    "    flatten_layer = Flatten()(dropout_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(flatten_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the training data for this fold\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=32)\n",
    "    \n",
    "    # Make predictions on the test data for this fold\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.round(y_pred).flatten()\n",
    "    \n",
    "    # Calculate the metrics for this fold and append to the list of metrics\n",
    "    metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "    metrics['precision'].append(precision_score(y_test, y_pred))\n",
    "    metrics['recall'].append(recall_score(y_test, y_pred))\n",
    "    metrics['f1_score'].append(f1_score(y_test, y_pred))\n",
    "    metrics['roc_auc'].append(roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    # Print the confusion matrix for this fold\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "\n",
    "# Calculate and print the average metrics over all folds\n",
    "for metric in metrics:\n",
    "    print(metric, np.mean(metrics[metric]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0724d57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': [0.8559155621011291, 0.868295077942801, 0.8732048606849147], 'precision': [0.8851875808538163, 0.9486808818214673, 0.9827784156142365], 'recall': [0.7696850393700787, 0.7381889763779528, 0.7221597300337458], 'f1_score': [0.8234055354993983, 0.8303020718013601, 0.8325498460042146], 'roc_auc': [0.8461883385003703, 0.8536294478927446, 0.8561789719652502]}\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0b6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
