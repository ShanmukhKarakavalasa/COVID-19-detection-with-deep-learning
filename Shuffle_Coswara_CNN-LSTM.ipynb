{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044b9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('updated_coswara.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76231e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc{1}</th>\n",
       "      <th>mfcc{2}</th>\n",
       "      <th>mfcc{3}</th>\n",
       "      <th>mfcc{4}</th>\n",
       "      <th>...</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>ddpJitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>aqpq5Shimmer</th>\n",
       "      <th>apq11Shimmer</th>\n",
       "      <th>ddaShimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039102</td>\n",
       "      <td>0.378903</td>\n",
       "      <td>786.823461</td>\n",
       "      <td>966.699650</td>\n",
       "      <td>1387.984940</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>-412.08945</td>\n",
       "      <td>126.752335</td>\n",
       "      <td>31.558170</td>\n",
       "      <td>18.483738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.212818</td>\n",
       "      <td>1.734878</td>\n",
       "      <td>0.097995</td>\n",
       "      <td>0.147546</td>\n",
       "      <td>0.206467</td>\n",
       "      <td>0.293984</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051812</td>\n",
       "      <td>0.436672</td>\n",
       "      <td>2219.820298</td>\n",
       "      <td>1874.652272</td>\n",
       "      <td>4134.754998</td>\n",
       "      <td>0.209401</td>\n",
       "      <td>-398.93295</td>\n",
       "      <td>50.929253</td>\n",
       "      <td>-17.480385</td>\n",
       "      <td>4.325164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.092139</td>\n",
       "      <td>0.283908</td>\n",
       "      <td>2.113383</td>\n",
       "      <td>0.137225</td>\n",
       "      <td>0.241707</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.411675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.238371</td>\n",
       "      <td>2002.598308</td>\n",
       "      <td>2058.223470</td>\n",
       "      <td>4324.443295</td>\n",
       "      <td>0.146994</td>\n",
       "      <td>-599.82200</td>\n",
       "      <td>40.048190</td>\n",
       "      <td>6.373952</td>\n",
       "      <td>13.369130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.022602</td>\n",
       "      <td>0.056016</td>\n",
       "      <td>0.134015</td>\n",
       "      <td>1.358899</td>\n",
       "      <td>0.061172</td>\n",
       "      <td>0.091838</td>\n",
       "      <td>0.164672</td>\n",
       "      <td>0.183516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.078437</td>\n",
       "      <td>0.242433</td>\n",
       "      <td>569.328347</td>\n",
       "      <td>506.956042</td>\n",
       "      <td>979.591497</td>\n",
       "      <td>0.035271</td>\n",
       "      <td>-423.74878</td>\n",
       "      <td>77.112580</td>\n",
       "      <td>-11.768955</td>\n",
       "      <td>6.080464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050445</td>\n",
       "      <td>0.070875</td>\n",
       "      <td>0.151334</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>1.673087</td>\n",
       "      <td>0.101366</td>\n",
       "      <td>0.093390</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.304098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072712</td>\n",
       "      <td>0.327475</td>\n",
       "      <td>1344.446613</td>\n",
       "      <td>1062.582139</td>\n",
       "      <td>2431.292693</td>\n",
       "      <td>0.089481</td>\n",
       "      <td>-327.21740</td>\n",
       "      <td>160.910540</td>\n",
       "      <td>-60.493977</td>\n",
       "      <td>-20.299130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.003605</td>\n",
       "      <td>0.085836</td>\n",
       "      <td>0.803849</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>0.054323</td>\n",
       "      <td>0.093975</td>\n",
       "      <td>0.107976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24123</th>\n",
       "      <td>0.056854</td>\n",
       "      <td>0.467262</td>\n",
       "      <td>2845.574993</td>\n",
       "      <td>2044.704050</td>\n",
       "      <td>5269.369989</td>\n",
       "      <td>0.235494</td>\n",
       "      <td>-388.71793</td>\n",
       "      <td>32.990032</td>\n",
       "      <td>-8.879510</td>\n",
       "      <td>20.782476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023928</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>0.071784</td>\n",
       "      <td>0.160208</td>\n",
       "      <td>1.659819</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>0.137543</td>\n",
       "      <td>0.331425</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24124</th>\n",
       "      <td>0.050149</td>\n",
       "      <td>0.267105</td>\n",
       "      <td>1199.737564</td>\n",
       "      <td>1636.413852</td>\n",
       "      <td>2392.060470</td>\n",
       "      <td>0.059086</td>\n",
       "      <td>-433.10117</td>\n",
       "      <td>58.741127</td>\n",
       "      <td>37.350792</td>\n",
       "      <td>51.354850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.419731</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.041864</td>\n",
       "      <td>0.054251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24125</th>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.525523</td>\n",
       "      <td>4996.270042</td>\n",
       "      <td>2579.510698</td>\n",
       "      <td>8270.021928</td>\n",
       "      <td>0.418827</td>\n",
       "      <td>-745.49110</td>\n",
       "      <td>-27.318123</td>\n",
       "      <td>-5.540486</td>\n",
       "      <td>16.303144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.050605</td>\n",
       "      <td>0.137923</td>\n",
       "      <td>1.255627</td>\n",
       "      <td>0.061327</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.183981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24126</th>\n",
       "      <td>0.031053</td>\n",
       "      <td>0.167581</td>\n",
       "      <td>1890.217497</td>\n",
       "      <td>2748.644250</td>\n",
       "      <td>5175.546000</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>-499.73740</td>\n",
       "      <td>32.764600</td>\n",
       "      <td>46.792103</td>\n",
       "      <td>40.457275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.016609</td>\n",
       "      <td>0.134111</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.023441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24127</th>\n",
       "      <td>0.024217</td>\n",
       "      <td>0.287329</td>\n",
       "      <td>1296.724388</td>\n",
       "      <td>1259.118567</td>\n",
       "      <td>2319.688878</td>\n",
       "      <td>0.079520</td>\n",
       "      <td>-457.49475</td>\n",
       "      <td>98.845490</td>\n",
       "      <td>-1.881587</td>\n",
       "      <td>40.956110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>0.013602</td>\n",
       "      <td>0.037466</td>\n",
       "      <td>0.106427</td>\n",
       "      <td>1.033221</td>\n",
       "      <td>0.047599</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>0.095882</td>\n",
       "      <td>0.142798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24128 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse  chroma_stft    spec_cent      spec_bw      rolloff       zcr  \\\n",
       "0      0.039102     0.378903   786.823461   966.699650  1387.984940  0.043870   \n",
       "1      0.051812     0.436672  2219.820298  1874.652272  4134.754998  0.209401   \n",
       "2      0.010413     0.238371  2002.598308  2058.223470  4324.443295  0.146994   \n",
       "3      0.078437     0.242433   569.328347   506.956042   979.591497  0.035271   \n",
       "4      0.072712     0.327475  1344.446613  1062.582139  2431.292693  0.089481   \n",
       "...         ...          ...          ...          ...          ...       ...   \n",
       "24123  0.056854     0.467262  2845.574993  2044.704050  5269.369989  0.235494   \n",
       "24124  0.050149     0.267105  1199.737564  1636.413852  2392.060470  0.059086   \n",
       "24125  0.000454     0.525523  4996.270042  2579.510698  8270.021928  0.418827   \n",
       "24126  0.031053     0.167581  1890.217497  2748.644250  5175.546000  0.033493   \n",
       "24127  0.024217     0.287329  1296.724388  1259.118567  2319.688878  0.079520   \n",
       "\n",
       "         mfcc{1}     mfcc{2}    mfcc{3}    mfcc{4}  ...  rapJitter  \\\n",
       "0     -412.08945  126.752335  31.558170  18.483738  ...   0.018672   \n",
       "1     -398.93295   50.929253 -17.480385   4.325164  ...   0.030713   \n",
       "2     -599.82200   40.048190   6.373952  13.369130  ...   0.018672   \n",
       "3     -423.74878   77.112580 -11.768955   6.080464  ...   0.050445   \n",
       "4     -327.21740  160.910540 -60.493977 -20.299130  ...   0.001202   \n",
       "...          ...         ...        ...        ...  ...        ...   \n",
       "24123 -388.71793   32.990032  -8.879510  20.782476  ...   0.023928   \n",
       "24124 -433.10117   58.741127  37.350792  51.354850  ...   0.002124   \n",
       "24125 -745.49110  -27.318123  -5.540486  16.303144  ...   0.016868   \n",
       "24126 -499.73740   32.764600  46.792103  40.457275  ...   0.001794   \n",
       "24127 -457.49475   98.845490  -1.881587  40.956110  ...   0.012489   \n",
       "\n",
       "       ppq5Jitter  ddpJitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "0        0.021894   0.056015      0.212818        1.734878     0.097995   \n",
       "1        0.037553   0.092139      0.283908        2.113383     0.137225   \n",
       "2        0.022602   0.056016      0.134015        1.358899     0.061172   \n",
       "3        0.070875   0.151334      0.181641        1.673087     0.101366   \n",
       "4        0.001537   0.003605      0.085836        0.803849     0.035992   \n",
       "...           ...        ...           ...             ...          ...   \n",
       "24123    0.029485   0.071784      0.160208        1.659819     0.064901   \n",
       "24124    0.002396   0.006373      0.042722        0.419731     0.018084   \n",
       "24125    0.019114   0.050605      0.137923        1.255627     0.061327   \n",
       "24126    0.001854   0.005381      0.016609        0.134111     0.007814   \n",
       "24127    0.013602   0.037466      0.106427        1.033221     0.047599   \n",
       "\n",
       "       aqpq5Shimmer  apq11Shimmer  ddaShimmer  label  \n",
       "0          0.147546      0.206467    0.293984      1  \n",
       "1          0.241707      0.116884    0.411675      0  \n",
       "2          0.091838      0.164672    0.183516      1  \n",
       "3          0.093390      0.116884    0.304098      0  \n",
       "4          0.054323      0.093975    0.107976      1  \n",
       "...             ...           ...         ...    ...  \n",
       "24123      0.137543      0.331425    0.194703      0  \n",
       "24124      0.027000      0.041864    0.054251      0  \n",
       "24125      0.081247      0.116884    0.183981      0  \n",
       "24126      0.009204      0.011837    0.023441      1  \n",
       "24127      0.060751      0.095882    0.142798      0  \n",
       "\n",
       "[24128 rows x 80 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289820e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, Activation, Multiply, Permute, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdef142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e88abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the features to fit the CNN-LSTM model\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (X.shape[1], X.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4191eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of filters and kernel size for the convolutional layers\n",
    "num_filters = 64\n",
    "kernel_size = 3\n",
    "\n",
    "# Define the number of units for the LSTM layer\n",
    "num_units = 128\n",
    "\n",
    "# Define the dropout rate\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf782dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # Compute the score by multiplying the input with a trainable weight vector\n",
    "    attention_weights = Dense(1, activation='tanh')(inputs)\n",
    "    # Reshape the score to a vector\n",
    "    attention_weights = Flatten()(attention_weights)\n",
    "    # Apply a softmax activation function to the score vector\n",
    "    attention_weights = Activation('softmax')(attention_weights)\n",
    "    # Reshape the score vector to a tensor\n",
    "    attention_weights = Reshape((-1, 1))(attention_weights)\n",
    "    # Multiply the input with the attention weights\n",
    "    multiplied = Multiply()([inputs, attention_weights])\n",
    "    # Compute the output of the attention mechanism\n",
    "    output = K.sum(multiplied, axis=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe917a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv1 = Conv1D(num_filters, kernel_size, activation='relu')(input_layer)\n",
    "maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "conv2 = Conv1D(num_filters, kernel_size, activation='relu')(maxpool1)\n",
    "maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "lstm_layer = LSTM(num_units, return_sequences=True)(maxpool2)\n",
    "attention_layer = attention_3d_block(lstm_layer)\n",
    "dropout_layer = Dropout(dropout_rate)(attention_layer)\n",
    "flatten_layer = Flatten()(dropout_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(flatten_layer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eced8ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52709c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "604/604 [==============================] - 50s 50ms/step - loss: 0.6740 - accuracy: 0.5792 - val_loss: 0.6329 - val_accuracy: 0.6577\n",
      "Epoch 2/20\n",
      "604/604 [==============================] - 21s 35ms/step - loss: 0.5851 - accuracy: 0.6883 - val_loss: 0.5698 - val_accuracy: 0.7186\n",
      "Epoch 3/20\n",
      "604/604 [==============================] - 26s 44ms/step - loss: 0.5214 - accuracy: 0.7383 - val_loss: 0.4904 - val_accuracy: 0.7576\n",
      "Epoch 4/20\n",
      "604/604 [==============================] - 27s 45ms/step - loss: 0.5022 - accuracy: 0.7499 - val_loss: 0.5103 - val_accuracy: 0.7464\n",
      "Epoch 5/20\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.4886 - accuracy: 0.7613 - val_loss: 0.5813 - val_accuracy: 0.6846\n",
      "Epoch 6/20\n",
      "604/604 [==============================] - 27s 45ms/step - loss: 0.4819 - accuracy: 0.7620 - val_loss: 0.4844 - val_accuracy: 0.7698\n",
      "Epoch 7/20\n",
      "604/604 [==============================] - 28s 47ms/step - loss: 0.4788 - accuracy: 0.7652 - val_loss: 0.5045 - val_accuracy: 0.7476\n",
      "Epoch 8/20\n",
      "604/604 [==============================] - 29s 47ms/step - loss: 0.4684 - accuracy: 0.7712 - val_loss: 0.4995 - val_accuracy: 0.7451\n",
      "Epoch 9/20\n",
      "604/604 [==============================] - 28s 46ms/step - loss: 0.4623 - accuracy: 0.7747 - val_loss: 0.4538 - val_accuracy: 0.7822\n",
      "Epoch 10/20\n",
      "604/604 [==============================] - 28s 46ms/step - loss: 0.4565 - accuracy: 0.7782 - val_loss: 0.5243 - val_accuracy: 0.7375\n",
      "Epoch 11/20\n",
      "604/604 [==============================] - 29s 48ms/step - loss: 0.4518 - accuracy: 0.7775 - val_loss: 0.4435 - val_accuracy: 0.7845\n",
      "Epoch 12/20\n",
      "604/604 [==============================] - 40s 67ms/step - loss: 0.4432 - accuracy: 0.7865 - val_loss: 0.4453 - val_accuracy: 0.7855\n",
      "Epoch 13/20\n",
      "604/604 [==============================] - 35s 58ms/step - loss: 0.4370 - accuracy: 0.7902 - val_loss: 0.4306 - val_accuracy: 0.7955\n",
      "Epoch 14/20\n",
      "604/604 [==============================] - 31s 52ms/step - loss: 0.4343 - accuracy: 0.7946 - val_loss: 0.4646 - val_accuracy: 0.7712\n",
      "Epoch 15/20\n",
      "604/604 [==============================] - 31s 51ms/step - loss: 0.4244 - accuracy: 0.7984 - val_loss: 0.4438 - val_accuracy: 0.7870\n",
      "Epoch 16/20\n",
      "604/604 [==============================] - 29s 49ms/step - loss: 0.4290 - accuracy: 0.7968 - val_loss: 0.5093 - val_accuracy: 0.7545\n",
      "Epoch 17/20\n",
      "604/604 [==============================] - 34s 56ms/step - loss: 0.4266 - accuracy: 0.7986 - val_loss: 0.4371 - val_accuracy: 0.7901\n",
      "Epoch 18/20\n",
      "604/604 [==============================] - 29s 48ms/step - loss: 0.4204 - accuracy: 0.7992 - val_loss: 0.4135 - val_accuracy: 0.8083\n",
      "Epoch 19/20\n",
      "604/604 [==============================] - 25s 41ms/step - loss: 0.4186 - accuracy: 0.8004 - val_loss: 0.4193 - val_accuracy: 0.8036\n",
      "Epoch 20/20\n",
      "604/604 [==============================] - 22s 36ms/step - loss: 0.4140 - accuracy: 0.8063 - val_loss: 0.4505 - val_accuracy: 0.7710\n",
      "754/754 [==============================] - 10s 13ms/step - loss: 0.4311 - accuracy: 0.7850\n",
      "Test accuracy: 0.7850215435028076\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "history = model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "648514f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv1 = Conv1D(num_filters, kernel_size, activation='relu')(input_layer)\n",
    "maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "conv2 = Conv1D(num_filters, kernel_size, activation='relu')(maxpool1)\n",
    "maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "conv3 = Conv1D(num_filters, kernel_size, activation='relu')(maxpool2)\n",
    "maxpool3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "lstm_layer = LSTM(num_units, return_sequences=True)(maxpool3)\n",
    "lstm_layer2 = LSTM(num_units, return_sequences=True)(lstm_layer)\n",
    "attention_layer = attention_3d_block(lstm_layer2)\n",
    "dropout_layer = Dropout(dropout_rate)(attention_layer)\n",
    "flatten_layer = Flatten()(dropout_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(flatten_layer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66e57485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "857b58b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "604/604 [==============================] - 72s 54ms/step - loss: 0.6452 - accuracy: 0.6155 - val_loss: 0.6099 - val_accuracy: 0.6732\n",
      "Epoch 2/20\n",
      "604/604 [==============================] - 31s 51ms/step - loss: 0.5139 - accuracy: 0.7440 - val_loss: 0.5401 - val_accuracy: 0.7350\n",
      "Epoch 3/20\n",
      "604/604 [==============================] - 31s 51ms/step - loss: 0.4832 - accuracy: 0.7625 - val_loss: 0.4692 - val_accuracy: 0.7694\n",
      "Epoch 4/20\n",
      "604/604 [==============================] - 28s 47ms/step - loss: 0.4711 - accuracy: 0.7719 - val_loss: 0.4648 - val_accuracy: 0.7795\n",
      "Epoch 5/20\n",
      "604/604 [==============================] - 29s 48ms/step - loss: 0.4616 - accuracy: 0.7779 - val_loss: 0.4989 - val_accuracy: 0.7445\n",
      "Epoch 6/20\n",
      "604/604 [==============================] - 28s 46ms/step - loss: 0.4589 - accuracy: 0.7776 - val_loss: 0.4563 - val_accuracy: 0.7808\n",
      "Epoch 7/20\n",
      "604/604 [==============================] - 34s 56ms/step - loss: 0.4472 - accuracy: 0.7832 - val_loss: 0.4559 - val_accuracy: 0.7843\n",
      "Epoch 8/20\n",
      "604/604 [==============================] - 31s 51ms/step - loss: 0.4431 - accuracy: 0.7879 - val_loss: 0.4431 - val_accuracy: 0.7866\n",
      "Epoch 9/20\n",
      "604/604 [==============================] - 30s 50ms/step - loss: 0.4402 - accuracy: 0.7902 - val_loss: 0.4550 - val_accuracy: 0.7727\n",
      "Epoch 10/20\n",
      "604/604 [==============================] - 25s 42ms/step - loss: 0.4282 - accuracy: 0.7975 - val_loss: 0.4345 - val_accuracy: 0.7949\n",
      "Epoch 11/20\n",
      "604/604 [==============================] - 30s 49ms/step - loss: 0.4254 - accuracy: 0.7982 - val_loss: 0.4298 - val_accuracy: 0.7955\n",
      "Epoch 12/20\n",
      "604/604 [==============================] - 28s 46ms/step - loss: 0.4240 - accuracy: 0.7994 - val_loss: 0.4226 - val_accuracy: 0.7978\n",
      "Epoch 13/20\n",
      "604/604 [==============================] - 29s 49ms/step - loss: 0.4185 - accuracy: 0.8010 - val_loss: 0.4263 - val_accuracy: 0.7976\n",
      "Epoch 14/20\n",
      "604/604 [==============================] - 28s 47ms/step - loss: 0.4122 - accuracy: 0.8097 - val_loss: 0.4372 - val_accuracy: 0.7915\n",
      "Epoch 15/20\n",
      "604/604 [==============================] - 27s 45ms/step - loss: 0.4121 - accuracy: 0.8074 - val_loss: 0.4661 - val_accuracy: 0.7708\n",
      "Epoch 16/20\n",
      "604/604 [==============================] - 26s 43ms/step - loss: 0.4072 - accuracy: 0.8082 - val_loss: 0.4163 - val_accuracy: 0.8054\n",
      "Epoch 17/20\n",
      "604/604 [==============================] - 26s 43ms/step - loss: 0.4010 - accuracy: 0.8140 - val_loss: 0.4164 - val_accuracy: 0.8023\n",
      "Epoch 18/20\n",
      "604/604 [==============================] - 24s 40ms/step - loss: 0.4013 - accuracy: 0.8142 - val_loss: 0.4289 - val_accuracy: 0.7940\n",
      "Epoch 19/20\n",
      "604/604 [==============================] - 22s 37ms/step - loss: 0.3992 - accuracy: 0.8147 - val_loss: 0.4116 - val_accuracy: 0.8112\n",
      "Epoch 20/20\n",
      "604/604 [==============================] - 27s 45ms/step - loss: 0.3962 - accuracy: 0.8151 - val_loss: 0.4153 - val_accuracy: 0.8098\n",
      "754/754 [==============================] - 16s 21ms/step - loss: 0.3847 - accuracy: 0.8219\n",
      "Test accuracy: 0.8219496011734009\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "history = model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac15d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754/754 [==============================] - 17s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X, y, verbose=0)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Convert the predicted probabilities to labels\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred_labels)\n",
    "precision = precision_score(y, y_pred_labels)\n",
    "recall = recall_score(y, y_pred_labels)\n",
    "f1_score = f1_score(y, y_pred_labels)\n",
    "roc_auc = roc_auc_score(y, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred_labels).ravel()\n",
    "specificity = tn / (tn + fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eca08d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.821949602122016\n",
      "Precision:  0.8967325042672519\n",
      "Recall:  0.6807663828211773\n",
      "f1_score 0.7739661159633799\n",
      "AUC:  0.894014925590453\n",
      "Specificity:  0.9364305013509456\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('f1_score',f1_score)\n",
    "print('AUC: ', roc_auc)\n",
    "print('Specificity: ', specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bbcda1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "252/252 [==============================] - 6s 17ms/step\n",
      "Confusion Matrix:\n",
      "[[3400 1041]\n",
      " [ 817 2785]]\n",
      "\n",
      "Fold: 2\n",
      "252/252 [==============================] - 11s 22ms/step\n",
      "Confusion Matrix:\n",
      "[[3353 1089]\n",
      " [ 842 2759]]\n",
      "\n",
      "Fold: 3\n",
      "252/252 [==============================] - 11s 23ms/step\n",
      "Confusion Matrix:\n",
      "[[4202  239]\n",
      " [1463 2138]]\n",
      "\n",
      "accuracy 0.7724227428034399\n",
      "precision 0.7814544016358881\n",
      "recall 0.7110271978556145\n",
      "f1_score 0.7353084473123893\n",
      "roc_auc 0.7666163970903049\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding, Bidirectional, Conv1D, MaxPooling1D, Flatten, Activation, multiply, Lambda\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Define the metrics to be calculated\n",
    "metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': [], 'roc_auc': []}\n",
    "\n",
    "# Perform 3-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "for fold, (train, test) in enumerate(kfold.split(X, y)):\n",
    "    print('Fold:', fold+1)\n",
    "    \n",
    "    # Define the model for this fold\n",
    "    conv1 = Conv1D(num_filters, kernel_size, activation='relu')(input_layer)\n",
    "    maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    conv2 = Conv1D(num_filters, kernel_size, activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    conv3 = Conv1D(num_filters, kernel_size, activation='relu')(maxpool2)\n",
    "    maxpool3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "    lstm_layer = LSTM(num_units, return_sequences=True)(maxpool3)\n",
    "    lstm_layer2 = LSTM(num_units, return_sequences=True)(lstm_layer)\n",
    "    attention_layer = attention_3d_block(lstm_layer2)\n",
    "    dropout_layer = Dropout(dropout_rate)(attention_layer)\n",
    "    flatten_layer = Flatten()(dropout_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(flatten_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the training data for this fold\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Make predictions on the test data for this fold\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.round(y_pred).flatten()\n",
    "    \n",
    "    # Calculate the metrics for this fold and append to the list of metrics\n",
    "    metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "    metrics['precision'].append(precision_score(y_test, y_pred))\n",
    "    metrics['recall'].append(recall_score(y_test, y_pred))\n",
    "    metrics['f1_score'].append(f1_score(y_test, y_pred))\n",
    "    metrics['roc_auc'].append(roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    # Print the confusion matrix for this fold\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "\n",
    "# Calculate and print the average metrics over all folds\n",
    "for metric in metrics:\n",
    "    print(metric, np.mean(metrics[metric]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0724d57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': [0.7689916697749596, 0.7599154544324257, 0.7883611042029346], 'precision': [0.7279142707788814, 0.716995841995842, 0.8994530921329407], 'recall': [0.7731815657967795, 0.766176062204943, 0.5937239655651207], 'f1_score': [0.7498653742595585, 0.7407705732313064, 0.7152893944463031], 'roc_auc': [0.7693874503156382, 0.7605081121470461, 0.7699536288082303]}\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0b6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
