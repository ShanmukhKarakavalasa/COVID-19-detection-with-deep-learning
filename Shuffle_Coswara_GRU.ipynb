{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33d3df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('updated_coswara.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d90e38b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc{1}</th>\n",
       "      <th>mfcc{2}</th>\n",
       "      <th>mfcc{3}</th>\n",
       "      <th>mfcc{4}</th>\n",
       "      <th>...</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>ddpJitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>aqpq5Shimmer</th>\n",
       "      <th>apq11Shimmer</th>\n",
       "      <th>ddaShimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039102</td>\n",
       "      <td>0.378903</td>\n",
       "      <td>786.823461</td>\n",
       "      <td>966.699650</td>\n",
       "      <td>1387.984940</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>-412.08945</td>\n",
       "      <td>126.752335</td>\n",
       "      <td>31.558170</td>\n",
       "      <td>18.483738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.212818</td>\n",
       "      <td>1.734878</td>\n",
       "      <td>0.097995</td>\n",
       "      <td>0.147546</td>\n",
       "      <td>0.206467</td>\n",
       "      <td>0.293984</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051812</td>\n",
       "      <td>0.436672</td>\n",
       "      <td>2219.820298</td>\n",
       "      <td>1874.652272</td>\n",
       "      <td>4134.754998</td>\n",
       "      <td>0.209401</td>\n",
       "      <td>-398.93295</td>\n",
       "      <td>50.929253</td>\n",
       "      <td>-17.480385</td>\n",
       "      <td>4.325164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.092139</td>\n",
       "      <td>0.283908</td>\n",
       "      <td>2.113383</td>\n",
       "      <td>0.137225</td>\n",
       "      <td>0.241707</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.411675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.238371</td>\n",
       "      <td>2002.598308</td>\n",
       "      <td>2058.223470</td>\n",
       "      <td>4324.443295</td>\n",
       "      <td>0.146994</td>\n",
       "      <td>-599.82200</td>\n",
       "      <td>40.048190</td>\n",
       "      <td>6.373952</td>\n",
       "      <td>13.369130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.022602</td>\n",
       "      <td>0.056016</td>\n",
       "      <td>0.134015</td>\n",
       "      <td>1.358899</td>\n",
       "      <td>0.061172</td>\n",
       "      <td>0.091838</td>\n",
       "      <td>0.164672</td>\n",
       "      <td>0.183516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.078437</td>\n",
       "      <td>0.242433</td>\n",
       "      <td>569.328347</td>\n",
       "      <td>506.956042</td>\n",
       "      <td>979.591497</td>\n",
       "      <td>0.035271</td>\n",
       "      <td>-423.74878</td>\n",
       "      <td>77.112580</td>\n",
       "      <td>-11.768955</td>\n",
       "      <td>6.080464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050445</td>\n",
       "      <td>0.070875</td>\n",
       "      <td>0.151334</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>1.673087</td>\n",
       "      <td>0.101366</td>\n",
       "      <td>0.093390</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.304098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072712</td>\n",
       "      <td>0.327475</td>\n",
       "      <td>1344.446613</td>\n",
       "      <td>1062.582139</td>\n",
       "      <td>2431.292693</td>\n",
       "      <td>0.089481</td>\n",
       "      <td>-327.21740</td>\n",
       "      <td>160.910540</td>\n",
       "      <td>-60.493977</td>\n",
       "      <td>-20.299130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.003605</td>\n",
       "      <td>0.085836</td>\n",
       "      <td>0.803849</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>0.054323</td>\n",
       "      <td>0.093975</td>\n",
       "      <td>0.107976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24123</th>\n",
       "      <td>0.056854</td>\n",
       "      <td>0.467262</td>\n",
       "      <td>2845.574993</td>\n",
       "      <td>2044.704050</td>\n",
       "      <td>5269.369989</td>\n",
       "      <td>0.235494</td>\n",
       "      <td>-388.71793</td>\n",
       "      <td>32.990032</td>\n",
       "      <td>-8.879510</td>\n",
       "      <td>20.782476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023928</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>0.071784</td>\n",
       "      <td>0.160208</td>\n",
       "      <td>1.659819</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>0.137543</td>\n",
       "      <td>0.331425</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24124</th>\n",
       "      <td>0.050149</td>\n",
       "      <td>0.267105</td>\n",
       "      <td>1199.737564</td>\n",
       "      <td>1636.413852</td>\n",
       "      <td>2392.060470</td>\n",
       "      <td>0.059086</td>\n",
       "      <td>-433.10117</td>\n",
       "      <td>58.741127</td>\n",
       "      <td>37.350792</td>\n",
       "      <td>51.354850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.419731</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.041864</td>\n",
       "      <td>0.054251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24125</th>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.525523</td>\n",
       "      <td>4996.270042</td>\n",
       "      <td>2579.510698</td>\n",
       "      <td>8270.021928</td>\n",
       "      <td>0.418827</td>\n",
       "      <td>-745.49110</td>\n",
       "      <td>-27.318123</td>\n",
       "      <td>-5.540486</td>\n",
       "      <td>16.303144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.050605</td>\n",
       "      <td>0.137923</td>\n",
       "      <td>1.255627</td>\n",
       "      <td>0.061327</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.183981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24126</th>\n",
       "      <td>0.031053</td>\n",
       "      <td>0.167581</td>\n",
       "      <td>1890.217497</td>\n",
       "      <td>2748.644250</td>\n",
       "      <td>5175.546000</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>-499.73740</td>\n",
       "      <td>32.764600</td>\n",
       "      <td>46.792103</td>\n",
       "      <td>40.457275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.016609</td>\n",
       "      <td>0.134111</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.023441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24127</th>\n",
       "      <td>0.024217</td>\n",
       "      <td>0.287329</td>\n",
       "      <td>1296.724388</td>\n",
       "      <td>1259.118567</td>\n",
       "      <td>2319.688878</td>\n",
       "      <td>0.079520</td>\n",
       "      <td>-457.49475</td>\n",
       "      <td>98.845490</td>\n",
       "      <td>-1.881587</td>\n",
       "      <td>40.956110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>0.013602</td>\n",
       "      <td>0.037466</td>\n",
       "      <td>0.106427</td>\n",
       "      <td>1.033221</td>\n",
       "      <td>0.047599</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>0.095882</td>\n",
       "      <td>0.142798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24128 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse  chroma_stft    spec_cent      spec_bw      rolloff       zcr  \\\n",
       "0      0.039102     0.378903   786.823461   966.699650  1387.984940  0.043870   \n",
       "1      0.051812     0.436672  2219.820298  1874.652272  4134.754998  0.209401   \n",
       "2      0.010413     0.238371  2002.598308  2058.223470  4324.443295  0.146994   \n",
       "3      0.078437     0.242433   569.328347   506.956042   979.591497  0.035271   \n",
       "4      0.072712     0.327475  1344.446613  1062.582139  2431.292693  0.089481   \n",
       "...         ...          ...          ...          ...          ...       ...   \n",
       "24123  0.056854     0.467262  2845.574993  2044.704050  5269.369989  0.235494   \n",
       "24124  0.050149     0.267105  1199.737564  1636.413852  2392.060470  0.059086   \n",
       "24125  0.000454     0.525523  4996.270042  2579.510698  8270.021928  0.418827   \n",
       "24126  0.031053     0.167581  1890.217497  2748.644250  5175.546000  0.033493   \n",
       "24127  0.024217     0.287329  1296.724388  1259.118567  2319.688878  0.079520   \n",
       "\n",
       "         mfcc{1}     mfcc{2}    mfcc{3}    mfcc{4}  ...  rapJitter  \\\n",
       "0     -412.08945  126.752335  31.558170  18.483738  ...   0.018672   \n",
       "1     -398.93295   50.929253 -17.480385   4.325164  ...   0.030713   \n",
       "2     -599.82200   40.048190   6.373952  13.369130  ...   0.018672   \n",
       "3     -423.74878   77.112580 -11.768955   6.080464  ...   0.050445   \n",
       "4     -327.21740  160.910540 -60.493977 -20.299130  ...   0.001202   \n",
       "...          ...         ...        ...        ...  ...        ...   \n",
       "24123 -388.71793   32.990032  -8.879510  20.782476  ...   0.023928   \n",
       "24124 -433.10117   58.741127  37.350792  51.354850  ...   0.002124   \n",
       "24125 -745.49110  -27.318123  -5.540486  16.303144  ...   0.016868   \n",
       "24126 -499.73740   32.764600  46.792103  40.457275  ...   0.001794   \n",
       "24127 -457.49475   98.845490  -1.881587  40.956110  ...   0.012489   \n",
       "\n",
       "       ppq5Jitter  ddpJitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "0        0.021894   0.056015      0.212818        1.734878     0.097995   \n",
       "1        0.037553   0.092139      0.283908        2.113383     0.137225   \n",
       "2        0.022602   0.056016      0.134015        1.358899     0.061172   \n",
       "3        0.070875   0.151334      0.181641        1.673087     0.101366   \n",
       "4        0.001537   0.003605      0.085836        0.803849     0.035992   \n",
       "...           ...        ...           ...             ...          ...   \n",
       "24123    0.029485   0.071784      0.160208        1.659819     0.064901   \n",
       "24124    0.002396   0.006373      0.042722        0.419731     0.018084   \n",
       "24125    0.019114   0.050605      0.137923        1.255627     0.061327   \n",
       "24126    0.001854   0.005381      0.016609        0.134111     0.007814   \n",
       "24127    0.013602   0.037466      0.106427        1.033221     0.047599   \n",
       "\n",
       "       aqpq5Shimmer  apq11Shimmer  ddaShimmer  label  \n",
       "0          0.147546      0.206467    0.293984      1  \n",
       "1          0.241707      0.116884    0.411675      0  \n",
       "2          0.091838      0.164672    0.183516      1  \n",
       "3          0.093390      0.116884    0.304098      0  \n",
       "4          0.054323      0.093975    0.107976      1  \n",
       "...             ...           ...         ...    ...  \n",
       "24123      0.137543      0.331425    0.194703      0  \n",
       "24124      0.027000      0.041864    0.054251      0  \n",
       "24125      0.081247      0.116884    0.183981      0  \n",
       "24126      0.009204      0.011837    0.023441      1  \n",
       "24127      0.060751      0.095882    0.142798      0  \n",
       "\n",
       "[24128 rows x 80 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "311e092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d973c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "43a4f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57675174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f4ddacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(units=32, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4775a830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 1s 8ms/step\n",
      "Accuracy: 0.7368421052631579\n",
      "Precision: 0.7970071567989591\n",
      "Recall: 0.5611543747136968\n",
      "F1 Score: 0.6586021505376345\n",
      "ROC AUC: 0.8156025934936648\n",
      "Specificity: 0.8819523269012486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model on test set\n",
    "accuracy = accuracy_score(y_test, y_pred.round())\n",
    "precision = precision_score(y_test, y_pred.round())\n",
    "recall = recall_score(y_test, y_pred.round())\n",
    "f1 = f1_score(y_test, y_pred.round())\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred.round()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b589f85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "604/604 [==============================] - 55s 74ms/step - loss: 0.7255 - accuracy: 0.5841 - val_loss: 0.6604 - val_accuracy: 0.6119\n",
      "Epoch 2/50\n",
      "604/604 [==============================] - 42s 70ms/step - loss: 0.6687 - accuracy: 0.6002 - val_loss: 0.6478 - val_accuracy: 0.6202\n",
      "Epoch 3/50\n",
      "604/604 [==============================] - 45s 75ms/step - loss: 0.6703 - accuracy: 0.6020 - val_loss: 0.6294 - val_accuracy: 0.6349\n",
      "Epoch 4/50\n",
      "604/604 [==============================] - 45s 74ms/step - loss: 0.6569 - accuracy: 0.6118 - val_loss: 0.6630 - val_accuracy: 0.6022\n",
      "Epoch 5/50\n",
      "604/604 [==============================] - 47s 78ms/step - loss: 0.6490 - accuracy: 0.6276 - val_loss: 0.6190 - val_accuracy: 0.6511\n",
      "Epoch 6/50\n",
      "604/604 [==============================] - 46s 76ms/step - loss: 0.6368 - accuracy: 0.6457 - val_loss: 0.6142 - val_accuracy: 0.6631\n",
      "Epoch 7/50\n",
      "604/604 [==============================] - 45s 75ms/step - loss: 0.6296 - accuracy: 0.6549 - val_loss: 0.6083 - val_accuracy: 0.6695\n",
      "Epoch 8/50\n",
      "604/604 [==============================] - 44s 73ms/step - loss: 0.6252 - accuracy: 0.6602 - val_loss: 0.6151 - val_accuracy: 0.6645\n",
      "Epoch 9/50\n",
      "604/604 [==============================] - 44s 73ms/step - loss: 0.6261 - accuracy: 0.6549 - val_loss: 0.5955 - val_accuracy: 0.6842\n",
      "Epoch 10/50\n",
      "604/604 [==============================] - 44s 73ms/step - loss: 0.6161 - accuracy: 0.6717 - val_loss: 0.5934 - val_accuracy: 0.6873\n",
      "Epoch 11/50\n",
      "604/604 [==============================] - 47s 78ms/step - loss: 0.6408 - accuracy: 0.6372 - val_loss: 0.6260 - val_accuracy: 0.6527\n",
      "Epoch 12/50\n",
      "604/604 [==============================] - 44s 72ms/step - loss: 0.6465 - accuracy: 0.6341 - val_loss: 0.6671 - val_accuracy: 0.5756\n",
      "Epoch 13/50\n",
      "604/604 [==============================] - 53s 88ms/step - loss: 0.6555 - accuracy: 0.6134 - val_loss: 0.6343 - val_accuracy: 0.6349\n",
      "Epoch 14/50\n",
      "604/604 [==============================] - 41s 68ms/step - loss: 0.6393 - accuracy: 0.6356 - val_loss: 0.6246 - val_accuracy: 0.6403\n",
      "Epoch 15/50\n",
      "604/604 [==============================] - 47s 79ms/step - loss: 0.6353 - accuracy: 0.6403 - val_loss: 0.6157 - val_accuracy: 0.6542\n",
      "Epoch 16/50\n",
      "604/604 [==============================] - 45s 75ms/step - loss: 0.6264 - accuracy: 0.6577 - val_loss: 0.6155 - val_accuracy: 0.6658\n",
      "Epoch 17/50\n",
      "604/604 [==============================] - 43s 71ms/step - loss: 0.6166 - accuracy: 0.6670 - val_loss: 0.5950 - val_accuracy: 0.6801\n",
      "Epoch 18/50\n",
      "604/604 [==============================] - 45s 75ms/step - loss: 0.6211 - accuracy: 0.6611 - val_loss: 0.5960 - val_accuracy: 0.6801\n",
      "Epoch 19/50\n",
      "604/604 [==============================] - 45s 74ms/step - loss: 0.6072 - accuracy: 0.6735 - val_loss: 0.6079 - val_accuracy: 0.6809\n",
      "Epoch 20/50\n",
      "604/604 [==============================] - 46s 76ms/step - loss: 0.6085 - accuracy: 0.6796 - val_loss: 0.5841 - val_accuracy: 0.6908\n",
      "Epoch 21/50\n",
      "604/604 [==============================] - 50s 83ms/step - loss: 0.6424 - accuracy: 0.6297 - val_loss: 0.6083 - val_accuracy: 0.6581\n",
      "Epoch 22/50\n",
      "604/604 [==============================] - 55s 91ms/step - loss: 0.6027 - accuracy: 0.6779 - val_loss: 0.5871 - val_accuracy: 0.6935\n",
      "Epoch 23/50\n",
      "604/604 [==============================] - 56s 92ms/step - loss: 0.5917 - accuracy: 0.6927 - val_loss: 0.5736 - val_accuracy: 0.7058\n",
      "Epoch 24/50\n",
      "604/604 [==============================] - 54s 89ms/step - loss: 0.6164 - accuracy: 0.6614 - val_loss: 0.6269 - val_accuracy: 0.6637\n",
      "Epoch 25/50\n",
      "604/604 [==============================] - 53s 88ms/step - loss: 0.6222 - accuracy: 0.6684 - val_loss: 0.5857 - val_accuracy: 0.6861\n",
      "Epoch 26/50\n",
      "604/604 [==============================] - 53s 87ms/step - loss: 0.5938 - accuracy: 0.6900 - val_loss: 0.5771 - val_accuracy: 0.7066\n",
      "Epoch 27/50\n",
      "604/604 [==============================] - 54s 89ms/step - loss: 0.5848 - accuracy: 0.6977 - val_loss: 0.5799 - val_accuracy: 0.7018\n",
      "Epoch 28/50\n",
      "604/604 [==============================] - 59s 98ms/step - loss: 0.5940 - accuracy: 0.6828 - val_loss: 0.6208 - val_accuracy: 0.6552\n",
      "Epoch 29/50\n",
      "604/604 [==============================] - 60s 99ms/step - loss: 0.6025 - accuracy: 0.6831 - val_loss: 0.5815 - val_accuracy: 0.7053\n",
      "Epoch 30/50\n",
      "604/604 [==============================] - 68s 113ms/step - loss: 0.5799 - accuracy: 0.7008 - val_loss: 0.5715 - val_accuracy: 0.7012\n",
      "Epoch 31/50\n",
      "604/604 [==============================] - 54s 89ms/step - loss: 0.5835 - accuracy: 0.6986 - val_loss: 0.5568 - val_accuracy: 0.7176\n",
      "Epoch 32/50\n",
      "604/604 [==============================] - 32s 53ms/step - loss: 0.5747 - accuracy: 0.7101 - val_loss: 0.5700 - val_accuracy: 0.6989\n",
      "Epoch 33/50\n",
      "604/604 [==============================] - 32s 53ms/step - loss: 0.5911 - accuracy: 0.6906 - val_loss: 0.5529 - val_accuracy: 0.7082\n",
      "Epoch 34/50\n",
      "604/604 [==============================] - 34s 56ms/step - loss: 0.6366 - accuracy: 0.6359 - val_loss: 0.6645 - val_accuracy: 0.5968\n",
      "Epoch 35/50\n",
      "604/604 [==============================] - 32s 53ms/step - loss: 0.6462 - accuracy: 0.6283 - val_loss: 0.6182 - val_accuracy: 0.6558\n",
      "Epoch 36/50\n",
      "604/604 [==============================] - 32s 53ms/step - loss: 0.6201 - accuracy: 0.6571 - val_loss: 0.6013 - val_accuracy: 0.6699\n",
      "Epoch 37/50\n",
      "604/604 [==============================] - 33s 55ms/step - loss: 0.6046 - accuracy: 0.6716 - val_loss: 0.5952 - val_accuracy: 0.6741\n",
      "Epoch 38/50\n",
      "604/604 [==============================] - 34s 56ms/step - loss: 0.5917 - accuracy: 0.6888 - val_loss: 0.5733 - val_accuracy: 0.6952\n",
      "Epoch 39/50\n",
      "604/604 [==============================] - 32s 54ms/step - loss: 0.5788 - accuracy: 0.7047 - val_loss: 0.5751 - val_accuracy: 0.7012\n",
      "Epoch 40/50\n",
      "604/604 [==============================] - 29s 49ms/step - loss: 0.5821 - accuracy: 0.7012 - val_loss: 0.5975 - val_accuracy: 0.6792\n",
      "Epoch 41/50\n",
      "604/604 [==============================] - 33s 55ms/step - loss: 0.5600 - accuracy: 0.7212 - val_loss: 0.5373 - val_accuracy: 0.7269\n",
      "Epoch 42/50\n",
      "604/604 [==============================] - 32s 54ms/step - loss: 0.6085 - accuracy: 0.6723 - val_loss: 0.5591 - val_accuracy: 0.7188\n",
      "Epoch 43/50\n",
      "604/604 [==============================] - 32s 54ms/step - loss: 0.5514 - accuracy: 0.7276 - val_loss: 0.5236 - val_accuracy: 0.7346\n",
      "Epoch 44/50\n",
      "604/604 [==============================] - 31s 51ms/step - loss: 0.5376 - accuracy: 0.7387 - val_loss: 0.5248 - val_accuracy: 0.7395\n",
      "Epoch 45/50\n",
      "604/604 [==============================] - 31s 52ms/step - loss: 0.6685 - accuracy: 0.5891 - val_loss: 0.6260 - val_accuracy: 0.6459\n",
      "Epoch 46/50\n",
      "604/604 [==============================] - 32s 53ms/step - loss: 0.6289 - accuracy: 0.6480 - val_loss: 0.6127 - val_accuracy: 0.6560\n",
      "Epoch 47/50\n",
      "604/604 [==============================] - 31s 52ms/step - loss: 0.6088 - accuracy: 0.6763 - val_loss: 0.6549 - val_accuracy: 0.6073\n",
      "Epoch 48/50\n",
      "604/604 [==============================] - 32s 54ms/step - loss: 0.5984 - accuracy: 0.6906 - val_loss: 0.5533 - val_accuracy: 0.7211\n",
      "Epoch 49/50\n",
      "604/604 [==============================] - 31s 51ms/step - loss: 0.5524 - accuracy: 0.7314 - val_loss: 0.5175 - val_accuracy: 0.7474\n",
      "Epoch 50/50\n",
      "604/604 [==============================] - 32s 53ms/step - loss: 0.5802 - accuracy: 0.7016 - val_loss: 0.5236 - val_accuracy: 0.7348\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout\n",
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    GRU(units=64, return_sequences=True,  input_shape=(X_train.shape[1], 1)),\n",
    "    Dropout(0.2),\n",
    "    GRU(units=64),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f86c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 5s 25ms/step\n",
      "Accuracy: 0.7347699958557812\n",
      "Precision: 0.7318952234206472\n",
      "Recall: 0.6527714154832799\n",
      "F1 Score: 0.6900726392251816\n",
      "ROC AUC: 0.8095024168630818\n",
      "Specificity: 0.8024971623155505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model on test set\n",
    "accuracy = accuracy_score(y_test, y_pred.round())\n",
    "precision = precision_score(y_test, y_pred.round())\n",
    "recall = recall_score(y_test, y_pred.round())\n",
    "f1 = f1_score(y_test, y_pred.round())\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred.round()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "48d42f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "604/604 [==============================] - 52s 76ms/step - loss: 0.6611 - accuracy: 0.6051 - val_loss: 0.6233 - val_accuracy: 0.6496\n",
      "Epoch 2/50\n",
      "604/604 [==============================] - 46s 76ms/step - loss: 0.6272 - accuracy: 0.6517 - val_loss: 0.6144 - val_accuracy: 0.6674\n",
      "Epoch 3/50\n",
      "604/604 [==============================] - 59s 98ms/step - loss: 0.6067 - accuracy: 0.6792 - val_loss: 0.5987 - val_accuracy: 0.6768\n",
      "Epoch 4/50\n",
      "604/604 [==============================] - 67s 110ms/step - loss: 0.5928 - accuracy: 0.6851 - val_loss: 0.5844 - val_accuracy: 0.6933\n",
      "Epoch 5/50\n",
      "604/604 [==============================] - 66s 110ms/step - loss: 0.5796 - accuracy: 0.6953 - val_loss: 0.5606 - val_accuracy: 0.7099\n",
      "Epoch 6/50\n",
      "604/604 [==============================] - 68s 113ms/step - loss: 0.5645 - accuracy: 0.7127 - val_loss: 0.5672 - val_accuracy: 0.7014\n",
      "Epoch 7/50\n",
      "604/604 [==============================] - 63s 104ms/step - loss: 0.5530 - accuracy: 0.7195 - val_loss: 0.5408 - val_accuracy: 0.7250\n",
      "Epoch 8/50\n",
      "604/604 [==============================] - 66s 110ms/step - loss: 0.5368 - accuracy: 0.7327 - val_loss: 0.5271 - val_accuracy: 0.7333\n",
      "Epoch 9/50\n",
      "604/604 [==============================] - 70s 115ms/step - loss: 0.5241 - accuracy: 0.7373 - val_loss: 0.5194 - val_accuracy: 0.7395\n",
      "Epoch 10/50\n",
      "604/604 [==============================] - 63s 104ms/step - loss: 0.5133 - accuracy: 0.7461 - val_loss: 0.5076 - val_accuracy: 0.7389\n",
      "Epoch 11/50\n",
      "604/604 [==============================] - 67s 111ms/step - loss: 0.5002 - accuracy: 0.7534 - val_loss: 0.5101 - val_accuracy: 0.7424\n",
      "Epoch 12/50\n",
      "604/604 [==============================] - 64s 106ms/step - loss: 0.4882 - accuracy: 0.7638 - val_loss: 0.5013 - val_accuracy: 0.7478\n",
      "Epoch 13/50\n",
      "604/604 [==============================] - 64s 105ms/step - loss: 0.4845 - accuracy: 0.7656 - val_loss: 0.4824 - val_accuracy: 0.7582\n",
      "Epoch 14/50\n",
      "604/604 [==============================] - 74s 123ms/step - loss: 0.4753 - accuracy: 0.7684 - val_loss: 0.4916 - val_accuracy: 0.7553\n",
      "Epoch 15/50\n",
      "604/604 [==============================] - 77s 128ms/step - loss: 0.4673 - accuracy: 0.7739 - val_loss: 0.4813 - val_accuracy: 0.7659\n",
      "Epoch 16/50\n",
      "604/604 [==============================] - 69s 115ms/step - loss: 0.4600 - accuracy: 0.7805 - val_loss: 0.4709 - val_accuracy: 0.7683\n",
      "Epoch 17/50\n",
      "604/604 [==============================] - 68s 112ms/step - loss: 0.4503 - accuracy: 0.7860 - val_loss: 0.4709 - val_accuracy: 0.7729\n",
      "Epoch 18/50\n",
      "604/604 [==============================] - 65s 107ms/step - loss: 0.4431 - accuracy: 0.7905 - val_loss: 0.4726 - val_accuracy: 0.7725\n",
      "Epoch 19/50\n",
      "604/604 [==============================] - 69s 115ms/step - loss: 0.4370 - accuracy: 0.7933 - val_loss: 0.4742 - val_accuracy: 0.7696\n",
      "Epoch 20/50\n",
      "604/604 [==============================] - 52s 86ms/step - loss: 0.4331 - accuracy: 0.7953 - val_loss: 0.4722 - val_accuracy: 0.7671\n",
      "Epoch 21/50\n",
      "604/604 [==============================] - 55s 91ms/step - loss: 0.4245 - accuracy: 0.8006 - val_loss: 0.4709 - val_accuracy: 0.7675\n",
      "Epoch 22/50\n",
      "604/604 [==============================] - 52s 87ms/step - loss: 0.4153 - accuracy: 0.8082 - val_loss: 0.4814 - val_accuracy: 0.7743\n",
      "Epoch 23/50\n",
      "604/604 [==============================] - 56s 92ms/step - loss: 0.4104 - accuracy: 0.8082 - val_loss: 0.4685 - val_accuracy: 0.7723\n",
      "Epoch 24/50\n",
      "604/604 [==============================] - 55s 91ms/step - loss: 0.4043 - accuracy: 0.8117 - val_loss: 0.4731 - val_accuracy: 0.7719\n",
      "Epoch 25/50\n",
      "604/604 [==============================] - 54s 89ms/step - loss: 0.3995 - accuracy: 0.8167 - val_loss: 0.4730 - val_accuracy: 0.7731\n",
      "Epoch 26/50\n",
      "604/604 [==============================] - 63s 105ms/step - loss: 0.3931 - accuracy: 0.8185 - val_loss: 0.4829 - val_accuracy: 0.7750\n",
      "Epoch 27/50\n",
      "604/604 [==============================] - 70s 117ms/step - loss: 0.3877 - accuracy: 0.8220 - val_loss: 0.4981 - val_accuracy: 0.7644\n",
      "Epoch 28/50\n",
      "604/604 [==============================] - 77s 128ms/step - loss: 0.3805 - accuracy: 0.8260 - val_loss: 0.4785 - val_accuracy: 0.7812\n",
      "Epoch 29/50\n",
      "604/604 [==============================] - 78s 129ms/step - loss: 0.3692 - accuracy: 0.8321 - val_loss: 0.4906 - val_accuracy: 0.7727\n",
      "Epoch 30/50\n",
      "604/604 [==============================] - 81s 134ms/step - loss: 0.3671 - accuracy: 0.8332 - val_loss: 0.4721 - val_accuracy: 0.7810\n",
      "Epoch 31/50\n",
      "604/604 [==============================] - 74s 123ms/step - loss: 0.3621 - accuracy: 0.8341 - val_loss: 0.4754 - val_accuracy: 0.7779\n",
      "Epoch 32/50\n",
      "604/604 [==============================] - 71s 118ms/step - loss: 0.3571 - accuracy: 0.8375 - val_loss: 0.4796 - val_accuracy: 0.7822\n",
      "Epoch 33/50\n",
      "604/604 [==============================] - 78s 129ms/step - loss: 0.3505 - accuracy: 0.8420 - val_loss: 0.4790 - val_accuracy: 0.7841\n",
      "Epoch 34/50\n",
      "604/604 [==============================] - 89s 147ms/step - loss: 0.3462 - accuracy: 0.8439 - val_loss: 0.4942 - val_accuracy: 0.7768\n",
      "Epoch 35/50\n",
      "604/604 [==============================] - 71s 118ms/step - loss: 0.3419 - accuracy: 0.8461 - val_loss: 0.4877 - val_accuracy: 0.7764\n",
      "Epoch 36/50\n",
      "604/604 [==============================] - 69s 114ms/step - loss: 0.3348 - accuracy: 0.8483 - val_loss: 0.5064 - val_accuracy: 0.7712\n",
      "Epoch 37/50\n",
      "604/604 [==============================] - 70s 115ms/step - loss: 0.3310 - accuracy: 0.8526 - val_loss: 0.5210 - val_accuracy: 0.7683\n",
      "Epoch 38/50\n",
      "604/604 [==============================] - 76s 125ms/step - loss: 0.3302 - accuracy: 0.8543 - val_loss: 0.5059 - val_accuracy: 0.7712\n",
      "Epoch 39/50\n",
      "604/604 [==============================] - 76s 126ms/step - loss: 0.3146 - accuracy: 0.8588 - val_loss: 0.4930 - val_accuracy: 0.7766\n",
      "Epoch 40/50\n",
      "604/604 [==============================] - 81s 135ms/step - loss: 0.3129 - accuracy: 0.8598 - val_loss: 0.5216 - val_accuracy: 0.7710\n",
      "Epoch 41/50\n",
      "604/604 [==============================] - 77s 127ms/step - loss: 0.3117 - accuracy: 0.8623 - val_loss: 0.5042 - val_accuracy: 0.7748\n",
      "Epoch 42/50\n",
      "604/604 [==============================] - 79s 131ms/step - loss: 0.3033 - accuracy: 0.8671 - val_loss: 0.5296 - val_accuracy: 0.7675\n",
      "Epoch 43/50\n",
      "604/604 [==============================] - 83s 137ms/step - loss: 0.3052 - accuracy: 0.8652 - val_loss: 0.5134 - val_accuracy: 0.7683\n",
      "Epoch 44/50\n",
      "604/604 [==============================] - 71s 117ms/step - loss: 0.2949 - accuracy: 0.8718 - val_loss: 0.5488 - val_accuracy: 0.7685\n",
      "Epoch 45/50\n",
      "604/604 [==============================] - 82s 136ms/step - loss: 0.2906 - accuracy: 0.8756 - val_loss: 0.5406 - val_accuracy: 0.7692\n",
      "Epoch 46/50\n",
      "604/604 [==============================] - 72s 119ms/step - loss: 0.2860 - accuracy: 0.8764 - val_loss: 0.5380 - val_accuracy: 0.7758\n",
      "Epoch 47/50\n",
      "604/604 [==============================] - 76s 126ms/step - loss: 0.2858 - accuracy: 0.8772 - val_loss: 0.5289 - val_accuracy: 0.7677\n",
      "Epoch 48/50\n",
      "604/604 [==============================] - 86s 142ms/step - loss: 0.2781 - accuracy: 0.8807 - val_loss: 0.5580 - val_accuracy: 0.7607\n",
      "Epoch 49/50\n",
      "604/604 [==============================] - 102s 168ms/step - loss: 0.2747 - accuracy: 0.8786 - val_loss: 0.5511 - val_accuracy: 0.7611\n",
      "Epoch 50/50\n",
      "604/604 [==============================] - 86s 142ms/step - loss: 0.2663 - accuracy: 0.8843 - val_loss: 0.5491 - val_accuracy: 0.7710\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23768\\582713396.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Make predictions on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Compute evaluation metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    Dropout(0.2),\n",
    "    GRU(units=64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(units=64),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "349a8510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 5s 36ms/step\n",
      "Accuracy: 0.7710\n",
      "Precision: 0.7870\n",
      "Recall: 0.6770\n",
      "F1 score: 0.7279\n",
      "AUC: 0.8451\n",
      "Specificity: 0.8487\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "acc = (y_pred_binary.flatten() == y_test).mean()\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 score: {f1:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7620f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "604/604 [==============================] - 295s 455ms/step - loss: 0.6710 - accuracy: 0.5894 - precision_11: 0.5607 - recall_11: 0.3725 - auc: 0.6087 - specificity: 0.7636 - val_loss: 0.6750 - val_accuracy: 0.5535 - val_precision_11: 0.8684 - val_recall_11: 0.0151 - val_auc: 0.6435 - val_specificity: 0.9978\n",
      "Epoch 2/50\n",
      "604/604 [==============================] - 271s 448ms/step - loss: 0.6356 - accuracy: 0.6454 - precision_11: 0.6309 - recall_11: 0.4966 - auc: 0.6810 - specificity: 0.7667 - val_loss: 0.6269 - val_accuracy: 0.6540 - val_precision_11: 0.6803 - val_recall_11: 0.4434 - val_auc: 0.7092 - val_specificity: 0.8260\n",
      "Epoch 3/50\n",
      "604/604 [==============================] - 375s 621ms/step - loss: 0.6164 - accuracy: 0.6668 - precision_11: 0.6571 - recall_11: 0.5310 - auc: 0.7094 - specificity: 0.7773 - val_loss: 0.5972 - val_accuracy: 0.6815 - val_precision_11: 0.6850 - val_recall_11: 0.5479 - val_auc: 0.7356 - val_specificity: 0.7909\n",
      "Epoch 4/50\n",
      "604/604 [==============================] - 302s 500ms/step - loss: 0.6061 - accuracy: 0.6774 - precision_11: 0.6746 - recall_11: 0.5365 - auc: 0.7224 - specificity: 0.7924 - val_loss: 0.6128 - val_accuracy: 0.6693 - val_precision_11: 0.6826 - val_recall_11: 0.5025 - val_auc: 0.7160 - val_specificity: 0.8054\n",
      "Epoch 5/50\n",
      "604/604 [==============================] - 272s 450ms/step - loss: 0.5972 - accuracy: 0.6865 - precision_11: 0.6873 - recall_11: 0.5468 - auc: 0.7333 - specificity: 0.7982 - val_loss: 0.5891 - val_accuracy: 0.6792 - val_precision_11: 0.7357 - val_recall_11: 0.4540 - val_auc: 0.7512 - val_specificity: 0.8647\n",
      "Epoch 6/50\n",
      "604/604 [==============================] - 234s 387ms/step - loss: 0.5867 - accuracy: 0.6919 - precision_11: 0.6919 - recall_11: 0.5593 - auc: 0.7444 - specificity: 0.7978 - val_loss: 0.5830 - val_accuracy: 0.6830 - val_precision_11: 0.7428 - val_recall_11: 0.4576 - val_auc: 0.7600 - val_specificity: 0.8678\n",
      "Epoch 7/50\n",
      "604/604 [==============================] - 247s 409ms/step - loss: 0.5771 - accuracy: 0.7009 - precision_11: 0.7061 - recall_11: 0.5657 - auc: 0.7571 - specificity: 0.8106 - val_loss: 0.5656 - val_accuracy: 0.7070 - val_precision_11: 0.7531 - val_recall_11: 0.5240 - val_auc: 0.7742 - val_specificity: 0.8576\n",
      "Epoch 8/50\n",
      "604/604 [==============================] - 235s 389ms/step - loss: 0.5630 - accuracy: 0.7132 - precision_11: 0.7197 - recall_11: 0.5864 - auc: 0.7721 - specificity: 0.8165 - val_loss: 0.5748 - val_accuracy: 0.6931 - val_precision_11: 0.7627 - val_recall_11: 0.4668 - val_auc: 0.7702 - val_specificity: 0.8814\n",
      "Epoch 9/50\n",
      "604/604 [==============================] - 298s 493ms/step - loss: 0.5470 - accuracy: 0.7227 - precision_11: 0.7274 - recall_11: 0.6064 - auc: 0.7873 - specificity: 0.8156 - val_loss: 0.5350 - val_accuracy: 0.7317 - val_precision_11: 0.7694 - val_recall_11: 0.5809 - val_auc: 0.8011 - val_specificity: 0.8567\n",
      "Epoch 10/50\n",
      "604/604 [==============================] - 349s 577ms/step - loss: 0.5319 - accuracy: 0.7369 - precision_11: 0.7473 - recall_11: 0.6207 - auc: 0.7990 - specificity: 0.8305 - val_loss: 0.5187 - val_accuracy: 0.7375 - val_precision_11: 0.7993 - val_recall_11: 0.5602 - val_auc: 0.8162 - val_specificity: 0.8839\n",
      "Epoch 11/50\n",
      "604/604 [==============================] - 239s 396ms/step - loss: 0.5082 - accuracy: 0.7509 - precision_11: 0.7707 - recall_11: 0.6295 - auc: 0.8160 - specificity: 0.8501 - val_loss: 0.5147 - val_accuracy: 0.7455 - val_precision_11: 0.7817 - val_recall_11: 0.6070 - val_auc: 0.8136 - val_specificity: 0.8604\n",
      "Epoch 12/50\n",
      "604/604 [==============================] - 214s 355ms/step - loss: 0.4835 - accuracy: 0.7675 - precision_11: 0.7924 - recall_11: 0.6498 - auc: 0.8360 - specificity: 0.8622 - val_loss: 0.4913 - val_accuracy: 0.7586 - val_precision_11: 0.7831 - val_recall_11: 0.6450 - val_auc: 0.8352 - val_specificity: 0.8530\n",
      "Epoch 13/50\n",
      "604/604 [==============================] - 208s 344ms/step - loss: 0.4640 - accuracy: 0.7783 - precision_11: 0.7975 - recall_11: 0.6750 - auc: 0.8503 - specificity: 0.8621 - val_loss: 0.4822 - val_accuracy: 0.7545 - val_precision_11: 0.7248 - val_recall_11: 0.7371 - val_auc: 0.8449 - val_specificity: 0.7680\n",
      "Epoch 14/50\n",
      "604/604 [==============================] - 218s 360ms/step - loss: 0.4458 - accuracy: 0.7892 - precision_11: 0.8168 - recall_11: 0.6809 - auc: 0.8607 - specificity: 0.8760 - val_loss: 0.4705 - val_accuracy: 0.7712 - val_precision_11: 0.7880 - val_recall_11: 0.6761 - val_auc: 0.8459 - val_specificity: 0.8493\n",
      "Epoch 15/50\n",
      "604/604 [==============================] - 195s 322ms/step - loss: 0.4340 - accuracy: 0.7976 - precision_11: 0.8239 - recall_11: 0.6956 - auc: 0.8688 - specificity: 0.8801 - val_loss: 0.5093 - val_accuracy: 0.7561 - val_precision_11: 0.8776 - val_recall_11: 0.5355 - val_auc: 0.8413 - val_specificity: 0.9378\n",
      "Epoch 16/50\n",
      "604/604 [==============================] - 207s 343ms/step - loss: 0.4122 - accuracy: 0.8098 - precision_11: 0.8350 - recall_11: 0.7155 - auc: 0.8832 - specificity: 0.8860 - val_loss: 0.4664 - val_accuracy: 0.7793 - val_precision_11: 0.7970 - val_recall_11: 0.6871 - val_auc: 0.8574 - val_specificity: 0.8562\n",
      "Epoch 17/50\n",
      "604/604 [==============================] - 242s 401ms/step - loss: 0.3986 - accuracy: 0.8188 - precision_11: 0.8448 - recall_11: 0.7280 - auc: 0.8910 - specificity: 0.8924 - val_loss: 0.4756 - val_accuracy: 0.7731 - val_precision_11: 0.8178 - val_recall_11: 0.6413 - val_auc: 0.8513 - val_specificity: 0.8822\n",
      "Epoch 18/50\n",
      "604/604 [==============================] - 225s 373ms/step - loss: 0.3888 - accuracy: 0.8204 - precision_11: 0.8422 - recall_11: 0.7356 - auc: 0.8967 - specificity: 0.8882 - val_loss: 0.4814 - val_accuracy: 0.7704 - val_precision_11: 0.7733 - val_recall_11: 0.6967 - val_auc: 0.8469 - val_specificity: 0.8323\n",
      "Epoch 19/50\n",
      "604/604 [==============================] - 207s 343ms/step - loss: 0.3713 - accuracy: 0.8322 - precision_11: 0.8515 - recall_11: 0.7562 - auc: 0.9067 - specificity: 0.8934 - val_loss: 0.4766 - val_accuracy: 0.7717 - val_precision_11: 0.8062 - val_recall_11: 0.6519 - val_auc: 0.8490 - val_specificity: 0.8720\n",
      "Epoch 20/50\n",
      "604/604 [==============================] - 240s 397ms/step - loss: 0.3569 - accuracy: 0.8386 - precision_11: 0.8564 - recall_11: 0.7672 - auc: 0.9143 - specificity: 0.8958 - val_loss: 0.4790 - val_accuracy: 0.7816 - val_precision_11: 0.8239 - val_recall_11: 0.6578 - val_auc: 0.8512 - val_specificity: 0.8829\n",
      "Epoch 21/50\n",
      "604/604 [==============================] - 231s 382ms/step - loss: 0.3387 - accuracy: 0.8491 - precision_11: 0.8690 - recall_11: 0.7796 - auc: 0.9233 - specificity: 0.9061 - val_loss: 0.4961 - val_accuracy: 0.7700 - val_precision_11: 0.7568 - val_recall_11: 0.7242 - val_auc: 0.8511 - val_specificity: 0.8093\n",
      "Epoch 22/50\n",
      "604/604 [==============================] - 219s 363ms/step - loss: 0.3294 - accuracy: 0.8574 - precision_11: 0.8716 - recall_11: 0.7983 - auc: 0.9276 - specificity: 0.9052 - val_loss: 0.4871 - val_accuracy: 0.7770 - val_precision_11: 0.7949 - val_recall_11: 0.6835 - val_auc: 0.8563 - val_specificity: 0.8554\n",
      "Epoch 23/50\n",
      "604/604 [==============================] - 202s 334ms/step - loss: 0.3134 - accuracy: 0.8631 - precision_11: 0.8793 - recall_11: 0.8037 - auc: 0.9349 - specificity: 0.9119 - val_loss: 0.5002 - val_accuracy: 0.7739 - val_precision_11: 0.7658 - val_recall_11: 0.7206 - val_auc: 0.8537 - val_specificity: 0.8182\n",
      "Epoch 24/50\n",
      "604/604 [==============================] - 197s 327ms/step - loss: 0.2964 - accuracy: 0.8724 - precision_11: 0.8849 - recall_11: 0.8211 - auc: 0.9416 - specificity: 0.9137 - val_loss: 0.4907 - val_accuracy: 0.7795 - val_precision_11: 0.7774 - val_recall_11: 0.7183 - val_auc: 0.8548 - val_specificity: 0.8314\n",
      "Epoch 25/50\n",
      "604/604 [==============================] - 212s 351ms/step - loss: 0.2839 - accuracy: 0.8781 - precision_11: 0.8888 - recall_11: 0.8311 - auc: 0.9470 - specificity: 0.9159 - val_loss: 0.5187 - val_accuracy: 0.7772 - val_precision_11: 0.7988 - val_recall_11: 0.6784 - val_auc: 0.8516 - val_specificity: 0.8596\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 203s 336ms/step - loss: 0.2771 - accuracy: 0.8783 - precision_11: 0.8891 - recall_11: 0.8311 - auc: 0.9499 - specificity: 0.9162 - val_loss: 0.5370 - val_accuracy: 0.7708 - val_precision_11: 0.7746 - val_recall_11: 0.6958 - val_auc: 0.8507 - val_specificity: 0.8337\n",
      "Epoch 26: early stopping\n",
      "151/151 [==============================] - 22s 129ms/step\n",
      "Loss:  0.5370438098907471\n",
      "Accuracy:  0.770824670791626\n",
      "Precision:  0.7746047973632812\n",
      "Recall:  0.6958314180374146\n",
      "AUC:  0.8506723642349243\n",
      "Specificity:  0.8337308168411255\n",
      "Confusion Matrix:  [[2201  442]\n",
      " [ 664 1519]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import keras.backend as K\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1-y_true)*(1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true)*y_pred, 0, 1)))\n",
    "    specificity = tn / (tn + fp + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(GRU(64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC(name='auc'), specificity])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "loss, accuracy, precision, recall, auc, specificity = model.evaluate(X_test, y_test, verbose=0)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print('Loss: ', loss)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('AUC: ', auc)\n",
    "print('Specificity: ', specificity)\n",
    "print('Confusion Matrix: ', conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "875690ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1...\n",
      "Epoch 1/5\n",
      "503/503 [==============================] - 167s 310ms/step - loss: 0.6868 - accuracy: 0.5519\n",
      "Epoch 2/5\n",
      "503/503 [==============================] - 170s 337ms/step - loss: 0.6611 - accuracy: 0.6015\n",
      "Epoch 3/5\n",
      "503/503 [==============================] - 278s 553ms/step - loss: 0.6298 - accuracy: 0.6522\n",
      "Epoch 4/5\n",
      "503/503 [==============================] - 282s 561ms/step - loss: 0.6168 - accuracy: 0.6629\n",
      "Epoch 5/5\n",
      "503/503 [==============================] - 280s 557ms/step - loss: 0.6093 - accuracy: 0.6717\n",
      "252/252 [==============================] - 60s 220ms/step\n",
      "Fold 2...\n",
      "Epoch 1/5\n",
      "503/503 [==============================] - 322s 576ms/step - loss: 0.6868 - accuracy: 0.5531\n",
      "Epoch 2/5\n",
      "503/503 [==============================] - 292s 580ms/step - loss: 0.6659 - accuracy: 0.5952\n",
      "Epoch 3/5\n",
      "503/503 [==============================] - 163s 323ms/step - loss: 0.6347 - accuracy: 0.6464\n",
      "Epoch 4/5\n",
      "503/503 [==============================] - 168s 334ms/step - loss: 0.6257 - accuracy: 0.6558\n",
      "Epoch 5/5\n",
      "503/503 [==============================] - 165s 328ms/step - loss: 0.6425 - accuracy: 0.6274\n",
      "252/252 [==============================] - 46s 168ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHANMUKH\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3...\n",
      "Epoch 1/5\n",
      "503/503 [==============================] - 177s 319ms/step - loss: 0.6824 - accuracy: 0.5645\n",
      "Epoch 2/5\n",
      "503/503 [==============================] - 184s 365ms/step - loss: 0.6467 - accuracy: 0.6347\n",
      "Epoch 3/5\n",
      "503/503 [==============================] - 191s 379ms/step - loss: 0.6301 - accuracy: 0.6516\n",
      "Epoch 4/5\n",
      "503/503 [==============================] - 177s 352ms/step - loss: 0.6176 - accuracy: 0.6622\n",
      "Epoch 5/5\n",
      "503/503 [==============================] - 193s 384ms/step - loss: 0.6087 - accuracy: 0.6742\n",
      "252/252 [==============================] - 38s 132ms/step\n",
      "Accuracy: 63.868% +/- 6.133%\n",
      "Precision: 48.203% +/- 34.087%\n",
      "Recall: 31.287% +/- 22.147%\n",
      "F1 Score: 37.942% +/- 26.843%\n",
      "ROC AUC: 69.525% +/- 5.644%\n",
      "ROC AUC: 85.380% +/- 0.000%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Dropout, Bidirectional\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Define evaluation metrics\n",
    "def evaluate(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    spec = recall_score(y_true, y_pred, pos_label=0)\n",
    "    return acc, prec, rec, f1, roc_auc, spec\n",
    "\n",
    "# Define GRU model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(256, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define number of folds and batch size\n",
    "n_folds = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "f1s = []\n",
    "roc_aucs = []\n",
    "specs = []\n",
    "\n",
    "# Create K-Fold cross-validation object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "# Perform cross-validation\n",
    "for i, (train, test) in enumerate(kf.split(X)):\n",
    "    print('Fold %d...' % (i+1))\n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    \n",
    "    # Create and fit model\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=5, verbose=1)\n",
    "    \n",
    "    # Evaluate model on test set and calculate metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred_binary)\n",
    "    prec = precision_score(y_test, y_pred_binary)\n",
    "    rec = recall_score(y_test, y_pred_binary)\n",
    "    f1 = f1_score(y_test, y_pred_binary)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n",
    "    spec = tn / (tn + fp)  \n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accs.append(acc)\n",
    "    precs.append(prec)\n",
    "    recs.append(rec)\n",
    "    f1s.append(f1)\n",
    "    roc_aucs.append(roc_auc)\n",
    "    specs.append(spec)\n",
    "\n",
    "# Calculate average and standard deviation of metrics across folds\n",
    "print('Accuracy: %.3f%% +/- %.3f%%' % (np.mean(accs)*100, np.std(accs)*100))\n",
    "print('Precision: %.3f%% +/- %.3f%%' % (np.mean(precs)*100, np.std(precs)*100))\n",
    "print('Recall: %.3f%% +/- %.3f%%' % (np.mean(recs)*100, np.std(recs)*100))\n",
    "print('F1 Score: %.3f%% +/- %.3f%%' % (np.mean(f1s)*100, np.std(f1s)*100))\n",
    "print('ROC AUC: %.3f%% +/- %.3f%%' % (np.mean(roc_aucs)*100, np.std(roc_aucs)*100))\n",
    "print('ROC AUC: %.3f%% +/- %.3f%%' % (np.mean(spec)*100, np.std(spec)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876f864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
