{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fe420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('updated_coswara.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2494b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc{1}</th>\n",
       "      <th>mfcc{2}</th>\n",
       "      <th>mfcc{3}</th>\n",
       "      <th>mfcc{4}</th>\n",
       "      <th>...</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>ddpJitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>aqpq5Shimmer</th>\n",
       "      <th>apq11Shimmer</th>\n",
       "      <th>ddaShimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039102</td>\n",
       "      <td>0.378903</td>\n",
       "      <td>786.823461</td>\n",
       "      <td>966.699650</td>\n",
       "      <td>1387.984940</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>-412.08945</td>\n",
       "      <td>126.752335</td>\n",
       "      <td>31.558170</td>\n",
       "      <td>18.483738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.212818</td>\n",
       "      <td>1.734878</td>\n",
       "      <td>0.097995</td>\n",
       "      <td>0.147546</td>\n",
       "      <td>0.206467</td>\n",
       "      <td>0.293984</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051812</td>\n",
       "      <td>0.436672</td>\n",
       "      <td>2219.820298</td>\n",
       "      <td>1874.652272</td>\n",
       "      <td>4134.754998</td>\n",
       "      <td>0.209401</td>\n",
       "      <td>-398.93295</td>\n",
       "      <td>50.929253</td>\n",
       "      <td>-17.480385</td>\n",
       "      <td>4.325164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.092139</td>\n",
       "      <td>0.283908</td>\n",
       "      <td>2.113383</td>\n",
       "      <td>0.137225</td>\n",
       "      <td>0.241707</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.411675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.238371</td>\n",
       "      <td>2002.598308</td>\n",
       "      <td>2058.223470</td>\n",
       "      <td>4324.443295</td>\n",
       "      <td>0.146994</td>\n",
       "      <td>-599.82200</td>\n",
       "      <td>40.048190</td>\n",
       "      <td>6.373952</td>\n",
       "      <td>13.369130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.022602</td>\n",
       "      <td>0.056016</td>\n",
       "      <td>0.134015</td>\n",
       "      <td>1.358899</td>\n",
       "      <td>0.061172</td>\n",
       "      <td>0.091838</td>\n",
       "      <td>0.164672</td>\n",
       "      <td>0.183516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.078437</td>\n",
       "      <td>0.242433</td>\n",
       "      <td>569.328347</td>\n",
       "      <td>506.956042</td>\n",
       "      <td>979.591497</td>\n",
       "      <td>0.035271</td>\n",
       "      <td>-423.74878</td>\n",
       "      <td>77.112580</td>\n",
       "      <td>-11.768955</td>\n",
       "      <td>6.080464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050445</td>\n",
       "      <td>0.070875</td>\n",
       "      <td>0.151334</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>1.673087</td>\n",
       "      <td>0.101366</td>\n",
       "      <td>0.093390</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.304098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072712</td>\n",
       "      <td>0.327475</td>\n",
       "      <td>1344.446613</td>\n",
       "      <td>1062.582139</td>\n",
       "      <td>2431.292693</td>\n",
       "      <td>0.089481</td>\n",
       "      <td>-327.21740</td>\n",
       "      <td>160.910540</td>\n",
       "      <td>-60.493977</td>\n",
       "      <td>-20.299130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.003605</td>\n",
       "      <td>0.085836</td>\n",
       "      <td>0.803849</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>0.054323</td>\n",
       "      <td>0.093975</td>\n",
       "      <td>0.107976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24123</th>\n",
       "      <td>0.056854</td>\n",
       "      <td>0.467262</td>\n",
       "      <td>2845.574993</td>\n",
       "      <td>2044.704050</td>\n",
       "      <td>5269.369989</td>\n",
       "      <td>0.235494</td>\n",
       "      <td>-388.71793</td>\n",
       "      <td>32.990032</td>\n",
       "      <td>-8.879510</td>\n",
       "      <td>20.782476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023928</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>0.071784</td>\n",
       "      <td>0.160208</td>\n",
       "      <td>1.659819</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>0.137543</td>\n",
       "      <td>0.331425</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24124</th>\n",
       "      <td>0.050149</td>\n",
       "      <td>0.267105</td>\n",
       "      <td>1199.737564</td>\n",
       "      <td>1636.413852</td>\n",
       "      <td>2392.060470</td>\n",
       "      <td>0.059086</td>\n",
       "      <td>-433.10117</td>\n",
       "      <td>58.741127</td>\n",
       "      <td>37.350792</td>\n",
       "      <td>51.354850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.419731</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.041864</td>\n",
       "      <td>0.054251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24125</th>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.525523</td>\n",
       "      <td>4996.270042</td>\n",
       "      <td>2579.510698</td>\n",
       "      <td>8270.021928</td>\n",
       "      <td>0.418827</td>\n",
       "      <td>-745.49110</td>\n",
       "      <td>-27.318123</td>\n",
       "      <td>-5.540486</td>\n",
       "      <td>16.303144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.050605</td>\n",
       "      <td>0.137923</td>\n",
       "      <td>1.255627</td>\n",
       "      <td>0.061327</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.183981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24126</th>\n",
       "      <td>0.031053</td>\n",
       "      <td>0.167581</td>\n",
       "      <td>1890.217497</td>\n",
       "      <td>2748.644250</td>\n",
       "      <td>5175.546000</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>-499.73740</td>\n",
       "      <td>32.764600</td>\n",
       "      <td>46.792103</td>\n",
       "      <td>40.457275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.016609</td>\n",
       "      <td>0.134111</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.023441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24127</th>\n",
       "      <td>0.024217</td>\n",
       "      <td>0.287329</td>\n",
       "      <td>1296.724388</td>\n",
       "      <td>1259.118567</td>\n",
       "      <td>2319.688878</td>\n",
       "      <td>0.079520</td>\n",
       "      <td>-457.49475</td>\n",
       "      <td>98.845490</td>\n",
       "      <td>-1.881587</td>\n",
       "      <td>40.956110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>0.013602</td>\n",
       "      <td>0.037466</td>\n",
       "      <td>0.106427</td>\n",
       "      <td>1.033221</td>\n",
       "      <td>0.047599</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>0.095882</td>\n",
       "      <td>0.142798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24128 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse  chroma_stft    spec_cent      spec_bw      rolloff       zcr  \\\n",
       "0      0.039102     0.378903   786.823461   966.699650  1387.984940  0.043870   \n",
       "1      0.051812     0.436672  2219.820298  1874.652272  4134.754998  0.209401   \n",
       "2      0.010413     0.238371  2002.598308  2058.223470  4324.443295  0.146994   \n",
       "3      0.078437     0.242433   569.328347   506.956042   979.591497  0.035271   \n",
       "4      0.072712     0.327475  1344.446613  1062.582139  2431.292693  0.089481   \n",
       "...         ...          ...          ...          ...          ...       ...   \n",
       "24123  0.056854     0.467262  2845.574993  2044.704050  5269.369989  0.235494   \n",
       "24124  0.050149     0.267105  1199.737564  1636.413852  2392.060470  0.059086   \n",
       "24125  0.000454     0.525523  4996.270042  2579.510698  8270.021928  0.418827   \n",
       "24126  0.031053     0.167581  1890.217497  2748.644250  5175.546000  0.033493   \n",
       "24127  0.024217     0.287329  1296.724388  1259.118567  2319.688878  0.079520   \n",
       "\n",
       "         mfcc{1}     mfcc{2}    mfcc{3}    mfcc{4}  ...  rapJitter  \\\n",
       "0     -412.08945  126.752335  31.558170  18.483738  ...   0.018672   \n",
       "1     -398.93295   50.929253 -17.480385   4.325164  ...   0.030713   \n",
       "2     -599.82200   40.048190   6.373952  13.369130  ...   0.018672   \n",
       "3     -423.74878   77.112580 -11.768955   6.080464  ...   0.050445   \n",
       "4     -327.21740  160.910540 -60.493977 -20.299130  ...   0.001202   \n",
       "...          ...         ...        ...        ...  ...        ...   \n",
       "24123 -388.71793   32.990032  -8.879510  20.782476  ...   0.023928   \n",
       "24124 -433.10117   58.741127  37.350792  51.354850  ...   0.002124   \n",
       "24125 -745.49110  -27.318123  -5.540486  16.303144  ...   0.016868   \n",
       "24126 -499.73740   32.764600  46.792103  40.457275  ...   0.001794   \n",
       "24127 -457.49475   98.845490  -1.881587  40.956110  ...   0.012489   \n",
       "\n",
       "       ppq5Jitter  ddpJitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "0        0.021894   0.056015      0.212818        1.734878     0.097995   \n",
       "1        0.037553   0.092139      0.283908        2.113383     0.137225   \n",
       "2        0.022602   0.056016      0.134015        1.358899     0.061172   \n",
       "3        0.070875   0.151334      0.181641        1.673087     0.101366   \n",
       "4        0.001537   0.003605      0.085836        0.803849     0.035992   \n",
       "...           ...        ...           ...             ...          ...   \n",
       "24123    0.029485   0.071784      0.160208        1.659819     0.064901   \n",
       "24124    0.002396   0.006373      0.042722        0.419731     0.018084   \n",
       "24125    0.019114   0.050605      0.137923        1.255627     0.061327   \n",
       "24126    0.001854   0.005381      0.016609        0.134111     0.007814   \n",
       "24127    0.013602   0.037466      0.106427        1.033221     0.047599   \n",
       "\n",
       "       aqpq5Shimmer  apq11Shimmer  ddaShimmer  label  \n",
       "0          0.147546      0.206467    0.293984      1  \n",
       "1          0.241707      0.116884    0.411675      0  \n",
       "2          0.091838      0.164672    0.183516      1  \n",
       "3          0.093390      0.116884    0.304098      0  \n",
       "4          0.054323      0.093975    0.107976      1  \n",
       "...             ...           ...         ...    ...  \n",
       "24123      0.137543      0.331425    0.194703      0  \n",
       "24124      0.027000      0.041864    0.054251      0  \n",
       "24125      0.081247      0.116884    0.183981      0  \n",
       "24126      0.009204      0.011837    0.023441      1  \n",
       "24127      0.060751      0.095882    0.142798      0  \n",
       "\n",
       "[24128 rows x 80 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17640099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69fc5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout,BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Separate file names and labels from features\n",
    "filenames = df.iloc[:, 0]\n",
    "labels = df.iloc[:, -1]\n",
    "features = df.iloc[:, 1:-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6037d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be1e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "sequence_length = 10\n",
    "n_features = features.shape[1]\n",
    "sequences = []\n",
    "targets = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8ae5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(sequence_length, len(df)-1):\n",
    "    sequences.append(features[i-sequence_length:i])\n",
    "    targets.append(labels[i-1])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e4d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "y = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81573010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice filenames to match the number of samples in X and y\n",
    "filenames = filenames[sequence_length:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80bfd512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test, filenames_train, filenames_test = train_test_split(X, y, filenames, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5d1531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1206/1206 [==============================] - 24s 12ms/step - loss: 4.3785 - accuracy: 0.6059 - val_loss: 3.7703 - val_accuracy: 0.6482\n",
      "Epoch 2/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 4.0964 - accuracy: 0.6307 - val_loss: 4.0847 - val_accuracy: 0.6374\n",
      "Epoch 3/50\n",
      "1206/1206 [==============================] - 13s 11ms/step - loss: 4.0462 - accuracy: 0.6433 - val_loss: 3.9460 - val_accuracy: 0.6590\n",
      "Epoch 4/50\n",
      "1206/1206 [==============================] - 13s 11ms/step - loss: 4.7012 - accuracy: 0.6162 - val_loss: 4.6903 - val_accuracy: 0.6097\n",
      "Epoch 5/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 4.6003 - accuracy: 0.6123 - val_loss: 4.2072 - val_accuracy: 0.6370\n",
      "Epoch 6/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 4.0359 - accuracy: 0.6443 - val_loss: 3.5769 - val_accuracy: 0.6702\n",
      "Epoch 7/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 3.9057 - accuracy: 0.6460 - val_loss: 3.9549 - val_accuracy: 0.6499\n",
      "Epoch 8/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 3.8834 - accuracy: 0.6404 - val_loss: 3.4356 - val_accuracy: 0.6663\n",
      "Epoch 9/50\n",
      "1206/1206 [==============================] - 11s 10ms/step - loss: 3.5532 - accuracy: 0.6534 - val_loss: 3.1912 - val_accuracy: 0.6737\n",
      "Epoch 10/50\n",
      "1206/1206 [==============================] - 10s 9ms/step - loss: 3.2044 - accuracy: 0.6672 - val_loss: 2.6963 - val_accuracy: 0.6953\n",
      "Epoch 11/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 2.8732 - accuracy: 0.6737 - val_loss: 2.2850 - val_accuracy: 0.7112\n",
      "Epoch 12/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 2.4000 - accuracy: 0.6792 - val_loss: 1.6766 - val_accuracy: 0.7071\n",
      "Epoch 13/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 2.4080 - accuracy: 0.6804 - val_loss: 2.2514 - val_accuracy: 0.6853\n",
      "Epoch 14/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 2.5253 - accuracy: 0.6648 - val_loss: 1.7750 - val_accuracy: 0.6980\n",
      "Epoch 15/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 1.7485 - accuracy: 0.6784 - val_loss: 1.1279 - val_accuracy: 0.7185\n",
      "Epoch 16/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 1.3444 - accuracy: 0.6913 - val_loss: 0.8836 - val_accuracy: 0.7260\n",
      "Epoch 17/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 1.2507 - accuracy: 0.6793 - val_loss: 0.8427 - val_accuracy: 0.6994\n",
      "Epoch 18/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 0.9033 - accuracy: 0.6724 - val_loss: 0.6544 - val_accuracy: 0.7193\n",
      "Epoch 19/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 0.7056 - accuracy: 0.6992 - val_loss: 0.5861 - val_accuracy: 0.7216\n",
      "Epoch 20/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 0.6100 - accuracy: 0.7158 - val_loss: 0.5257 - val_accuracy: 0.7579\n",
      "Epoch 21/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 0.5708 - accuracy: 0.7318 - val_loss: 0.5054 - val_accuracy: 0.7674\n",
      "Epoch 22/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 0.5418 - accuracy: 0.7480 - val_loss: 0.4787 - val_accuracy: 0.7772\n",
      "Epoch 23/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 0.5212 - accuracy: 0.7640 - val_loss: 0.4895 - val_accuracy: 0.7848\n",
      "Epoch 24/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 0.5106 - accuracy: 0.7686 - val_loss: 0.4776 - val_accuracy: 0.7865\n",
      "Epoch 25/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 0.4975 - accuracy: 0.7770 - val_loss: 0.4729 - val_accuracy: 0.7838\n",
      "Epoch 26/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 0.5052 - accuracy: 0.7701 - val_loss: 0.4654 - val_accuracy: 0.7815\n",
      "Epoch 27/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 0.4921 - accuracy: 0.7820 - val_loss: 0.4697 - val_accuracy: 0.7908\n",
      "Epoch 28/50\n",
      "1206/1206 [==============================] - 13s 11ms/step - loss: 0.4864 - accuracy: 0.7818 - val_loss: 0.4676 - val_accuracy: 0.7977\n",
      "Epoch 29/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 0.4627 - accuracy: 0.7933 - val_loss: 0.4773 - val_accuracy: 0.7985\n",
      "Epoch 30/50\n",
      "1206/1206 [==============================] - 13s 11ms/step - loss: 0.4595 - accuracy: 0.7889 - val_loss: 0.4575 - val_accuracy: 0.7937\n",
      "Epoch 31/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 0.4858 - accuracy: 0.7717 - val_loss: 0.4911 - val_accuracy: 0.7662\n",
      "Epoch 32/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 0.4684 - accuracy: 0.7873 - val_loss: 0.4726 - val_accuracy: 0.7902\n",
      "Epoch 33/50\n",
      "1206/1206 [==============================] - 14s 12ms/step - loss: 0.4550 - accuracy: 0.7929 - val_loss: 0.4698 - val_accuracy: 0.7937\n",
      "Epoch 34/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 0.4486 - accuracy: 0.8002 - val_loss: 0.4948 - val_accuracy: 0.7962\n",
      "Epoch 35/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 0.4543 - accuracy: 0.8032 - val_loss: 0.4904 - val_accuracy: 0.7991\n",
      "Epoch 36/50\n",
      "1206/1206 [==============================] - 13s 11ms/step - loss: 0.4549 - accuracy: 0.7975 - val_loss: 0.4736 - val_accuracy: 0.7998\n",
      "Epoch 37/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 0.4548 - accuracy: 0.8009 - val_loss: 0.4646 - val_accuracy: 0.8029\n",
      "Epoch 38/50\n",
      "1206/1206 [==============================] - 12s 10ms/step - loss: 0.4468 - accuracy: 0.8063 - val_loss: 0.4883 - val_accuracy: 0.8027\n",
      "Epoch 39/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 0.4569 - accuracy: 0.8005 - val_loss: 0.4864 - val_accuracy: 0.7836\n",
      "Epoch 40/50\n",
      "1206/1206 [==============================] - 10s 9ms/step - loss: 0.4383 - accuracy: 0.8067 - val_loss: 0.4713 - val_accuracy: 0.8014\n",
      "Epoch 41/50\n",
      "1206/1206 [==============================] - 10s 9ms/step - loss: 0.4331 - accuracy: 0.8113 - val_loss: 0.4957 - val_accuracy: 0.7892\n",
      "Epoch 42/50\n",
      "1206/1206 [==============================] - 8s 7ms/step - loss: 0.4277 - accuracy: 0.8085 - val_loss: 0.4833 - val_accuracy: 0.8020\n",
      "Epoch 43/50\n",
      "1206/1206 [==============================] - 11s 9ms/step - loss: 0.4228 - accuracy: 0.8161 - val_loss: 0.4980 - val_accuracy: 0.8053\n",
      "Epoch 44/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 0.4283 - accuracy: 0.8176 - val_loss: 0.5185 - val_accuracy: 0.8043\n",
      "Epoch 45/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 0.4328 - accuracy: 0.8189 - val_loss: 0.5098 - val_accuracy: 0.7971\n",
      "Epoch 46/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 0.4232 - accuracy: 0.8243 - val_loss: 0.5219 - val_accuracy: 0.7958\n",
      "Epoch 47/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 0.4654 - accuracy: 0.8006 - val_loss: 0.5105 - val_accuracy: 0.7649\n",
      "Epoch 48/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 0.4273 - accuracy: 0.8061 - val_loss: 0.5343 - val_accuracy: 0.7861\n",
      "Epoch 49/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 0.4178 - accuracy: 0.8162 - val_loss: 0.5395 - val_accuracy: 0.7977\n",
      "Epoch 50/50\n",
      "1206/1206 [==============================] - 10s 8ms/step - loss: 0.4603 - accuracy: 0.8026 - val_loss: 0.5346 - val_accuracy: 0.7743\n",
      "151/151 [==============================] - 1s 3ms/step - loss: 0.5346 - accuracy: 0.7743\n",
      "Test accuracy: 0.7742537260055542\n",
      "151/151 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(sequence_length, n_features)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Save predictions and corresponding file names to a CSV file\n",
    "results = pd.DataFrame({'filename': filenames_test, 'label': y_test, 'prediction': predictions.flatten()})\n",
    "results.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03cda1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 0s 3ms/step\n",
      "Test loss: 0.5346227884292603\n",
      "Test accuracy: 0.7742537260055542\n",
      "Test precision: 0.753411306042885\n",
      "Test recall: 0.7261625176139033\n",
      "Test F1 score: 0.7395359961731642\n",
      "Test ROC AUC: 0.7692037077865435\n",
      "Test specificity: 0.8122448979591836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "# Evaluate the model on test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test precision: {precision}\")\n",
    "print(f\"Test recall: {recall}\")\n",
    "print(f\"Test F1 score: {f1}\")\n",
    "print(f\"Test ROC AUC: {auc}\")\n",
    "print(f\"Test specificity: {specificity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0a7471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('coswara_LSTM.h5')\n",
    "\n",
    "# import pickle\n",
    "# import joblib\n",
    "# with open('coswara_LSTM.h5', 'wb') as f:\n",
    "#     # Use pickle to dump the dictionary into the file\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58befe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "603/603 [==============================] - 23s 28ms/step - loss: 0.6345 - accuracy: 0.6188 - val_loss: 0.4923 - val_accuracy: 0.7655\n",
      "Epoch 2/50\n",
      "603/603 [==============================] - 14s 23ms/step - loss: 0.4658 - accuracy: 0.7836 - val_loss: 0.4374 - val_accuracy: 0.8043\n",
      "Epoch 3/50\n",
      "603/603 [==============================] - 14s 23ms/step - loss: 0.4286 - accuracy: 0.8050 - val_loss: 0.4320 - val_accuracy: 0.8031\n",
      "Epoch 4/50\n",
      "603/603 [==============================] - 14s 24ms/step - loss: 0.4005 - accuracy: 0.8223 - val_loss: 0.4281 - val_accuracy: 0.8033\n",
      "Epoch 5/50\n",
      "603/603 [==============================] - 14s 23ms/step - loss: 0.3807 - accuracy: 0.8350 - val_loss: 0.4241 - val_accuracy: 0.8103\n",
      "Epoch 6/50\n",
      "603/603 [==============================] - 12s 20ms/step - loss: 0.3566 - accuracy: 0.8463 - val_loss: 0.4278 - val_accuracy: 0.8066\n",
      "Epoch 7/50\n",
      "603/603 [==============================] - 13s 21ms/step - loss: 0.3390 - accuracy: 0.8549 - val_loss: 0.4325 - val_accuracy: 0.8066\n",
      "Epoch 8/50\n",
      "603/603 [==============================] - 12s 20ms/step - loss: 0.3154 - accuracy: 0.8697 - val_loss: 0.4670 - val_accuracy: 0.7989\n",
      "Epoch 9/50\n",
      "603/603 [==============================] - 13s 22ms/step - loss: 0.2985 - accuracy: 0.8797 - val_loss: 0.4575 - val_accuracy: 0.7987\n",
      "Epoch 10/50\n",
      "603/603 [==============================] - 13s 21ms/step - loss: 0.2782 - accuracy: 0.8886 - val_loss: 0.4903 - val_accuracy: 0.7919\n",
      "Epoch 11/50\n",
      "603/603 [==============================] - 11s 19ms/step - loss: 0.2617 - accuracy: 0.8966 - val_loss: 0.4997 - val_accuracy: 0.7993\n",
      "Epoch 12/50\n",
      "603/603 [==============================] - 11s 18ms/step - loss: 0.2435 - accuracy: 0.9039 - val_loss: 0.5371 - val_accuracy: 0.7939\n",
      "Epoch 13/50\n",
      "603/603 [==============================] - 13s 21ms/step - loss: 0.2300 - accuracy: 0.9127 - val_loss: 0.5546 - val_accuracy: 0.7915\n",
      "Epoch 14/50\n",
      "603/603 [==============================] - 14s 23ms/step - loss: 0.2143 - accuracy: 0.9187 - val_loss: 0.5891 - val_accuracy: 0.7809\n",
      "Epoch 15/50\n",
      "603/603 [==============================] - 14s 23ms/step - loss: 0.2017 - accuracy: 0.9235 - val_loss: 0.6028 - val_accuracy: 0.7915\n",
      "Epoch 16/50\n",
      "603/603 [==============================] - 12s 20ms/step - loss: 0.1852 - accuracy: 0.9289 - val_loss: 0.6209 - val_accuracy: 0.7859\n",
      "Epoch 17/50\n",
      "603/603 [==============================] - 13s 21ms/step - loss: 0.1769 - accuracy: 0.9330 - val_loss: 0.6790 - val_accuracy: 0.7813\n",
      "Epoch 18/50\n",
      "603/603 [==============================] - 14s 23ms/step - loss: 0.1686 - accuracy: 0.9365 - val_loss: 0.6745 - val_accuracy: 0.7776\n",
      "Epoch 19/50\n",
      "603/603 [==============================] - 16s 27ms/step - loss: 0.1555 - accuracy: 0.9423 - val_loss: 0.7081 - val_accuracy: 0.7825\n",
      "Epoch 20/50\n",
      "603/603 [==============================] - 13s 22ms/step - loss: 0.1494 - accuracy: 0.9454 - val_loss: 0.7205 - val_accuracy: 0.7799\n",
      "Epoch 21/50\n",
      "603/603 [==============================] - 14s 23ms/step - loss: 0.1376 - accuracy: 0.9472 - val_loss: 0.7451 - val_accuracy: 0.7691\n",
      "Epoch 22/50\n",
      "603/603 [==============================] - 14s 24ms/step - loss: 0.1304 - accuracy: 0.9501 - val_loss: 0.7737 - val_accuracy: 0.7786\n",
      "Epoch 23/50\n",
      "603/603 [==============================] - 14s 24ms/step - loss: 0.1190 - accuracy: 0.9554 - val_loss: 0.7828 - val_accuracy: 0.7745\n",
      "Epoch 24/50\n",
      "603/603 [==============================] - 15s 26ms/step - loss: 0.1129 - accuracy: 0.9570 - val_loss: 0.8339 - val_accuracy: 0.7724\n",
      "Epoch 25/50\n",
      "603/603 [==============================] - 14s 24ms/step - loss: 0.1118 - accuracy: 0.9591 - val_loss: 0.8568 - val_accuracy: 0.7747\n",
      "Epoch 26/50\n",
      "603/603 [==============================] - 15s 25ms/step - loss: 0.1075 - accuracy: 0.9597 - val_loss: 0.8293 - val_accuracy: 0.7765\n",
      "Epoch 27/50\n",
      "603/603 [==============================] - 14s 23ms/step - loss: 0.0961 - accuracy: 0.9642 - val_loss: 0.9142 - val_accuracy: 0.7720\n",
      "Epoch 28/50\n",
      "603/603 [==============================] - 11s 19ms/step - loss: 0.0933 - accuracy: 0.9662 - val_loss: 0.8748 - val_accuracy: 0.7629\n",
      "Epoch 29/50\n",
      "603/603 [==============================] - 12s 20ms/step - loss: 0.0880 - accuracy: 0.9679 - val_loss: 0.8631 - val_accuracy: 0.7803\n",
      "Epoch 30/50\n",
      "603/603 [==============================] - 12s 19ms/step - loss: 0.0844 - accuracy: 0.9677 - val_loss: 0.9271 - val_accuracy: 0.7740\n",
      "Epoch 31/50\n",
      "603/603 [==============================] - 12s 20ms/step - loss: 0.0793 - accuracy: 0.9700 - val_loss: 0.8814 - val_accuracy: 0.7784\n",
      "Epoch 32/50\n",
      "603/603 [==============================] - 13s 22ms/step - loss: 0.0711 - accuracy: 0.9739 - val_loss: 0.9540 - val_accuracy: 0.7705\n",
      "Epoch 33/50\n",
      "603/603 [==============================] - 13s 21ms/step - loss: 0.0765 - accuracy: 0.9706 - val_loss: 0.9961 - val_accuracy: 0.7730\n",
      "Epoch 34/50\n",
      "603/603 [==============================] - 13s 22ms/step - loss: 0.0682 - accuracy: 0.9739 - val_loss: 1.0165 - val_accuracy: 0.7755\n",
      "Epoch 35/50\n",
      "603/603 [==============================] - 13s 22ms/step - loss: 0.0662 - accuracy: 0.9754 - val_loss: 0.9996 - val_accuracy: 0.7676\n",
      "Epoch 36/50\n",
      "603/603 [==============================] - 12s 19ms/step - loss: 0.0670 - accuracy: 0.9747 - val_loss: 1.0067 - val_accuracy: 0.7639\n",
      "Epoch 37/50\n",
      "603/603 [==============================] - 13s 21ms/step - loss: 0.0569 - accuracy: 0.9794 - val_loss: 1.0683 - val_accuracy: 0.7658\n",
      "Epoch 38/50\n",
      "603/603 [==============================] - 13s 22ms/step - loss: 0.0566 - accuracy: 0.9773 - val_loss: 1.0720 - val_accuracy: 0.7610\n",
      "Epoch 39/50\n",
      "603/603 [==============================] - 14s 24ms/step - loss: 0.0619 - accuracy: 0.9763 - val_loss: 1.0681 - val_accuracy: 0.7680\n",
      "Epoch 40/50\n",
      "603/603 [==============================] - 15s 25ms/step - loss: 0.0567 - accuracy: 0.9788 - val_loss: 1.0569 - val_accuracy: 0.7573\n",
      "Epoch 41/50\n",
      "603/603 [==============================] - 13s 21ms/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 1.1430 - val_accuracy: 0.7626\n",
      "Epoch 42/50\n",
      "603/603 [==============================] - 15s 24ms/step - loss: 0.0531 - accuracy: 0.9801 - val_loss: 1.0530 - val_accuracy: 0.7718\n",
      "Epoch 43/50\n",
      "603/603 [==============================] - 15s 25ms/step - loss: 0.0536 - accuracy: 0.9804 - val_loss: 1.0442 - val_accuracy: 0.7699\n",
      "Epoch 44/50\n",
      "603/603 [==============================] - 16s 27ms/step - loss: 0.0476 - accuracy: 0.9822 - val_loss: 1.1524 - val_accuracy: 0.7629\n",
      "Epoch 45/50\n",
      "603/603 [==============================] - 16s 27ms/step - loss: 0.0455 - accuracy: 0.9838 - val_loss: 1.1577 - val_accuracy: 0.7695\n",
      "Epoch 46/50\n",
      "603/603 [==============================] - 16s 26ms/step - loss: 0.0463 - accuracy: 0.9823 - val_loss: 1.1659 - val_accuracy: 0.7629\n",
      "Epoch 47/50\n",
      "603/603 [==============================] - 16s 27ms/step - loss: 0.0474 - accuracy: 0.9827 - val_loss: 1.0928 - val_accuracy: 0.7678\n",
      "Epoch 48/50\n",
      "603/603 [==============================] - 16s 26ms/step - loss: 0.0469 - accuracy: 0.9833 - val_loss: 1.1500 - val_accuracy: 0.7608\n",
      "Epoch 49/50\n",
      "603/603 [==============================] - 16s 27ms/step - loss: 0.0463 - accuracy: 0.9827 - val_loss: 1.1543 - val_accuracy: 0.7641\n",
      "Epoch 50/50\n",
      "603/603 [==============================] - 16s 26ms/step - loss: 0.0423 - accuracy: 0.9844 - val_loss: 1.1812 - val_accuracy: 0.7595\n",
      "151/151 [==============================] - 2s 8ms/step\n",
      "Accuracy: 0.7595356550580431\n",
      "Precision: 0.7292948414576431\n",
      "Recall: 0.7238139971817755\n",
      "F1 Score: 0.7265440829797266\n",
      "ROC AUC: 0.830319442350577\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    LSTM(units=64, return_sequences=True, input_shape=(sequence_length, n_features)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(units=64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(units=64),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "acc = accuracy_score(y_test, y_pred.round())\n",
    "prec = precision_score(y_test, y_pred.round())\n",
    "rec = recall_score(y_test, y_pred.round())\n",
    "f1 = f1_score(y_test, y_pred.round())\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47cfe53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "603/603 [==============================] - 34s 44ms/step - loss: 0.6893 - accuracy: 0.5475 - val_loss: 0.6860 - val_accuracy: 0.5587\n",
      "Epoch 2/50\n",
      "603/603 [==============================] - 26s 42ms/step - loss: 0.5924 - accuracy: 0.6643 - val_loss: 0.4790 - val_accuracy: 0.7751\n",
      "Epoch 3/50\n",
      "603/603 [==============================] - 26s 44ms/step - loss: 0.4504 - accuracy: 0.7964 - val_loss: 0.4469 - val_accuracy: 0.7933\n",
      "Epoch 4/50\n",
      "603/603 [==============================] - 25s 42ms/step - loss: 0.4145 - accuracy: 0.8129 - val_loss: 0.4363 - val_accuracy: 0.7989\n",
      "Epoch 5/50\n",
      "603/603 [==============================] - 20s 33ms/step - loss: 0.3787 - accuracy: 0.8334 - val_loss: 0.4524 - val_accuracy: 0.7981\n",
      "Epoch 6/50\n",
      "603/603 [==============================] - 24s 39ms/step - loss: 0.3451 - accuracy: 0.8528 - val_loss: 0.4451 - val_accuracy: 0.8016\n",
      "Epoch 7/50\n",
      "603/603 [==============================] - 21s 36ms/step - loss: 0.3070 - accuracy: 0.8731 - val_loss: 0.4792 - val_accuracy: 0.7933\n",
      "Epoch 8/50\n",
      "603/603 [==============================] - 21s 35ms/step - loss: 0.2713 - accuracy: 0.8905 - val_loss: 0.4829 - val_accuracy: 0.7964\n",
      "Epoch 9/50\n",
      "603/603 [==============================] - 19s 32ms/step - loss: 0.2327 - accuracy: 0.9082 - val_loss: 0.5534 - val_accuracy: 0.7892\n",
      "Epoch 10/50\n",
      "603/603 [==============================] - 22s 37ms/step - loss: 0.2024 - accuracy: 0.9237 - val_loss: 0.5856 - val_accuracy: 0.7908\n",
      "Epoch 11/50\n",
      "603/603 [==============================] - 25s 41ms/step - loss: 0.1719 - accuracy: 0.9358 - val_loss: 0.6437 - val_accuracy: 0.7894\n",
      "Epoch 12/50\n",
      "603/603 [==============================] - 55s 91ms/step - loss: 0.1477 - accuracy: 0.9458 - val_loss: 0.6613 - val_accuracy: 0.7863\n",
      "Epoch 13/50\n",
      "603/603 [==============================] - 20s 33ms/step - loss: 0.1266 - accuracy: 0.9541 - val_loss: 0.7206 - val_accuracy: 0.7799\n",
      "Epoch 14/50\n",
      "603/603 [==============================] - 24s 40ms/step - loss: 0.1066 - accuracy: 0.9602 - val_loss: 0.7442 - val_accuracy: 0.7732\n",
      "Epoch 15/50\n",
      "603/603 [==============================] - 26s 44ms/step - loss: 0.0876 - accuracy: 0.9687 - val_loss: 0.7849 - val_accuracy: 0.7830\n",
      "Epoch 16/50\n",
      "603/603 [==============================] - 26s 44ms/step - loss: 0.0810 - accuracy: 0.9709 - val_loss: 0.7645 - val_accuracy: 0.7871\n",
      "Epoch 17/50\n",
      "603/603 [==============================] - 19s 31ms/step - loss: 0.0727 - accuracy: 0.9742 - val_loss: 0.8667 - val_accuracy: 0.7825\n",
      "Epoch 18/50\n",
      "603/603 [==============================] - 22s 37ms/step - loss: 0.0613 - accuracy: 0.9789 - val_loss: 0.9185 - val_accuracy: 0.7697\n",
      "Epoch 19/50\n",
      "603/603 [==============================] - 24s 40ms/step - loss: 0.0572 - accuracy: 0.9797 - val_loss: 0.8890 - val_accuracy: 0.7738\n",
      "Epoch 20/50\n",
      "603/603 [==============================] - 25s 41ms/step - loss: 0.0552 - accuracy: 0.9805 - val_loss: 0.9516 - val_accuracy: 0.7703\n",
      "Epoch 21/50\n",
      "603/603 [==============================] - 25s 41ms/step - loss: 0.0470 - accuracy: 0.9837 - val_loss: 0.9701 - val_accuracy: 0.7784\n",
      "Epoch 22/50\n",
      "603/603 [==============================] - 27s 45ms/step - loss: 0.0472 - accuracy: 0.9841 - val_loss: 0.9216 - val_accuracy: 0.7745\n",
      "Epoch 23/50\n",
      "603/603 [==============================] - 28s 46ms/step - loss: 0.0441 - accuracy: 0.9844 - val_loss: 1.0566 - val_accuracy: 0.7765\n",
      "Epoch 24/50\n",
      "603/603 [==============================] - 27s 45ms/step - loss: 0.0375 - accuracy: 0.9869 - val_loss: 1.0442 - val_accuracy: 0.7799\n",
      "Epoch 25/50\n",
      "603/603 [==============================] - 25s 41ms/step - loss: 0.0404 - accuracy: 0.9857 - val_loss: 1.0143 - val_accuracy: 0.7772\n",
      "Epoch 26/50\n",
      "603/603 [==============================] - 29s 49ms/step - loss: 0.0352 - accuracy: 0.9885 - val_loss: 0.9401 - val_accuracy: 0.7774\n",
      "Epoch 27/50\n",
      "603/603 [==============================] - 30s 50ms/step - loss: 0.0373 - accuracy: 0.9868 - val_loss: 1.0876 - val_accuracy: 0.7730\n",
      "Epoch 28/50\n",
      "603/603 [==============================] - 30s 50ms/step - loss: 0.0328 - accuracy: 0.9893 - val_loss: 1.0142 - val_accuracy: 0.7854\n",
      "Epoch 29/50\n",
      "603/603 [==============================] - 29s 49ms/step - loss: 0.0339 - accuracy: 0.9885 - val_loss: 1.0370 - val_accuracy: 0.7722\n",
      "Epoch 30/50\n",
      "603/603 [==============================] - 27s 44ms/step - loss: 0.0316 - accuracy: 0.9891 - val_loss: 1.1753 - val_accuracy: 0.7776\n",
      "Epoch 31/50\n",
      "603/603 [==============================] - 669s 1s/step - loss: 0.0307 - accuracy: 0.9897 - val_loss: 1.1671 - val_accuracy: 0.7799\n",
      "Epoch 32/50\n",
      "603/603 [==============================] - 27s 45ms/step - loss: 0.0290 - accuracy: 0.9904 - val_loss: 1.1737 - val_accuracy: 0.7859\n",
      "Epoch 33/50\n",
      "603/603 [==============================] - 29s 48ms/step - loss: 0.0276 - accuracy: 0.9908 - val_loss: 1.1078 - val_accuracy: 0.7838\n",
      "Epoch 34/50\n",
      "603/603 [==============================] - 28s 46ms/step - loss: 0.0295 - accuracy: 0.9892 - val_loss: 1.1345 - val_accuracy: 0.7730\n",
      "Epoch 35/50\n",
      "603/603 [==============================] - 30s 50ms/step - loss: 0.0274 - accuracy: 0.9902 - val_loss: 1.1681 - val_accuracy: 0.7817\n",
      "Epoch 36/50\n",
      "603/603 [==============================] - 32s 53ms/step - loss: 0.0267 - accuracy: 0.9899 - val_loss: 1.2292 - val_accuracy: 0.7755\n",
      "Epoch 37/50\n",
      "603/603 [==============================] - 31s 51ms/step - loss: 0.0233 - accuracy: 0.9916 - val_loss: 1.1685 - val_accuracy: 0.7699\n",
      "Epoch 38/50\n",
      "603/603 [==============================] - 31s 52ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 1.2509 - val_accuracy: 0.7689\n",
      "Epoch 39/50\n",
      "603/603 [==============================] - 30s 50ms/step - loss: 0.0227 - accuracy: 0.9924 - val_loss: 1.2207 - val_accuracy: 0.7801\n",
      "Epoch 40/50\n",
      "603/603 [==============================] - 31s 52ms/step - loss: 0.0263 - accuracy: 0.9914 - val_loss: 1.1266 - val_accuracy: 0.7769\n",
      "Epoch 41/50\n",
      "603/603 [==============================] - 29s 47ms/step - loss: 0.0219 - accuracy: 0.9925 - val_loss: 1.1908 - val_accuracy: 0.7778\n",
      "Epoch 42/50\n",
      "603/603 [==============================] - 34s 57ms/step - loss: 0.0252 - accuracy: 0.9907 - val_loss: 1.1862 - val_accuracy: 0.7772\n",
      "Epoch 43/50\n",
      "603/603 [==============================] - 27s 44ms/step - loss: 0.0213 - accuracy: 0.9922 - val_loss: 1.2152 - val_accuracy: 0.7767\n",
      "Epoch 44/50\n",
      "603/603 [==============================] - 26s 44ms/step - loss: 0.0222 - accuracy: 0.9924 - val_loss: 1.2190 - val_accuracy: 0.7736\n",
      "Epoch 45/50\n",
      "603/603 [==============================] - 28s 47ms/step - loss: 0.0249 - accuracy: 0.9918 - val_loss: 1.1278 - val_accuracy: 0.7751\n",
      "Epoch 46/50\n",
      "603/603 [==============================] - 28s 46ms/step - loss: 0.0199 - accuracy: 0.9933 - val_loss: 1.3060 - val_accuracy: 0.7821\n",
      "Epoch 47/50\n",
      "603/603 [==============================] - 26s 42ms/step - loss: 0.0217 - accuracy: 0.9921 - val_loss: 1.1696 - val_accuracy: 0.7730\n",
      "Epoch 48/50\n",
      "603/603 [==============================] - 24s 40ms/step - loss: 0.0202 - accuracy: 0.9927 - val_loss: 1.3104 - val_accuracy: 0.7691\n",
      "Epoch 49/50\n",
      "603/603 [==============================] - 27s 45ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 1.2770 - val_accuracy: 0.7740\n",
      "Epoch 50/50\n",
      "603/603 [==============================] - 25s 42ms/step - loss: 0.0225 - accuracy: 0.9923 - val_loss: 1.1565 - val_accuracy: 0.7743\n",
      "151/151 [==============================] - 5s 19ms/step\n",
      "Accuracy: 0.7742537313432836\n",
      "Precision: 0.7529182879377432\n",
      "Recall: 0.7271019257867544\n",
      "F1 Score: 0.7397849462365591\n",
      "ROC AUC Score: 0.841831636792383\n",
      "Specificity: 0.8115027829313544\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    LSTM(units=128, return_sequences=True, input_shape=(sequence_length, n_features)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(units=128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(units=64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(units=64),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "precision = precision_score(y_test, y_pred_classes)\n",
    "recall = recall_score(y_test, y_pred_classes)\n",
    "f1 = f1_score(y_test, y_pred_classes)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classes).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "print(\"Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7546c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 2s 8ms/step\n",
      "Fold 5\n",
      "Accuracy: 0.7665837479270315\n",
      "Precision: 0.7488833746898264\n",
      "Recall: 0.7087834664161579\n",
      "F1 score: 0.7282818532818535\n",
      "ROC AUC score: 0.7605141821876708\n",
      "Specificity: 0.8122448979591836\n",
      "\n",
      "\n",
      "151/151 [==============================] - 2s 8ms/step\n",
      "Fold 6\n",
      "Accuracy: 0.7696932006633499\n",
      "Precision: 0.7633018398806564\n",
      "Recall: 0.7073732718894009\n",
      "F1 score: 0.7342740971059556\n",
      "ROC AUC score: 0.7640106751308345\n",
      "Specificity: 0.8206480783722683\n",
      "\n",
      "\n",
      "151/151 [==============================] - 3s 7ms/step\n",
      "Fold 7\n",
      "Accuracy: 0.7559610201119635\n",
      "Precision: 0.726530612244898\n",
      "Recall: 0.7362132352941176\n",
      "F1 score: 0.7313398767404703\n",
      "ROC AUC score: 0.7542040864796996\n",
      "Specificity: 0.7721949376652815\n",
      "\n",
      "\n",
      "151/151 [==============================] - 3s 7ms/step\n",
      "Fold 8\n",
      "Accuracy: 0.772133526850508\n",
      "Precision: 0.7525325615050651\n",
      "Recall: 0.7269338303821062\n",
      "F1 score: 0.7395117326380658\n",
      "ROC AUC score: 0.7676507030132421\n",
      "Specificity: 0.808367575644378\n",
      "\n",
      "\n",
      "151/151 [==============================] - 3s 9ms/step\n",
      "Fold 9\n",
      "Accuracy: 0.7652913124611238\n",
      "Precision: 0.7525325615050651\n",
      "Recall: 0.7159247361174851\n",
      "F1 score: 0.7337723424270931\n",
      "ROC AUC score: 0.7609502651843099\n",
      "Specificity: 0.8059757942511346\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(16),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the evaluation metrics\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    return accuracy, precision, recall, f1, roc_auc, specificity\n",
    "\n",
    "\n",
    "\n",
    "# Define the cross-validation folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the folds\n",
    "fold = 5\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into train and test sets for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Create the model\n",
    "    model = create_model(X_train.shape[1:])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.round(y_pred)\n",
    "    y_true = y_test\n",
    "    \n",
    "    # Evaluate the model on test set\n",
    "    accuracy, precision, recall, f1, roc_auc, specificity = evaluate_metrics(y_true, y_pred)\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print(f\"Fold {fold}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    print(f\"ROC AUC score: {roc_auc}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Increase the fold counter\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b6c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
