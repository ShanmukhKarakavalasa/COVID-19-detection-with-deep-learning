{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1c15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('updated_coswara.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2650c66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc{1}</th>\n",
       "      <th>mfcc{2}</th>\n",
       "      <th>mfcc{3}</th>\n",
       "      <th>mfcc{4}</th>\n",
       "      <th>...</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>ddpJitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>aqpq5Shimmer</th>\n",
       "      <th>apq11Shimmer</th>\n",
       "      <th>ddaShimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039102</td>\n",
       "      <td>0.378903</td>\n",
       "      <td>786.823461</td>\n",
       "      <td>966.699650</td>\n",
       "      <td>1387.984940</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>-412.08945</td>\n",
       "      <td>126.752335</td>\n",
       "      <td>31.558170</td>\n",
       "      <td>18.483738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.212818</td>\n",
       "      <td>1.734878</td>\n",
       "      <td>0.097995</td>\n",
       "      <td>0.147546</td>\n",
       "      <td>0.206467</td>\n",
       "      <td>0.293984</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051812</td>\n",
       "      <td>0.436672</td>\n",
       "      <td>2219.820298</td>\n",
       "      <td>1874.652272</td>\n",
       "      <td>4134.754998</td>\n",
       "      <td>0.209401</td>\n",
       "      <td>-398.93295</td>\n",
       "      <td>50.929253</td>\n",
       "      <td>-17.480385</td>\n",
       "      <td>4.325164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.092139</td>\n",
       "      <td>0.283908</td>\n",
       "      <td>2.113383</td>\n",
       "      <td>0.137225</td>\n",
       "      <td>0.241707</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.411675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.238371</td>\n",
       "      <td>2002.598308</td>\n",
       "      <td>2058.223470</td>\n",
       "      <td>4324.443295</td>\n",
       "      <td>0.146994</td>\n",
       "      <td>-599.82200</td>\n",
       "      <td>40.048190</td>\n",
       "      <td>6.373952</td>\n",
       "      <td>13.369130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.022602</td>\n",
       "      <td>0.056016</td>\n",
       "      <td>0.134015</td>\n",
       "      <td>1.358899</td>\n",
       "      <td>0.061172</td>\n",
       "      <td>0.091838</td>\n",
       "      <td>0.164672</td>\n",
       "      <td>0.183516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.078437</td>\n",
       "      <td>0.242433</td>\n",
       "      <td>569.328347</td>\n",
       "      <td>506.956042</td>\n",
       "      <td>979.591497</td>\n",
       "      <td>0.035271</td>\n",
       "      <td>-423.74878</td>\n",
       "      <td>77.112580</td>\n",
       "      <td>-11.768955</td>\n",
       "      <td>6.080464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050445</td>\n",
       "      <td>0.070875</td>\n",
       "      <td>0.151334</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>1.673087</td>\n",
       "      <td>0.101366</td>\n",
       "      <td>0.093390</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.304098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072712</td>\n",
       "      <td>0.327475</td>\n",
       "      <td>1344.446613</td>\n",
       "      <td>1062.582139</td>\n",
       "      <td>2431.292693</td>\n",
       "      <td>0.089481</td>\n",
       "      <td>-327.21740</td>\n",
       "      <td>160.910540</td>\n",
       "      <td>-60.493977</td>\n",
       "      <td>-20.299130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.003605</td>\n",
       "      <td>0.085836</td>\n",
       "      <td>0.803849</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>0.054323</td>\n",
       "      <td>0.093975</td>\n",
       "      <td>0.107976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24123</th>\n",
       "      <td>0.056854</td>\n",
       "      <td>0.467262</td>\n",
       "      <td>2845.574993</td>\n",
       "      <td>2044.704050</td>\n",
       "      <td>5269.369989</td>\n",
       "      <td>0.235494</td>\n",
       "      <td>-388.71793</td>\n",
       "      <td>32.990032</td>\n",
       "      <td>-8.879510</td>\n",
       "      <td>20.782476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023928</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>0.071784</td>\n",
       "      <td>0.160208</td>\n",
       "      <td>1.659819</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>0.137543</td>\n",
       "      <td>0.331425</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24124</th>\n",
       "      <td>0.050149</td>\n",
       "      <td>0.267105</td>\n",
       "      <td>1199.737564</td>\n",
       "      <td>1636.413852</td>\n",
       "      <td>2392.060470</td>\n",
       "      <td>0.059086</td>\n",
       "      <td>-433.10117</td>\n",
       "      <td>58.741127</td>\n",
       "      <td>37.350792</td>\n",
       "      <td>51.354850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.419731</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.041864</td>\n",
       "      <td>0.054251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24125</th>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.525523</td>\n",
       "      <td>4996.270042</td>\n",
       "      <td>2579.510698</td>\n",
       "      <td>8270.021928</td>\n",
       "      <td>0.418827</td>\n",
       "      <td>-745.49110</td>\n",
       "      <td>-27.318123</td>\n",
       "      <td>-5.540486</td>\n",
       "      <td>16.303144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016868</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.050605</td>\n",
       "      <td>0.137923</td>\n",
       "      <td>1.255627</td>\n",
       "      <td>0.061327</td>\n",
       "      <td>0.081247</td>\n",
       "      <td>0.116884</td>\n",
       "      <td>0.183981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24126</th>\n",
       "      <td>0.031053</td>\n",
       "      <td>0.167581</td>\n",
       "      <td>1890.217497</td>\n",
       "      <td>2748.644250</td>\n",
       "      <td>5175.546000</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>-499.73740</td>\n",
       "      <td>32.764600</td>\n",
       "      <td>46.792103</td>\n",
       "      <td>40.457275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.016609</td>\n",
       "      <td>0.134111</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.023441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24127</th>\n",
       "      <td>0.024217</td>\n",
       "      <td>0.287329</td>\n",
       "      <td>1296.724388</td>\n",
       "      <td>1259.118567</td>\n",
       "      <td>2319.688878</td>\n",
       "      <td>0.079520</td>\n",
       "      <td>-457.49475</td>\n",
       "      <td>98.845490</td>\n",
       "      <td>-1.881587</td>\n",
       "      <td>40.956110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>0.013602</td>\n",
       "      <td>0.037466</td>\n",
       "      <td>0.106427</td>\n",
       "      <td>1.033221</td>\n",
       "      <td>0.047599</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>0.095882</td>\n",
       "      <td>0.142798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24128 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse  chroma_stft    spec_cent      spec_bw      rolloff       zcr  \\\n",
       "0      0.039102     0.378903   786.823461   966.699650  1387.984940  0.043870   \n",
       "1      0.051812     0.436672  2219.820298  1874.652272  4134.754998  0.209401   \n",
       "2      0.010413     0.238371  2002.598308  2058.223470  4324.443295  0.146994   \n",
       "3      0.078437     0.242433   569.328347   506.956042   979.591497  0.035271   \n",
       "4      0.072712     0.327475  1344.446613  1062.582139  2431.292693  0.089481   \n",
       "...         ...          ...          ...          ...          ...       ...   \n",
       "24123  0.056854     0.467262  2845.574993  2044.704050  5269.369989  0.235494   \n",
       "24124  0.050149     0.267105  1199.737564  1636.413852  2392.060470  0.059086   \n",
       "24125  0.000454     0.525523  4996.270042  2579.510698  8270.021928  0.418827   \n",
       "24126  0.031053     0.167581  1890.217497  2748.644250  5175.546000  0.033493   \n",
       "24127  0.024217     0.287329  1296.724388  1259.118567  2319.688878  0.079520   \n",
       "\n",
       "         mfcc{1}     mfcc{2}    mfcc{3}    mfcc{4}  ...  rapJitter  \\\n",
       "0     -412.08945  126.752335  31.558170  18.483738  ...   0.018672   \n",
       "1     -398.93295   50.929253 -17.480385   4.325164  ...   0.030713   \n",
       "2     -599.82200   40.048190   6.373952  13.369130  ...   0.018672   \n",
       "3     -423.74878   77.112580 -11.768955   6.080464  ...   0.050445   \n",
       "4     -327.21740  160.910540 -60.493977 -20.299130  ...   0.001202   \n",
       "...          ...         ...        ...        ...  ...        ...   \n",
       "24123 -388.71793   32.990032  -8.879510  20.782476  ...   0.023928   \n",
       "24124 -433.10117   58.741127  37.350792  51.354850  ...   0.002124   \n",
       "24125 -745.49110  -27.318123  -5.540486  16.303144  ...   0.016868   \n",
       "24126 -499.73740   32.764600  46.792103  40.457275  ...   0.001794   \n",
       "24127 -457.49475   98.845490  -1.881587  40.956110  ...   0.012489   \n",
       "\n",
       "       ppq5Jitter  ddpJitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "0        0.021894   0.056015      0.212818        1.734878     0.097995   \n",
       "1        0.037553   0.092139      0.283908        2.113383     0.137225   \n",
       "2        0.022602   0.056016      0.134015        1.358899     0.061172   \n",
       "3        0.070875   0.151334      0.181641        1.673087     0.101366   \n",
       "4        0.001537   0.003605      0.085836        0.803849     0.035992   \n",
       "...           ...        ...           ...             ...          ...   \n",
       "24123    0.029485   0.071784      0.160208        1.659819     0.064901   \n",
       "24124    0.002396   0.006373      0.042722        0.419731     0.018084   \n",
       "24125    0.019114   0.050605      0.137923        1.255627     0.061327   \n",
       "24126    0.001854   0.005381      0.016609        0.134111     0.007814   \n",
       "24127    0.013602   0.037466      0.106427        1.033221     0.047599   \n",
       "\n",
       "       aqpq5Shimmer  apq11Shimmer  ddaShimmer  label  \n",
       "0          0.147546      0.206467    0.293984      1  \n",
       "1          0.241707      0.116884    0.411675      0  \n",
       "2          0.091838      0.164672    0.183516      1  \n",
       "3          0.093390      0.116884    0.304098      0  \n",
       "4          0.054323      0.093975    0.107976      1  \n",
       "...             ...           ...         ...    ...  \n",
       "24123      0.137543      0.331425    0.194703      0  \n",
       "24124      0.027000      0.041864    0.054251      0  \n",
       "24125      0.081247      0.116884    0.183981      0  \n",
       "24126      0.009204      0.011837    0.023441      1  \n",
       "24127      0.060751      0.095882    0.142798      0  \n",
       "\n",
       "[24128 rows x 80 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170fafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd083f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5386d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca4ebd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DNN model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0489e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ceda148",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "995df0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "604/604 [==============================] - 7s 6ms/step - loss: 0.5425 - accuracy: 0.7361 - val_loss: 0.4774 - val_accuracy: 0.7723\n",
      "Epoch 2/50\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.4562 - accuracy: 0.7909 - val_loss: 0.4507 - val_accuracy: 0.7909\n",
      "Epoch 3/50\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.4316 - accuracy: 0.8046 - val_loss: 0.4324 - val_accuracy: 0.7994\n",
      "Epoch 4/50\n",
      "604/604 [==============================] - 2s 4ms/step - loss: 0.4172 - accuracy: 0.8089 - val_loss: 0.4225 - val_accuracy: 0.8067\n",
      "Epoch 5/50\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.4080 - accuracy: 0.8126 - val_loss: 0.4184 - val_accuracy: 0.8046\n",
      "Epoch 6/50\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.4004 - accuracy: 0.8184 - val_loss: 0.4096 - val_accuracy: 0.8104\n",
      "Epoch 7/50\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.3937 - accuracy: 0.8215 - val_loss: 0.4064 - val_accuracy: 0.8110\n",
      "Epoch 8/50\n",
      "604/604 [==============================] - 3s 6ms/step - loss: 0.3887 - accuracy: 0.8249 - val_loss: 0.4051 - val_accuracy: 0.8077\n",
      "Epoch 9/50\n",
      "604/604 [==============================] - 2s 4ms/step - loss: 0.3840 - accuracy: 0.8264 - val_loss: 0.4137 - val_accuracy: 0.8067\n",
      "Epoch 10/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3805 - accuracy: 0.8269 - val_loss: 0.4015 - val_accuracy: 0.8143\n",
      "Epoch 11/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3776 - accuracy: 0.8298 - val_loss: 0.4043 - val_accuracy: 0.8135\n",
      "Epoch 12/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3740 - accuracy: 0.8311 - val_loss: 0.4056 - val_accuracy: 0.8139\n",
      "Epoch 13/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3714 - accuracy: 0.8339 - val_loss: 0.3990 - val_accuracy: 0.8160\n",
      "Epoch 14/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3676 - accuracy: 0.8348 - val_loss: 0.3992 - val_accuracy: 0.8127\n",
      "Epoch 15/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3663 - accuracy: 0.8328 - val_loss: 0.4008 - val_accuracy: 0.8123\n",
      "Epoch 16/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3638 - accuracy: 0.8365 - val_loss: 0.3972 - val_accuracy: 0.8133\n",
      "Epoch 17/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3621 - accuracy: 0.8398 - val_loss: 0.4003 - val_accuracy: 0.8121\n",
      "Epoch 18/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3600 - accuracy: 0.8393 - val_loss: 0.3965 - val_accuracy: 0.8137\n",
      "Epoch 19/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3602 - accuracy: 0.8365 - val_loss: 0.4017 - val_accuracy: 0.8179\n",
      "Epoch 20/50\n",
      "604/604 [==============================] - 2s 2ms/step - loss: 0.3565 - accuracy: 0.8389 - val_loss: 0.3972 - val_accuracy: 0.8133\n",
      "Epoch 21/50\n",
      "604/604 [==============================] - 2s 2ms/step - loss: 0.3548 - accuracy: 0.8421 - val_loss: 0.4023 - val_accuracy: 0.8119\n",
      "Epoch 22/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3545 - accuracy: 0.8402 - val_loss: 0.4015 - val_accuracy: 0.8125\n",
      "Epoch 23/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3526 - accuracy: 0.8414 - val_loss: 0.4003 - val_accuracy: 0.8100\n",
      "Epoch 24/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3503 - accuracy: 0.8430 - val_loss: 0.4007 - val_accuracy: 0.8112\n",
      "Epoch 25/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3508 - accuracy: 0.8436 - val_loss: 0.4047 - val_accuracy: 0.8077\n",
      "Epoch 26/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3498 - accuracy: 0.8413 - val_loss: 0.3979 - val_accuracy: 0.8100\n",
      "Epoch 27/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3474 - accuracy: 0.8414 - val_loss: 0.4016 - val_accuracy: 0.8108\n",
      "Epoch 28/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3475 - accuracy: 0.8450 - val_loss: 0.3949 - val_accuracy: 0.8154\n",
      "Epoch 29/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3450 - accuracy: 0.8450 - val_loss: 0.4015 - val_accuracy: 0.8110\n",
      "Epoch 30/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3448 - accuracy: 0.8439 - val_loss: 0.4054 - val_accuracy: 0.8110\n",
      "Epoch 31/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3440 - accuracy: 0.8455 - val_loss: 0.3954 - val_accuracy: 0.8145\n",
      "Epoch 32/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3440 - accuracy: 0.8450 - val_loss: 0.4008 - val_accuracy: 0.8125\n",
      "Epoch 33/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3415 - accuracy: 0.8457 - val_loss: 0.4012 - val_accuracy: 0.8162\n",
      "Epoch 34/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3406 - accuracy: 0.8475 - val_loss: 0.4008 - val_accuracy: 0.8131\n",
      "Epoch 35/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3402 - accuracy: 0.8458 - val_loss: 0.3976 - val_accuracy: 0.8129\n",
      "Epoch 36/50\n",
      "604/604 [==============================] - 2s 2ms/step - loss: 0.3392 - accuracy: 0.8472 - val_loss: 0.3981 - val_accuracy: 0.8083\n",
      "Epoch 37/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3382 - accuracy: 0.8483 - val_loss: 0.3993 - val_accuracy: 0.8102\n",
      "Epoch 38/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3371 - accuracy: 0.8485 - val_loss: 0.4053 - val_accuracy: 0.8116\n",
      "Epoch 39/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3365 - accuracy: 0.8498 - val_loss: 0.4019 - val_accuracy: 0.8090\n",
      "Epoch 40/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3352 - accuracy: 0.8506 - val_loss: 0.3989 - val_accuracy: 0.8114\n",
      "Epoch 41/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3343 - accuracy: 0.8498 - val_loss: 0.3999 - val_accuracy: 0.8081\n",
      "Epoch 42/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3350 - accuracy: 0.8502 - val_loss: 0.4064 - val_accuracy: 0.8092\n",
      "Epoch 43/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3340 - accuracy: 0.8500 - val_loss: 0.3991 - val_accuracy: 0.8108\n",
      "Epoch 44/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3320 - accuracy: 0.8498 - val_loss: 0.4077 - val_accuracy: 0.8112\n",
      "Epoch 45/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8506 - val_loss: 0.3980 - val_accuracy: 0.8158\n",
      "Epoch 46/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3308 - accuracy: 0.8524 - val_loss: 0.4021 - val_accuracy: 0.8098\n",
      "Epoch 47/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 0.8527 - val_loss: 0.4004 - val_accuracy: 0.8139\n",
      "Epoch 48/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8520 - val_loss: 0.4053 - val_accuracy: 0.8096\n",
      "Epoch 49/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3301 - accuracy: 0.8533 - val_loss: 0.3985 - val_accuracy: 0.8100\n",
      "Epoch 50/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.3282 - accuracy: 0.8505 - val_loss: 0.4042 - val_accuracy: 0.8168\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b31cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 0s 1ms/step - loss: 0.4042 - accuracy: 0.8168\n",
      "Test accuracy: 0.8168255090713501\n",
      "151/151 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Save predictions and corresponding file names to a CSV file\n",
    "results = pd.DataFrame({'label': y_test, 'prediction': predictions.flatten()})\n",
    "results.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad81a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.8168255283878989\n",
      "Precision: 0.8586416344561016\n",
      "Recall: 0.7123224919835089\n",
      "F1 Score: 0.7786680020030046\n",
      "ROC AUC: 0.8077314313871384\n",
      "Specificity: 0.903140370790768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"Specificity: {specificity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98a89b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "604/604 [==============================] - 3s 3ms/step - loss: 0.4876 - accuracy: 0.7687 - precision: 0.7799 - recall: 0.6718 - auc: 0.8335 - false_positives: 1635.0000 - val_loss: 0.4342 - val_accuracy: 0.7961 - val_precision: 0.8211 - val_recall: 0.7022 - val_auc: 0.8641 - val_false_positives: 334.0000\n",
      "Epoch 2/50\n",
      "604/604 [==============================] - 2s 4ms/step - loss: 0.4148 - accuracy: 0.8097 - precision: 0.8419 - recall: 0.7066 - auc: 0.8790 - false_positives: 1144.0000 - val_loss: 0.4101 - val_accuracy: 0.8021 - val_precision: 0.8208 - val_recall: 0.7197 - val_auc: 0.8789 - val_false_positives: 343.0000\n",
      "Epoch 3/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3878 - accuracy: 0.8241 - precision: 0.8581 - recall: 0.7262 - auc: 0.8939 - false_positives: 1035.0000 - val_loss: 0.4057 - val_accuracy: 0.8123 - val_precision: 0.8798 - val_recall: 0.6775 - val_auc: 0.8835 - val_false_positives: 202.0000\n",
      "Epoch 4/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3725 - accuracy: 0.8297 - precision: 0.8645 - recall: 0.7337 - auc: 0.9016 - false_positives: 991.0000 - val_loss: 0.3935 - val_accuracy: 0.8152 - val_precision: 0.8403 - val_recall: 0.7302 - val_auc: 0.8878 - val_false_positives: 303.0000\n",
      "Epoch 5/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3574 - accuracy: 0.8407 - precision: 0.8766 - recall: 0.7488 - auc: 0.9104 - false_positives: 909.0000 - val_loss: 0.3947 - val_accuracy: 0.8187 - val_precision: 0.8798 - val_recall: 0.6940 - val_auc: 0.8921 - val_false_positives: 207.0000\n",
      "Epoch 6/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3452 - accuracy: 0.8452 - precision: 0.8772 - recall: 0.7598 - auc: 0.9163 - false_positives: 917.0000 - val_loss: 0.4260 - val_accuracy: 0.8139 - val_precision: 0.9186 - val_recall: 0.6459 - val_auc: 0.8859 - val_false_positives: 125.0000\n",
      "Epoch 7/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3341 - accuracy: 0.8503 - precision: 0.8847 - recall: 0.7645 - auc: 0.9227 - false_positives: 859.0000 - val_loss: 0.3909 - val_accuracy: 0.8168 - val_precision: 0.8563 - val_recall: 0.7151 - val_auc: 0.8931 - val_false_positives: 262.0000\n",
      "Epoch 8/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3219 - accuracy: 0.8543 - precision: 0.8867 - recall: 0.7725 - auc: 0.9287 - false_positives: 851.0000 - val_loss: 0.3929 - val_accuracy: 0.8139 - val_precision: 0.8544 - val_recall: 0.7096 - val_auc: 0.8906 - val_false_positives: 264.0000\n",
      "Epoch 9/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3136 - accuracy: 0.8572 - precision: 0.8870 - recall: 0.7795 - auc: 0.9325 - false_positives: 856.0000 - val_loss: 0.3885 - val_accuracy: 0.8203 - val_precision: 0.8651 - val_recall: 0.7142 - val_auc: 0.8942 - val_false_positives: 243.0000\n",
      "Epoch 10/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2995 - accuracy: 0.8667 - precision: 0.8964 - recall: 0.7932 - auc: 0.9387 - false_positives: 790.0000 - val_loss: 0.3869 - val_accuracy: 0.8270 - val_precision: 0.8732 - val_recall: 0.7224 - val_auc: 0.8958 - val_false_positives: 229.0000\n",
      "Epoch 11/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.2909 - accuracy: 0.8715 - precision: 0.8980 - recall: 0.8035 - auc: 0.9426 - false_positives: 787.0000 - val_loss: 0.4023 - val_accuracy: 0.8206 - val_precision: 0.8840 - val_recall: 0.6945 - val_auc: 0.8920 - val_false_positives: 199.0000\n",
      "Epoch 12/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.2804 - accuracy: 0.8753 - precision: 0.9009 - recall: 0.8100 - auc: 0.9473 - false_positives: 768.0000 - val_loss: 0.4062 - val_accuracy: 0.8197 - val_precision: 0.8641 - val_recall: 0.7137 - val_auc: 0.8891 - val_false_positives: 245.0000\n",
      "Epoch 13/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.2695 - accuracy: 0.8808 - precision: 0.9053 - recall: 0.8188 - auc: 0.9515 - false_positives: 738.0000 - val_loss: 0.4155 - val_accuracy: 0.8199 - val_precision: 0.8622 - val_recall: 0.7164 - val_auc: 0.8913 - val_false_positives: 250.0000\n",
      "Epoch 14/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.2605 - accuracy: 0.8848 - precision: 0.9079 - recall: 0.8259 - auc: 0.9551 - false_positives: 722.0000 - val_loss: 0.4335 - val_accuracy: 0.8228 - val_precision: 0.8502 - val_recall: 0.7384 - val_auc: 0.8893 - val_false_positives: 284.0000\n",
      "Epoch 15/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.2532 - accuracy: 0.8880 - precision: 0.9105 - recall: 0.8310 - auc: 0.9579 - false_positives: 704.0000 - val_loss: 0.4287 - val_accuracy: 0.8160 - val_precision: 0.8309 - val_recall: 0.7448 - val_auc: 0.8910 - val_false_positives: 331.0000\n",
      "Epoch 16/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.2442 - accuracy: 0.8941 - precision: 0.9161 - recall: 0.8398 - auc: 0.9610 - false_positives: 663.0000 - val_loss: 0.4350 - val_accuracy: 0.8139 - val_precision: 0.8192 - val_recall: 0.7554 - val_auc: 0.8898 - val_false_positives: 364.0000\n",
      "Epoch 17/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2330 - accuracy: 0.8981 - precision: 0.9180 - recall: 0.8477 - auc: 0.9649 - false_positives: 653.0000 - val_loss: 0.4409 - val_accuracy: 0.8193 - val_precision: 0.8477 - val_recall: 0.7320 - val_auc: 0.8924 - val_false_positives: 287.0000\n",
      "Epoch 18/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2255 - accuracy: 0.8999 - precision: 0.9167 - recall: 0.8533 - auc: 0.9671 - false_positives: 668.0000 - val_loss: 0.4535 - val_accuracy: 0.8166 - val_precision: 0.8321 - val_recall: 0.7448 - val_auc: 0.8851 - val_false_positives: 328.0000\n",
      "Epoch 19/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2174 - accuracy: 0.9055 - precision: 0.9230 - recall: 0.8602 - auc: 0.9695 - false_positives: 619.0000 - val_loss: 0.4477 - val_accuracy: 0.8108 - val_precision: 0.8314 - val_recall: 0.7297 - val_auc: 0.8866 - val_false_positives: 323.0000\n",
      "Epoch 20/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2074 - accuracy: 0.9102 - precision: 0.9289 - recall: 0.8651 - auc: 0.9725 - false_positives: 571.0000 - val_loss: 0.4689 - val_accuracy: 0.8092 - val_precision: 0.8269 - val_recall: 0.7311 - val_auc: 0.8803 - val_false_positives: 334.0000\n",
      "Epoch 21/50\n",
      "604/604 [==============================] - 2s 4ms/step - loss: 0.2044 - accuracy: 0.9125 - precision: 0.9275 - recall: 0.8723 - auc: 0.9731 - false_positives: 588.0000 - val_loss: 0.4780 - val_accuracy: 0.8050 - val_precision: 0.8111 - val_recall: 0.7416 - val_auc: 0.8840 - val_false_positives: 377.0000\n",
      "Epoch 22/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1971 - accuracy: 0.9167 - precision: 0.9321 - recall: 0.8774 - auc: 0.9754 - false_positives: 551.0000 - val_loss: 0.4896 - val_accuracy: 0.8119 - val_precision: 0.8295 - val_recall: 0.7352 - val_auc: 0.8832 - val_false_positives: 330.0000\n",
      "Epoch 23/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1881 - accuracy: 0.9179 - precision: 0.9332 - recall: 0.8792 - auc: 0.9776 - false_positives: 543.0000 - val_loss: 0.4917 - val_accuracy: 0.8114 - val_precision: 0.8272 - val_recall: 0.7371 - val_auc: 0.8843 - val_false_positives: 336.0000\n",
      "Epoch 24/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1837 - accuracy: 0.9206 - precision: 0.9310 - recall: 0.8879 - auc: 0.9788 - false_positives: 567.0000 - val_loss: 0.5151 - val_accuracy: 0.7961 - val_precision: 0.7759 - val_recall: 0.7723 - val_auc: 0.8799 - val_false_positives: 487.0000\n",
      "Epoch 25/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1738 - accuracy: 0.9254 - precision: 0.9366 - recall: 0.8934 - auc: 0.9814 - false_positives: 521.0000 - val_loss: 0.5326 - val_accuracy: 0.8073 - val_precision: 0.8399 - val_recall: 0.7091 - val_auc: 0.8788 - val_false_positives: 295.0000\n",
      "Epoch 26/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1665 - accuracy: 0.9291 - precision: 0.9385 - recall: 0.9004 - auc: 0.9830 - false_positives: 509.0000 - val_loss: 0.5206 - val_accuracy: 0.8090 - val_precision: 0.8232 - val_recall: 0.7357 - val_auc: 0.8829 - val_false_positives: 345.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.1617 - accuracy: 0.9317 - precision: 0.9415 - recall: 0.9033 - auc: 0.9838 - false_positives: 484.0000 - val_loss: 0.5493 - val_accuracy: 0.8139 - val_precision: 0.8494 - val_recall: 0.7155 - val_auc: 0.8790 - val_false_positives: 277.0000\n",
      "Epoch 28/50\n",
      "604/604 [==============================] - 3s 6ms/step - loss: 0.1581 - accuracy: 0.9320 - precision: 0.9424 - recall: 0.9029 - auc: 0.9846 - false_positives: 476.0000 - val_loss: 0.5571 - val_accuracy: 0.8036 - val_precision: 0.7717 - val_recall: 0.8035 - val_auc: 0.8832 - val_false_positives: 519.0000\n",
      "Epoch 29/50\n",
      "604/604 [==============================] - 3s 6ms/step - loss: 0.1569 - accuracy: 0.9333 - precision: 0.9421 - recall: 0.9064 - auc: 0.9848 - false_positives: 480.0000 - val_loss: 0.5808 - val_accuracy: 0.8083 - val_precision: 0.8246 - val_recall: 0.7320 - val_auc: 0.8810 - val_false_positives: 340.0000\n",
      "Epoch 30/50\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.1482 - accuracy: 0.9373 - precision: 0.9472 - recall: 0.9102 - auc: 0.9866 - false_positives: 437.0000 - val_loss: 0.5681 - val_accuracy: 0.8098 - val_precision: 0.7946 - val_recall: 0.7815 - val_auc: 0.8821 - val_false_positives: 441.0000\n",
      "Epoch 31/50\n",
      "604/604 [==============================] - 2s 4ms/step - loss: 0.1400 - accuracy: 0.9405 - precision: 0.9496 - recall: 0.9154 - auc: 0.9882 - false_positives: 419.0000 - val_loss: 0.6010 - val_accuracy: 0.8152 - val_precision: 0.8417 - val_recall: 0.7284 - val_auc: 0.8807 - val_false_positives: 299.0000\n",
      "Epoch 32/50\n",
      "604/604 [==============================] - 3s 6ms/step - loss: 0.1308 - accuracy: 0.9446 - precision: 0.9525 - recall: 0.9218 - auc: 0.9898 - false_positives: 396.0000 - val_loss: 0.6090 - val_accuracy: 0.8061 - val_precision: 0.8154 - val_recall: 0.7384 - val_auc: 0.8791 - val_false_positives: 365.0000\n",
      "Epoch 33/50\n",
      "604/604 [==============================] - 3s 6ms/step - loss: 0.1277 - accuracy: 0.9460 - precision: 0.9523 - recall: 0.9255 - auc: 0.9902 - false_positives: 400.0000 - val_loss: 0.6403 - val_accuracy: 0.8013 - val_precision: 0.8145 - val_recall: 0.7261 - val_auc: 0.8730 - val_false_positives: 361.0000\n",
      "Epoch 34/50\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.1260 - accuracy: 0.9471 - precision: 0.9521 - recall: 0.9283 - auc: 0.9904 - false_positives: 403.0000 - val_loss: 0.6412 - val_accuracy: 0.7949 - val_precision: 0.7820 - val_recall: 0.7577 - val_auc: 0.8738 - val_false_positives: 461.0000\n",
      "Epoch 35/50\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.1256 - accuracy: 0.9482 - precision: 0.9534 - recall: 0.9295 - auc: 0.9904 - false_positives: 392.0000 - val_loss: 0.6348 - val_accuracy: 0.8104 - val_precision: 0.8316 - val_recall: 0.7284 - val_auc: 0.8798 - val_false_positives: 322.0000\n",
      "Epoch 36/50\n",
      "604/604 [==============================] - 4s 6ms/step - loss: 0.1149 - accuracy: 0.9530 - precision: 0.9595 - recall: 0.9342 - auc: 0.9923 - false_positives: 340.0000 - val_loss: 0.6595 - val_accuracy: 0.8023 - val_precision: 0.8032 - val_recall: 0.7458 - val_auc: 0.8767 - val_false_positives: 399.0000\n",
      "Epoch 37/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1145 - accuracy: 0.9539 - precision: 0.9598 - recall: 0.9360 - auc: 0.9920 - false_positives: 338.0000 - val_loss: 0.6711 - val_accuracy: 0.8075 - val_precision: 0.8126 - val_recall: 0.7467 - val_auc: 0.8733 - val_false_positives: 376.0000\n",
      "Epoch 38/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.1103 - accuracy: 0.9551 - precision: 0.9603 - recall: 0.9382 - auc: 0.9927 - false_positives: 334.0000 - val_loss: 0.7078 - val_accuracy: 0.8185 - val_precision: 0.8343 - val_recall: 0.7471 - val_auc: 0.8755 - val_false_positives: 324.0000\n",
      "Epoch 39/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.1059 - accuracy: 0.9589 - precision: 0.9628 - recall: 0.9446 - auc: 0.9934 - false_positives: 315.0000 - val_loss: 0.7213 - val_accuracy: 0.8015 - val_precision: 0.7998 - val_recall: 0.7485 - val_auc: 0.8693 - val_false_positives: 409.0000\n",
      "Epoch 40/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.1038 - accuracy: 0.9590 - precision: 0.9643 - recall: 0.9430 - auc: 0.9939 - false_positives: 301.0000 - val_loss: 0.7296 - val_accuracy: 0.7994 - val_precision: 0.7889 - val_recall: 0.7600 - val_auc: 0.8723 - val_false_positives: 444.0000\n",
      "Epoch 41/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1030 - accuracy: 0.9563 - precision: 0.9581 - recall: 0.9434 - auc: 0.9936 - false_positives: 356.0000 - val_loss: 0.7396 - val_accuracy: 0.8050 - val_precision: 0.8009 - val_recall: 0.7572 - val_auc: 0.8693 - val_false_positives: 411.0000\n",
      "Epoch 42/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.0957 - accuracy: 0.9608 - precision: 0.9668 - recall: 0.9448 - auc: 0.9946 - false_positives: 280.0000 - val_loss: 0.7648 - val_accuracy: 0.7980 - val_precision: 0.8011 - val_recall: 0.7361 - val_auc: 0.8673 - val_false_positives: 399.0000\n",
      "Epoch 43/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.0872 - accuracy: 0.9638 - precision: 0.9680 - recall: 0.9504 - auc: 0.9957 - false_positives: 271.0000 - val_loss: 0.7963 - val_accuracy: 0.8034 - val_precision: 0.8113 - val_recall: 0.7366 - val_auc: 0.8687 - val_false_positives: 374.0000\n",
      "Epoch 44/50\n",
      "604/604 [==============================] - 3s 6ms/step - loss: 0.1004 - accuracy: 0.9572 - precision: 0.9614 - recall: 0.9420 - auc: 0.9939 - false_positives: 326.0000 - val_loss: 0.8183 - val_accuracy: 0.8046 - val_precision: 0.8100 - val_recall: 0.7421 - val_auc: 0.8680 - val_false_positives: 380.0000\n",
      "Epoch 45/50\n",
      "604/604 [==============================] - 4s 7ms/step - loss: 0.0867 - accuracy: 0.9648 - precision: 0.9720 - recall: 0.9486 - auc: 0.9955 - false_positives: 236.0000 - val_loss: 0.8053 - val_accuracy: 0.8025 - val_precision: 0.8119 - val_recall: 0.7334 - val_auc: 0.8711 - val_false_positives: 371.0000\n",
      "Epoch 46/50\n",
      "604/604 [==============================] - 4s 6ms/step - loss: 0.0812 - accuracy: 0.9667 - precision: 0.9753 - recall: 0.9494 - auc: 0.9962 - false_positives: 207.0000 - val_loss: 0.8598 - val_accuracy: 0.8075 - val_precision: 0.8339 - val_recall: 0.7174 - val_auc: 0.8667 - val_false_positives: 312.0000\n",
      "Epoch 47/50\n",
      "604/604 [==============================] - 3s 6ms/step - loss: 0.0838 - accuracy: 0.9652 - precision: 0.9742 - recall: 0.9471 - auc: 0.9958 - false_positives: 216.0000 - val_loss: 0.8376 - val_accuracy: 0.7982 - val_precision: 0.7855 - val_recall: 0.7618 - val_auc: 0.8696 - val_false_positives: 454.0000\n",
      "Epoch 48/50\n",
      "604/604 [==============================] - 2s 4ms/step - loss: 0.0875 - accuracy: 0.9641 - precision: 0.9751 - recall: 0.9437 - auc: 0.9952 - false_positives: 208.0000 - val_loss: 0.8590 - val_accuracy: 0.7860 - val_precision: 0.7667 - val_recall: 0.7572 - val_auc: 0.8597 - val_false_positives: 503.0000\n",
      "Epoch 49/50\n",
      "604/604 [==============================] - 4s 6ms/step - loss: 0.0740 - accuracy: 0.9693 - precision: 0.9776 - recall: 0.9531 - auc: 0.9970 - false_positives: 188.0000 - val_loss: 0.8660 - val_accuracy: 0.8002 - val_precision: 0.7983 - val_recall: 0.7471 - val_auc: 0.8671 - val_false_positives: 412.0000\n",
      "Epoch 50/50\n",
      "604/604 [==============================] - 4s 6ms/step - loss: 0.0710 - accuracy: 0.9718 - precision: 0.9816 - recall: 0.9548 - auc: 0.9972 - false_positives: 154.0000 - val_loss: 0.9050 - val_accuracy: 0.8013 - val_precision: 0.8060 - val_recall: 0.7384 - val_auc: 0.8640 - val_false_positives: 388.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fbda593790>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC(), keras.metrics.FalsePositives()])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aed7e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 1s 5ms/step - loss: 0.9050 - accuracy: 0.8013 - precision: 0.8060 - recall: 0.7384 - auc: 0.8640 - false_positives: 388.0000\n",
      "Test loss: 0.9050312042236328\n",
      "Test accuracy: 0.8012847304344177\n",
      "Test precision: 0.8059999942779541\n",
      "Test recall: 0.7384333610534668\n",
      "Test F1 score: 0.46782697887264635\n",
      "Test ROC AUC: 0.8640339970588684\n",
      "Test specificity: 0.8601801801801802\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy, precision, recall, auc, fp = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test precision: {precision}\")\n",
    "print(f\"Test recall: {recall}\")\n",
    "print(f\"Test F1 score: {2*((precision*recall)/(precision+recall+1))}\")\n",
    "print(f\"Test ROC AUC: {auc}\")\n",
    "print(f\"Test specificity: {1-fp/(fp+tn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53e24a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "604/604 [==============================] - 5s 4ms/step - loss: 0.5159 - accuracy: 0.7470 - precision_1: 0.7736 - recall_1: 0.6130 - auc_1: 0.8161 - false_positives_1: 1547.0000 - val_loss: 0.4437 - val_accuracy: 0.7886 - val_precision_1: 0.8098 - val_recall_1: 0.6963 - val_auc_1: 0.8587 - val_false_positives_1: 357.0000\n",
      "Epoch 2/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.4270 - accuracy: 0.8036 - precision_1: 0.8339 - recall_1: 0.6996 - auc_1: 0.8706 - false_positives_1: 1201.0000 - val_loss: 0.4201 - val_accuracy: 0.7951 - val_precision_1: 0.7955 - val_recall_1: 0.7361 - val_auc_1: 0.8758 - val_false_positives_1: 413.0000\n",
      "Epoch 3/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.4016 - accuracy: 0.8135 - precision_1: 0.8470 - recall_1: 0.7108 - auc_1: 0.8858 - false_positives_1: 1107.0000 - val_loss: 0.4059 - val_accuracy: 0.8098 - val_precision_1: 0.8580 - val_recall_1: 0.6945 - val_auc_1: 0.8822 - val_false_positives_1: 251.0000\n",
      "Epoch 4/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3845 - accuracy: 0.8217 - precision_1: 0.8561 - recall_1: 0.7223 - auc_1: 0.8953 - false_positives_1: 1047.0000 - val_loss: 0.3983 - val_accuracy: 0.8127 - val_precision_1: 0.8455 - val_recall_1: 0.7169 - val_auc_1: 0.8865 - val_false_positives_1: 286.0000\n",
      "Epoch 5/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3738 - accuracy: 0.8316 - precision_1: 0.8658 - recall_1: 0.7372 - auc_1: 0.9014 - false_positives_1: 985.0000 - val_loss: 0.3948 - val_accuracy: 0.8106 - val_precision_1: 0.8649 - val_recall_1: 0.6890 - val_auc_1: 0.8882 - val_false_positives_1: 235.0000\n",
      "Epoch 6/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3631 - accuracy: 0.8332 - precision_1: 0.8670 - recall_1: 0.7402 - auc_1: 0.9070 - false_positives_1: 979.0000 - val_loss: 0.3939 - val_accuracy: 0.8191 - val_precision_1: 0.8786 - val_recall_1: 0.6963 - val_auc_1: 0.8902 - val_false_positives_1: 210.0000\n",
      "Epoch 7/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3548 - accuracy: 0.8389 - precision_1: 0.8754 - recall_1: 0.7455 - auc_1: 0.9120 - false_positives_1: 915.0000 - val_loss: 0.4001 - val_accuracy: 0.8179 - val_precision_1: 0.8626 - val_recall_1: 0.7105 - val_auc_1: 0.8898 - val_false_positives_1: 247.0000\n",
      "Epoch 8/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3479 - accuracy: 0.8410 - precision_1: 0.8752 - recall_1: 0.7511 - auc_1: 0.9158 - false_positives_1: 923.0000 - val_loss: 0.3979 - val_accuracy: 0.8096 - val_precision_1: 0.8775 - val_recall_1: 0.6729 - val_auc_1: 0.8879 - val_false_positives_1: 205.0000\n",
      "Epoch 9/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3378 - accuracy: 0.8488 - precision_1: 0.8809 - recall_1: 0.7648 - auc_1: 0.9208 - false_positives_1: 891.0000 - val_loss: 0.3977 - val_accuracy: 0.8141 - val_precision_1: 0.8572 - val_recall_1: 0.7068 - val_auc_1: 0.8902 - val_false_positives_1: 257.0000\n",
      "Epoch 10/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3313 - accuracy: 0.8499 - precision_1: 0.8844 - recall_1: 0.7637 - auc_1: 0.9237 - false_positives_1: 861.0000 - val_loss: 0.3929 - val_accuracy: 0.8119 - val_precision_1: 0.8774 - val_recall_1: 0.6789 - val_auc_1: 0.8924 - val_false_positives_1: 207.0000\n",
      "Epoch 11/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3239 - accuracy: 0.8537 - precision_1: 0.8829 - recall_1: 0.7752 - auc_1: 0.9277 - false_positives_1: 886.0000 - val_loss: 0.4067 - val_accuracy: 0.8102 - val_precision_1: 0.8581 - val_recall_1: 0.6954 - val_auc_1: 0.8850 - val_false_positives_1: 251.0000\n",
      "Epoch 12/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3163 - accuracy: 0.8593 - precision_1: 0.8877 - recall_1: 0.7841 - auc_1: 0.9313 - false_positives_1: 855.0000 - val_loss: 0.3974 - val_accuracy: 0.8100 - val_precision_1: 0.8131 - val_recall_1: 0.7531 - val_auc_1: 0.8934 - val_false_positives_1: 378.0000\n",
      "Epoch 13/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3111 - accuracy: 0.8620 - precision_1: 0.8962 - recall_1: 0.7816 - auc_1: 0.9333 - false_positives_1: 780.0000 - val_loss: 0.4030 - val_accuracy: 0.8150 - val_precision_1: 0.8457 - val_recall_1: 0.7229 - val_auc_1: 0.8909 - val_false_positives_1: 288.0000\n",
      "Epoch 14/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.3032 - accuracy: 0.8649 - precision_1: 0.8963 - recall_1: 0.7889 - auc_1: 0.9367 - false_positives_1: 787.0000 - val_loss: 0.4032 - val_accuracy: 0.8152 - val_precision_1: 0.8529 - val_recall_1: 0.7146 - val_auc_1: 0.8910 - val_false_positives_1: 269.0000\n",
      "Epoch 15/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2980 - accuracy: 0.8675 - precision_1: 0.8994 - recall_1: 0.7920 - auc_1: 0.9398 - false_positives_1: 764.0000 - val_loss: 0.4118 - val_accuracy: 0.8085 - val_precision_1: 0.8072 - val_recall_1: 0.7577 - val_auc_1: 0.8884 - val_false_positives_1: 395.0000\n",
      "Epoch 16/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2917 - accuracy: 0.8711 - precision_1: 0.8993 - recall_1: 0.8011 - auc_1: 0.9423 - false_positives_1: 773.0000 - val_loss: 0.4173 - val_accuracy: 0.8164 - val_precision_1: 0.8218 - val_recall_1: 0.7586 - val_auc_1: 0.8897 - val_false_positives_1: 359.0000\n",
      "Epoch 17/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2860 - accuracy: 0.8717 - precision_1: 0.8935 - recall_1: 0.8093 - auc_1: 0.9446 - false_positives_1: 832.0000 - val_loss: 0.4097 - val_accuracy: 0.8135 - val_precision_1: 0.8469 - val_recall_1: 0.7174 - val_auc_1: 0.8903 - val_false_positives_1: 283.0000\n",
      "Epoch 18/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2833 - accuracy: 0.8752 - precision_1: 0.9043 - recall_1: 0.8058 - auc_1: 0.9454 - false_positives_1: 735.0000 - val_loss: 0.4142 - val_accuracy: 0.8174 - val_precision_1: 0.8220 - val_recall_1: 0.7613 - val_auc_1: 0.8918 - val_false_positives_1: 360.0000\n",
      "Epoch 19/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2761 - accuracy: 0.8777 - precision_1: 0.9062 - recall_1: 0.8101 - auc_1: 0.9483 - false_positives_1: 723.0000 - val_loss: 0.4226 - val_accuracy: 0.8195 - val_precision_1: 0.8677 - val_recall_1: 0.7091 - val_auc_1: 0.8911 - val_false_positives_1: 236.0000\n",
      "Epoch 20/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2725 - accuracy: 0.8799 - precision_1: 0.9075 - recall_1: 0.8141 - auc_1: 0.9502 - false_positives_1: 715.0000 - val_loss: 0.4247 - val_accuracy: 0.8145 - val_precision_1: 0.8407 - val_recall_1: 0.7279 - val_auc_1: 0.8885 - val_false_positives_1: 301.0000\n",
      "Epoch 21/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2661 - accuracy: 0.8832 - precision_1: 0.9109 - recall_1: 0.8185 - auc_1: 0.9527 - false_positives_1: 690.0000 - val_loss: 0.4387 - val_accuracy: 0.8125 - val_precision_1: 0.8374 - val_recall_1: 0.7265 - val_auc_1: 0.8832 - val_false_positives_1: 308.0000\n",
      "Epoch 22/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2626 - accuracy: 0.8818 - precision_1: 0.9053 - recall_1: 0.8213 - auc_1: 0.9536 - false_positives_1: 741.0000 - val_loss: 0.4396 - val_accuracy: 0.8116 - val_precision_1: 0.8417 - val_recall_1: 0.7187 - val_auc_1: 0.8858 - val_false_positives_1: 295.0000\n",
      "Epoch 23/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2573 - accuracy: 0.8871 - precision_1: 0.9120 - recall_1: 0.8271 - auc_1: 0.9557 - false_positives_1: 688.0000 - val_loss: 0.4426 - val_accuracy: 0.8150 - val_precision_1: 0.8427 - val_recall_1: 0.7265 - val_auc_1: 0.8887 - val_false_positives_1: 296.0000\n",
      "Epoch 24/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2519 - accuracy: 0.8887 - precision_1: 0.9110 - recall_1: 0.8322 - auc_1: 0.9578 - false_positives_1: 701.0000 - val_loss: 0.4599 - val_accuracy: 0.8125 - val_precision_1: 0.8335 - val_recall_1: 0.7316 - val_auc_1: 0.8832 - val_false_positives_1: 319.0000\n",
      "Epoch 25/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2497 - accuracy: 0.8902 - precision_1: 0.9097 - recall_1: 0.8371 - auc_1: 0.9586 - false_positives_1: 716.0000 - val_loss: 0.4608 - val_accuracy: 0.8121 - val_precision_1: 0.8225 - val_recall_1: 0.7453 - val_auc_1: 0.8848 - val_false_positives_1: 351.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2478 - accuracy: 0.8912 - precision_1: 0.9093 - recall_1: 0.8403 - auc_1: 0.9594 - false_positives_1: 723.0000 - val_loss: 0.4594 - val_accuracy: 0.8141 - val_precision_1: 0.8670 - val_recall_1: 0.6958 - val_auc_1: 0.8877 - val_false_positives_1: 233.0000\n",
      "Epoch 27/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2451 - accuracy: 0.8924 - precision_1: 0.9096 - recall_1: 0.8429 - auc_1: 0.9604 - false_positives_1: 722.0000 - val_loss: 0.4609 - val_accuracy: 0.8065 - val_precision_1: 0.7978 - val_recall_1: 0.7664 - val_auc_1: 0.8883 - val_false_positives_1: 424.0000\n",
      "Epoch 28/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2375 - accuracy: 0.8950 - precision_1: 0.9156 - recall_1: 0.8426 - auc_1: 0.9626 - false_positives_1: 670.0000 - val_loss: 0.4736 - val_accuracy: 0.8172 - val_precision_1: 0.8511 - val_recall_1: 0.7224 - val_auc_1: 0.8833 - val_false_positives_1: 276.0000\n",
      "Epoch 29/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2363 - accuracy: 0.8986 - precision_1: 0.9165 - recall_1: 0.8505 - auc_1: 0.9631 - false_positives_1: 668.0000 - val_loss: 0.4917 - val_accuracy: 0.8096 - val_precision_1: 0.8416 - val_recall_1: 0.7132 - val_auc_1: 0.8807 - val_false_positives_1: 293.0000\n",
      "Epoch 30/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2325 - accuracy: 0.8966 - precision_1: 0.9159 - recall_1: 0.8463 - auc_1: 0.9645 - false_positives_1: 670.0000 - val_loss: 0.4717 - val_accuracy: 0.8085 - val_precision_1: 0.8072 - val_recall_1: 0.7577 - val_auc_1: 0.8856 - val_false_positives_1: 395.0000\n",
      "Epoch 31/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2251 - accuracy: 0.9027 - precision_1: 0.9208 - recall_1: 0.8556 - auc_1: 0.9668 - false_positives_1: 634.0000 - val_loss: 0.4889 - val_accuracy: 0.8096 - val_precision_1: 0.8302 - val_recall_1: 0.7279 - val_auc_1: 0.8815 - val_false_positives_1: 325.0000\n",
      "Epoch 32/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2260 - accuracy: 0.8994 - precision_1: 0.9154 - recall_1: 0.8537 - auc_1: 0.9663 - false_positives_1: 680.0000 - val_loss: 0.4991 - val_accuracy: 0.8119 - val_precision_1: 0.8422 - val_recall_1: 0.7187 - val_auc_1: 0.8821 - val_false_positives_1: 294.0000\n",
      "Epoch 33/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2204 - accuracy: 0.9039 - precision_1: 0.9192 - recall_1: 0.8606 - auc_1: 0.9683 - false_positives_1: 652.0000 - val_loss: 0.4976 - val_accuracy: 0.8063 - val_precision_1: 0.8098 - val_recall_1: 0.7471 - val_auc_1: 0.8809 - val_false_positives_1: 383.0000\n",
      "Epoch 34/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2203 - accuracy: 0.9028 - precision_1: 0.9170 - recall_1: 0.8602 - auc_1: 0.9686 - false_positives_1: 671.0000 - val_loss: 0.5095 - val_accuracy: 0.8123 - val_precision_1: 0.8249 - val_recall_1: 0.7426 - val_auc_1: 0.8856 - val_false_positives_1: 344.0000\n",
      "Epoch 35/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2164 - accuracy: 0.9035 - precision_1: 0.9184 - recall_1: 0.8603 - auc_1: 0.9695 - false_positives_1: 659.0000 - val_loss: 0.5276 - val_accuracy: 0.8102 - val_precision_1: 0.8325 - val_recall_1: 0.7265 - val_auc_1: 0.8824 - val_false_positives_1: 319.0000\n",
      "Epoch 36/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2103 - accuracy: 0.9072 - precision_1: 0.9236 - recall_1: 0.8637 - auc_1: 0.9711 - false_positives_1: 616.0000 - val_loss: 0.5287 - val_accuracy: 0.7990 - val_precision_1: 0.7929 - val_recall_1: 0.7522 - val_auc_1: 0.8810 - val_false_positives_1: 429.0000\n",
      "Epoch 37/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2087 - accuracy: 0.9082 - precision_1: 0.9217 - recall_1: 0.8683 - auc_1: 0.9718 - false_positives_1: 636.0000 - val_loss: 0.5487 - val_accuracy: 0.8127 - val_precision_1: 0.8512 - val_recall_1: 0.7100 - val_auc_1: 0.8815 - val_false_positives_1: 271.0000\n",
      "Epoch 38/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2061 - accuracy: 0.9094 - precision_1: 0.9210 - recall_1: 0.8719 - auc_1: 0.9724 - false_positives_1: 645.0000 - val_loss: 0.5568 - val_accuracy: 0.7982 - val_precision_1: 0.7891 - val_recall_1: 0.7558 - val_auc_1: 0.8776 - val_false_positives_1: 441.0000\n",
      "Epoch 39/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.2069 - accuracy: 0.9106 - precision_1: 0.9226 - recall_1: 0.8730 - auc_1: 0.9724 - false_positives_1: 631.0000 - val_loss: 0.5439 - val_accuracy: 0.8096 - val_precision_1: 0.8208 - val_recall_1: 0.7407 - val_auc_1: 0.8797 - val_false_positives_1: 353.0000\n",
      "Epoch 40/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1991 - accuracy: 0.9137 - precision_1: 0.9275 - recall_1: 0.8752 - auc_1: 0.9741 - false_positives_1: 590.0000 - val_loss: 0.5663 - val_accuracy: 0.8073 - val_precision_1: 0.8208 - val_recall_1: 0.7343 - val_auc_1: 0.8778 - val_false_positives_1: 350.0000\n",
      "Epoch 41/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1942 - accuracy: 0.9167 - precision_1: 0.9288 - recall_1: 0.8811 - auc_1: 0.9756 - false_positives_1: 582.0000 - val_loss: 0.5587 - val_accuracy: 0.8056 - val_precision_1: 0.7927 - val_recall_1: 0.7723 - val_auc_1: 0.8798 - val_false_positives_1: 441.0000\n",
      "Epoch 42/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1953 - accuracy: 0.9166 - precision_1: 0.9254 - recall_1: 0.8847 - auc_1: 0.9753 - false_positives_1: 615.0000 - val_loss: 0.5760 - val_accuracy: 0.8116 - val_precision_1: 0.8277 - val_recall_1: 0.7371 - val_auc_1: 0.8795 - val_false_positives_1: 335.0000\n",
      "Epoch 43/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1942 - accuracy: 0.9139 - precision_1: 0.9171 - recall_1: 0.8875 - auc_1: 0.9756 - false_positives_1: 692.0000 - val_loss: 0.5784 - val_accuracy: 0.8042 - val_precision_1: 0.8034 - val_recall_1: 0.7508 - val_auc_1: 0.8765 - val_false_positives_1: 401.0000\n",
      "Epoch 44/50\n",
      "604/604 [==============================] - 1s 2ms/step - loss: 0.1917 - accuracy: 0.9164 - precision_1: 0.9258 - recall_1: 0.8838 - auc_1: 0.9762 - false_positives_1: 611.0000 - val_loss: 0.6162 - val_accuracy: 0.8141 - val_precision_1: 0.8510 - val_recall_1: 0.7142 - val_auc_1: 0.8749 - val_false_positives_1: 273.0000\n",
      "Epoch 45/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1852 - accuracy: 0.9216 - precision_1: 0.9313 - recall_1: 0.8902 - auc_1: 0.9780 - false_positives_1: 566.0000 - val_loss: 0.6075 - val_accuracy: 0.8100 - val_precision_1: 0.8213 - val_recall_1: 0.7412 - val_auc_1: 0.8765 - val_false_positives_1: 352.0000\n",
      "Epoch 46/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9180 - precision_1: 0.9281 - recall_1: 0.8850 - auc_1: 0.9777 - false_positives_1: 591.0000 - val_loss: 0.6087 - val_accuracy: 0.7988 - val_precision_1: 0.8061 - val_recall_1: 0.7311 - val_auc_1: 0.8706 - val_false_positives_1: 384.0000\n",
      "Epoch 47/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1832 - accuracy: 0.9204 - precision_1: 0.9322 - recall_1: 0.8862 - auc_1: 0.9786 - false_positives_1: 556.0000 - val_loss: 0.6454 - val_accuracy: 0.8096 - val_precision_1: 0.8450 - val_recall_1: 0.7091 - val_auc_1: 0.8694 - val_false_positives_1: 284.0000\n",
      "Epoch 48/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1783 - accuracy: 0.9236 - precision_1: 0.9312 - recall_1: 0.8951 - auc_1: 0.9795 - false_positives_1: 570.0000 - val_loss: 0.6198 - val_accuracy: 0.8031 - val_precision_1: 0.8109 - val_recall_1: 0.7366 - val_auc_1: 0.8696 - val_false_positives_1: 375.0000\n",
      "Epoch 49/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1812 - accuracy: 0.9225 - precision_1: 0.9315 - recall_1: 0.8922 - auc_1: 0.9788 - false_positives_1: 566.0000 - val_loss: 0.6371 - val_accuracy: 0.8079 - val_precision_1: 0.8365 - val_recall_1: 0.7151 - val_auc_1: 0.8766 - val_false_positives_1: 305.0000\n",
      "Epoch 50/50\n",
      "604/604 [==============================] - 2s 3ms/step - loss: 0.1735 - accuracy: 0.9250 - precision_1: 0.9325 - recall_1: 0.8970 - auc_1: 0.9807 - false_positives_1: 560.0000 - val_loss: 0.6306 - val_accuracy: 0.8061 - val_precision_1: 0.8186 - val_recall_1: 0.7339 - val_auc_1: 0.8734 - val_false_positives_1: 355.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fbdd5a6dc0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(units=64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    keras.layers.Dense(units=32, activation='relu'),\n",
    "    keras.layers.Dense(units=16, activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC(), keras.metrics.FalsePositives()])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43239a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6306 - accuracy: 0.8061 - precision_1: 0.8186 - recall_1: 0.7339 - auc_1: 0.8734 - false_positives_1: 355.0000\n",
      "Test loss: 0.6305810809135437\n",
      "Test accuracy: 0.8060505390167236\n",
      "Test precision: 0.8185998797416687\n",
      "Test recall: 0.7338525056838989\n",
      "Test F1 score: 0.4707093274931376\n",
      "Test ROC AUC: 0.8734444379806519\n",
      "Test specificity: 0.8815877251501001\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy, precision, recall, auc, fp = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test precision: {precision}\")\n",
    "print(f\"Test recall: {recall}\")\n",
    "print(f\"Test F1 score: {2*((precision*recall)/(precision+recall+1))}\")\n",
    "print(f\"Test ROC AUC: {auc}\")\n",
    "print(f\"Test specificity: {1-fp/(fp+tn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40271995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, optimizers, metrics\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', metrics.Precision(), metrics.Recall(), metrics.AUC(), metrics.FalsePositives(), metrics.FalseNegatives()])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3aef786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.8089515126398674\n",
      "Precision: 0.825503355704698\n",
      "Recall: 0.7324782409528172\n",
      "F1 Score: 0.7762135922330097\n",
      "ROC AUC: 0.8827073962128503\n",
      "Specificity: 0.872115020809686\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model on test set\n",
    "accuracy = accuracy_score(y_test, y_pred.round())\n",
    "precision = precision_score(y_test, y_pred.round())\n",
    "recall = recall_score(y_test, y_pred.round())\n",
    "f1 = f1_score(y_test, y_pred.round())\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred.round()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f79e328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC(), keras.metrics.SpecificityAtSensitivity(0.5)])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2bc24714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.7942395358474927\n",
      "Precision: 0.7939588521831958\n",
      "Recall: 0.7942395358474927\n",
      "F1 Score: 0.7940167132047065\n",
      "ROC AUC: 0.7913763337203573\n",
      "Specificity: 0.8214150586454786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred,average='weighted')\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Convert y_test and y_pred to one-hot encoded format\n",
    "n_classes = len(np.unique(y_train))\n",
    "y_test_oh = label_binarize(y_test, classes=np.arange(n_classes))\n",
    "y_pred_oh = label_binarize(y_pred, classes=np.arange(n_classes))\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test_oh, y_pred_oh)\n",
    "precision = precision_score(y_test_oh, y_pred_oh, average='weighted')\n",
    "recall = recall_score(y_test_oh, y_pred_oh, average='weighted')\n",
    "f1 = f1_score(y_test_oh, y_pred_oh, average='weighted')\n",
    "roc_auc = roc_auc_score(y_test_oh, y_pred_oh, average='weighted')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_oh.argmax(axis=1), y_pred_oh.argmax(axis=1)).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"Specificity: {specificity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a6c1ca4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_48284\\867060989.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mfold_specificity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mskf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Perform K-fold cross-validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StratifiedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Define the DNN model\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the K-fold cross-validator\n",
    "k = 5\n",
    "fold_accuracy = []\n",
    "fold_precision = []\n",
    "fold_recall = []\n",
    "fold_f1 = []\n",
    "fold_roc_auc = []\n",
    "fold_specificity = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform K-fold cross-validation\n",
    "for train_index, test_index in skf.split(x_train, y_train):\n",
    "    x_train_fold, x_test_fold = x_train[train_index], x_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # Create the DNN model\n",
    "    model = create_model()\n",
    "\n",
    "    # Train the DNN model\n",
    "    model.fit(x_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = np.argmax(model.predict(x_test_fold), axis=-1)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test_fold, y_pred)\n",
    "    precision = precision_score(y_test_fold, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_fold, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_fold, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_test_fold, y_pred, average='weighted')\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_fold, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Append the performance metrics for the current fold\n",
    "    fold_accuracy.append(accuracy)\n",
    "    fold_precision.append(precision)\n",
    "    fold_recall.append(recall)\n",
    "    fold_f1.append(f1)\n",
    "    fold_roc_auc.append(roc_auc)\n",
    "    fold_specificity.append(specificity)\n",
    "\n",
    "# Print the mean performance metrics across all folds\n",
    "print(\"Mean Accuracy:\", np.mean(fold_accuracy))\n",
    "print(\"Mean Precision:\", np.mean(fold_precision))\n",
    "print(\"Mean Recall:\", np.mean(fold_recall))\n",
    "print(\"Mean F1 Score:\", np.mean(fold_f1))\n",
    "print(\"Mean ROC AUC Score:\", np.mean(fold_roc_auc))\n",
    "print(\"Mean Specificity:\", np.mean(fold_specificity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "edee36eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "121/121 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.810929810929811\n",
      "Precision: 0.8119371137562218\n",
      "Recall: 0.810929810929811\n",
      "F1-Score: 0.809409217247117\n",
      "ROC-AUC Score: 0.8031275990215321\n",
      "Specificity: 0.8740083994400373\n",
      "Fold: 2\n",
      "121/121 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8091168091168092\n",
      "Precision: 0.8128947975395725\n",
      "Recall: 0.8091168091168092\n",
      "F1-Score: 0.8065125303019917\n",
      "ROC-AUC Score: 0.7988498319277534\n",
      "Specificity: 0.8934579439252337\n",
      "Fold: 3\n",
      "121/121 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8080310880829016\n",
      "Precision: 0.810037361458529\n",
      "Recall: 0.8080310880829016\n",
      "F1-Score: 0.805745838361408\n",
      "ROC-AUC Score: 0.7974255213336331\n",
      "Specificity: 0.8845265588914549\n",
      "Fold: 4\n",
      "121/121 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.8191709844559586\n",
      "Precision: 0.8211835029644363\n",
      "Recall: 0.8191709844559586\n",
      "F1-Score: 0.8176276734210852\n",
      "ROC-AUC Score: 0.8120853750381781\n",
      "Specificity: 0.8852149267831837\n",
      "Fold: 5\n",
      "121/121 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8160621761658031\n",
      "Precision: 0.8224389119051351\n",
      "Recall: 0.8160621761658031\n",
      "F1-Score: 0.8131847439369875\n",
      "ROC-AUC Score: 0.8061726080019423\n",
      "Specificity: 0.9087901701323251\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Convert y_train and y_test to one-hot encoded arrays\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Define number of folds\n",
    "n_splits = 5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "# Initialize list to store accuracy for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores=[]\n",
    "\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X_train, y_train)):\n",
    "\n",
    "    # Print current fold\n",
    "    print(f'Fold: {fold+1}')\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[test_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "    # Define the DNN model architecture\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model on the current fold\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    y_pred = np.argmax(model.predict(X_val_fold), axis=-1)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(np.argmax(y_val_fold, axis=-1), y_pred)\n",
    "    precision = precision_score(np.argmax(y_val_fold, axis=-1), y_pred, average='weighted')\n",
    "    recall = recall_score(np.argmax(y_val_fold, axis=-1), y_pred, average='weighted')\n",
    "    f1 = f1_score(np.argmax(y_val_fold, axis=-1), y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(np.argmax(y_val_fold, axis=-1), y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(np.argmax(y_val_fold, axis=-1), y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1-Score: {f1}')\n",
    "    print(f'ROC-AUC Score: {roc_auc}')\n",
    "    print(f'Specificity: {specificity}')\n",
    "\n",
    "    # Add accuracy to the list of accuracy scores\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e66733e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.8223258223258223\n"
     ]
    }
   ],
   "source": [
    "# Print the mean accuracy score across all folds\n",
    "print(f'Mean Accuracy: {max(accuracy_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4c6a3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1fbe2be07c0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4d67e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <keras.engine.sequential.Sequential object at 0x000001FBE2BE07C0>>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f98a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('coswara_DNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "955d6a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_109 (Dense)           (None, 32)                2560      \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,122\n",
      "Trainable params: 3,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('coswara_DNN.h5')\n",
    "\n",
    "# Print the summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b32be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
